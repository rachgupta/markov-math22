All right. Welcome, welcome back. It's great to see everyone again.

I hope you all had a nice Thanksgiving break.

I got a chance to rest of it and relax.

Let's just quickly run through some announcements before we get going here today.

So in terms of things coming up in the course, we are kind of wrapping things up.

We have problems at  due on Wednesday, so our last problems at the last bit of web work is due on Wednesday.

Then we're totally done with all the quizzes, all the problem sets.

The project draft is also due on Wednesday. So keep that in mind once.

So what's going to happen on Wednesday? I think I set the due time to be like our normal sets for the project draft,

but there's a little bit of wiggle room there, but not a tremendous amount.

So please make sure that like, that's a deadline that we should really try to hit whatever you have.

You need to give that to me on Wednesday, and that's what we'll go out for the peer reviews.

OK? The peer reviews that is a part of your final project grade is sort of the thoughtfulness of your review,

your reviews of other projects, so everyone will review two projects.

These will be automatically assigned to you on canvas. You should get an email about them when they're assigned, it should come at.

I think it's automatically scheduled to go at five p.m. on on Wednesday.

So it's not that I expect that it'll take you a huge amount of time to do the peer reviews.

But I mean, I I'm giving a fair amount of time to actually do them, but I don't don't feel like you need to spend, you know,

all of your waking hours between Wednesday and Monday, just working on the peer reviews,

the then the final version of your project is actually do two weeks from today.

OK. So after you turn in your final project, then of course, your attention can turn to reading the peer reviews that are assigned to you,

trying to both give good feedback to your classmates, but also thinking about like,

how could you incorporate some of their good ideas into your project?

Does it suggest some different ways you might tailor your project to kind of address

some things you're noticing that are mildly awkward in someone else's organization?

So like, try to think about how you can apply some of those same ideas or lessons from that draft to back to your own project as well.

I think oftentimes that's a really good opportunity to improve the quality of your draft.

Then between Wednesday and I think Tuesday is the deadline that we've set, like end of day on Tuesday, you should get feedback from your t.f reader.

So either me or Caleb seeing Lucy or Kevin, it's.

I'm planning on meeting with all of my groups, but I've not made it a requirement that the TFS necessarily have a meeting,

some teams might prefer to write written feedback. It's then totally up to you if you would prefer to, like, meet again to discuss that feedback.

I mean, the staffs are encouraged to support you and what you think is best for your project.

They're not going to. They're not there to tell you what to do. OK? They're there to kind of help guide and answer questions as they come up.

But so that's why you can kind of expect if you're if I am your reader, then if I haven't,

you haven't heard from me by Thursday to schedule a meeting to talk about your

project and feel free to just email me and we can schedule the time to to chat.

OK? Are there questions on what's coming? Is everything clear on what's left for the class?

It's clear. OK, so what I wanted to do today, the last two classes are mostly just what I regard as just fun days.

Hopefully, the rest of the class agrees with me that their fun days.

Part of that is by design for the course because as I mean, you can note, there are no more major assessments left of the course on this material.

And so it's not as if I'm going to test you on any of the material that you're going to see on today's class or Wednesdays class.

Instead, what the point is is that I'm hoping to give some nice capstone, some nice tour of the material that we've seen this semester.

So you can kind of see like some of the themes coming together. And I also hope to set up some of the projects nicely.

So there are some major results that you might want for your project that hopefully I can put in place over the next class and a half or so.

So thinking back to one of the major questions that we had this semester, one of them was.

What's a nice coordinate system to work in, what's a nice basis?

So that was sort of a big goal that we had. A big goal.

To find a nice space.

And I sort of deliberately use the word nice there, because nice is in the eye of the beholder.

It kind of depends on what you want to do, what nice means. So it leads you to the question of, well, what do you really mean by nice?

So what exactly do you mean by this niceness so well, that could depend on your particular application,

so one version of being a nice basis that we've talked about would be saying giving an eigen basis.

So that's back to Chapter five. Answer one. An ongoing basis was pretty nice.

One reason why an eigen basis was nice was because it was diagonals.

So it was sort of a very simple representation for your transformation that your study.

OK. Another question, another answer rather to the question answer two, as you could say, well, I prefer when things weren't worth the normal.

I like to wear the normal bases, so that's what I mean by nice. So you could say, I want to know what the normal basis.

So you could think about this was the focus of Chapter five and Diane Analyzation.

This was the focus of Chapter six and optimization techniques in this context,

when we're thinking about orthogonal decomposition and orthogonal projection and understanding geometry.

It was really nice to have an author normal basis. So I guess I'm a greedy person.

But then the question that occurs to me is why not both?

So what I'd like to know is if I'm given some matrix, can I find an orthogonal or an author's normal again basis?

So can I have everything? So let's give a name to matrices that have this property in them.

The main point of today's class is really just to study and understand these matrices,

and this is really a capstone of a first course and linear algebra.

One of the most important theorems in linear algebra generalized is really nicely to the infinite dimensional setting.

It comes up a lot in physics, it comes up in all many of the projects people are discussing.

So it's really quite an important result. So.

Let's take a to B and and by and matrix.

Then we say, hey, is orthogonal, diagonals visible?

So not just diagonal lines of all but orthogonal diagonal visible.

If. A has an orthogonal and henceforth the normal again basis.

Namely, it has exactly the property that we can find a basis that does both tasks.

So you should note that the way that we define diagonal sizable meant that there was

some basis in which we could represent our linear transformation as a diagonal matrix.

Namely, that your matrix was similar to a diagonal matrix. So A was equal to PDP inverse, where D was a diagonal matrix.

So we're taking a little bit of a different feeling for the definition here,

but the diagonal ization theorem sort of immediately gives us that result.

So note. So if a is orthogonal or diagonal sizable, so then a has an orthogonal eigen basis.

So what that means is that then we can say that it's similar to a diagonal matrix.

So this means. There exists an orthogonal matrix.

Matrix. I guess as.

And a diagonal matrix. Did such that a is equal to s d s inverse?

So it's similar to a diagonal matrix, where now it's not just a random vertical matrix that did this change of coordinates,

it's a special one, as had the additional property that it's an orthogonal matrix.

So thinking back to before break, how do you invert orthogonal matrices?

Yeah. We take the transport, so in this case, it becomes a nicer formula,

so this will actually just be as the as transposed or orthogonal matrices had some pretty nice properties.

In particular, their inverse is given by the transpose.

So that gives you kind of a nice formula there, so to undo that coordinate change here.

You can just take the transpose of that matrix, Jonathan, or want to do on the floor or a.

I. Well, the deglaciation theorem says that if you have an linearly independent eigenvectors, then you can diagonals your matrix, right?

So then to diagonals, your matrix means that you can write it in the form A is equal to SD as inverse.

I guess the diagonal I get the I can't see how many people I pass.

So that's the orthogonal part, right? So the orthogonal part is where I'm getting a little bit of extra.

Otherwise, this is just diagonally visible. So if I'm saying that now, in addition to having my basis of eigenvectors, it's also an orthogonal basis.

So it's an orthogonal collection of actors. Well, then this s inverse thing here is that a matrix that has all the normal columns.

So there just. Elements of the cases.

Yes. OK. Yeah. Yeah. Mm-Hmm. Other questions.

OK. So now the important theorem, the spectral theorem.

So when we talk about the spectrum of a matrix, that's just the set of the eigenvalues.

So the spectral theorem is then going to characterize for us when we can answer these two questions,

when we can give an author normal again basis and the condition is going to be exactly that, your matrix needs to be symmetric.

So we have an end by end matrix with real entries.

Everything is tied to real. So this just means it's an end by end matrix.

It's an element in that vector space, then a is orthogonal diagonal, sizable diagonal, sizable if and only if a is a symmetric matrix.

So namely, they transpose is equal to a.

So this exactly characterizes when we can do this. So it's an if and only if statement.

So we know there are then two directions we need to settle. There are not equivalently difficult.

So let's settle this sort of easier direction before we consider the harder direction.

So saying that it is worth arguing diagonal sizable seems to give a lot of additional structure,

and checking that a matrix is symmetric doesn't seem wildly difficult.

So why not try to prove that direction first? So let's try to prove that first.

So suppose a is orthogonal a.

Diagonal sizable. Well, then by the previous smart remark or previous note, that means that a is equal to.

That's the first transpose where word is a diagonal matrix.

So now I just want to compute a transpose so that a transpose will be as d as transpose transpose.

Well, now I know how the transport effects matrix product, so it's going to reverse this.

So this will become as transpose transpose primes de transpose times as transpose well the transpose of the transpose giving back the original matrix.

We've seen that before. So this becomes s.

If I transpose a diagonal matrix, well, then I'm just going to get back DX and then I have s transpose and hey, look, that's a.

So hence. A is symmetric, so we just proved the forward direction.

We proved a necessary condition in order to be orthogonal, diagonally visible, so we must be a symmetric matrix.

So the counter positive would tell us that if we have a matrix, then it's not symmetric and it's not orthogonal diagonals all.

And all of these, if you're thinking about for your projects, for instance,

generalizing these to complex vector spaces, all of these ideas have nice extensions to complex numbers, too.

So it's kind of a nice thing to think about there. How would this look?

What's the right notion of cemetery in this case? And so forth.

So the other direction is a little bit more subtle and difficult, but it's a nice tour of the ideas that we've seen this semester.

But in order to really see how that proof is going to go,

I'm going to suggest that we take one of my favorite strategies for understanding a new statement,

and that's to try a small example to see what's really happening.

So let's just try an example and see it from the example.

We can identify any patterns that might be useful in proving a result like this.

So and this is often the way that I would even proceed in my own research is that I have some big theorem.

I want to prove I don't immediately see how to prove it. Well, then let's just try it.

Let's see what happens. Maybe write some code. Maybe do some simulations. Maybe do a small back of the envelope calculation, do a thought experiment.

But you want to do something to kind of get some evidence, some idea of how the structure could look generate some data.

All right. So let's do a small example. So my small example, I want to take a particular symmetric matrix because after all,

the reverse implication, the converse, I want to start with a symmetric matrix.

And then I want to be able to construct an Earth a normal eigen basis.

So let's just try to see how we might do that.

So my example again, I don't want my examples to be wildly computationally challenging, so let's just take a two by two.

So let's take a to be the Matrix three negative two negative to negative two three.

There, it's definitely a symmetric matrix. All real entries. So it's my theorem should apply here.

So if the theorem is true, there should be a way to do this.

Well, let's just proceed as we normally would. And let's find a diagonal ization.

So how would we start a financial organization? Exactly right.

So we want to start by finding the eigenvalues, then you'd find the eigen spaces and then you could use that idealization theorem.

So let's find the eigenvalues.

So step one, find our eigenvalues.

So we do that, the only way that I've really taught you to do that is to compute the zeros of the characteristic polynomial.

Certainly, you can look to some of the projects for nice extensions, for other ways of getting at the eigenvalues,

especially for a very large matrices where it's not going to be practical to compute the characteristic polynomial, let alone the zeros.

So, so I'm looking at now the determinant of a minus lambda times the two by two identity matrix.

So this comes out to be three minus lambda squared minus four.

And if you multiply this out, becomes five minus six lambda plus lambda squared.

So then we can just factor and get my two eigenvalues, so my eigenvalues.

Then one in five. OK, so now we find our eigen spaces.

So step to find our eigen spaces again now this is just finding some null spaces.

So the eigen space corresponding to the eigenvalue of one will be the null space of a minus one times the two by two identity matrix.

So this becomes the span of a single factor.

One one. And if you look at the eigen space corresponding to the eigenvalue of five.

Then you have that this would be a minus five times the two by two identity matrix, which nicely becomes the span of the single vector minus one one.

What do you notice about those two eigen spaces? There are orthogonal, so that seems quite nice.

That seems worthy of note. So we have one one dotted with minus one one is equal to zero, so e one and E five are orthogonal sub spaces.

So there's some additional geometry showing up here. So one might naturally wonder if that persists, if that stays to be the case.

But let's continue on. So how would I then find my diagonal analyzing space?

How could I do that? On the eigenvectors, so we've have the eigenvectors right here, one one minus one one.

So then a diagonal izing basis would be one one minus one one I set out for in all the normal eigen basis.

So then there are these vectors are orthogonal. So then what's left to do it, to make it in the normal eigen basis?

What do I have to do?

Yeah, divide by the norm, so then in this case, I would then get one over the square two to one over the square to two and then over here,

I then get minus one over the square two to one over the square to two.

So there is my worth the normal eigen basis. So if I now represent my well, if I represent my original matrix, I can factor it with this new basis.

And then I get the best of the both of both worlds. So. In this case, and we could say take a to be my change of coordinates Matrix P.

The one over the square root of two one over the square to two minus one for the square two to one over the square to two.

Then my matrix one zero zero five and then my inverse matrix.

If I wanted to compute the inverse of this matrix, how would I compute the inverse of this matrix?

Transpose, because all the normal columns, we might as well take that to our advantage.

So one over the square to two minus one over the square to one over square, two one over square to.

So then that gives me an orthogonal diagonal ization of my matrix now.

So I'm getting the best of both worlds of both worlds. I get.

A nice basis in order to understand and study the geometry,

and I also get a basis that reflects the operator that I'm studying so that it also takes into account what a actually does Tommy.

Yeah. So everything in one is orthogonal dot product with zero, with the thing in the other.

So we have our orthogonal compliments. Yeah. Yep.

If we have an orthogonal item basis, I can just scale them and there that won't change whether they're eigenvectors,

so that those will capture the same notion. So.

Other questions, but I did I used Orthogonal in the definition. OK.

So again, let's not lose sight of why we did this example.

One reason why we did this example is to try to inform how this argument might look.

So it's not just a matter of like throwing a party because we finished the calculation.

You know, we're all done. We get to box this up and go home. We really want to think about like, what can we take away from this?

And so there are a few observations that you might make that are sort of useful here.

One observation that we've already made is that these eigen spaces are orthogonal.

And so you might wonder if that's a general phenomenon. Does that always happen for symmetric matrices?

And the answer is that that is a general phenomenon. What so we can, let's prove.

So let's see, I'll move this down. The entire class is about one theorem, so it's tricky to think about what to address.

So building towards the reverse direction, the Converse statement, so we start with our hypothesis if a is a symmetric matrix, so if a symmetric.

Then the eigen spaces are orthogonal.

So then any eigenvectors from distinct Asian spaces.

Spaces are orthogonal. I mean, this would have been a reasonable question.

And you did prove a version of the spectral theorem under the additional hypothesis that you are distinct.

Distinct eigenvalues, so then that the eigen spaces.

Our small. So let's let's go ahead and prove the proof.

So let's just set up some machinery here,

so let's take the one to be an element in your first alien space slam to one and be two to be and I can vector corresponding to your second can space.

The nonzero. Where I'm the one is different from the two.

So now what I want to prove is just that the DOT product between V one and V two will be equal to zero.

And hence, any elements in those spaces will be zero.

I guess I don't even need to assume that they're non-zero because those that will be immediate from zero.

So. The way that we're going to approach this is actually, you know what, I'm going to move to the boards below to prove it.

So the way we're going to prove this is.

Well, I want to compute the DOT product of V and V, but the only thing I really have in the problem is these eigenvalues lurking around.

So I'm going to multiply the dot product by lambda one to try to get it into the problem.

That's really my whole heuristic here of trying to do. So I'm going to start with.

Lambda, one times the dot product of V one in V two.

And then I just want to sort of use that to somehow show that the DOT product would have to be equal to zero.

OK. Well, I can play around with the properties of DOT products now.

So, for instance, it's a scalar I can move it through the DOT product and I could apply it to the one or I can apply it to be two.

So let's move it through. So this becomes lambda one times v one.

We to. OK.

Well, now what could I do? Quinn.

Perfect, right, we use the fact that it's an eigenvalue, so this is the same as eight times the one dotted with B.

All right. Well, I don't know, it's hard to know what to do next.

So my thought is generally, if I don't know what to do, let's at least try to like, simplify things somehow.

So in my case, I'm going to try to use the definition of the dot product.

So the definition of the DOT product tells me that this is equal to a v one transpose times v two.

OK. Well, I now know that I can use properties of matrix transposition to then rewrite this as the one transpose a transpose V to.

OK. What do you know about a transpose? It's a because it's a symmetric matrix, so that's good, OK, fine.

So then I'm going to move over to the next board. Sure, keep writing large enough.

So then I have the one transpose times a times v two.

Well, now I can use a sensitivity and rewrite it like this.

And now this looks like a dot product again, so why not turn this into a dot product?

Well, actually, before we do that eight times v two, what could we do with that?

Lambda, too. OK. So then we have one transpose times Lambda two v two.

And now that's just a scalar that lambda, too. So you can just pull that out front, that's just multiplying by a single real number.

So this is then lambda to be one transpose times B to B one transpose times v two is again the dot product A V one and V two.

So then I have lambda two times v one dotted with V two.

All right, so now if you put this all together. We get the following.

So hence, we have lambda one times v one dotted with V two is equal to lambda two times v one dotted with v two.

So I mean, there are a few ways you could go from here, but I prefer to sort of move it over to the other side and factor.

So this becomes lambda one minus lambda two times v one dotted with V.

It's important to keep track of the types of all the things you're looking at here.

So the DOT product is just a real number. This is just giving you a real number.

This is just a difference of real numbers. So you have the product of two real numbers being equal to zero.

So, you know, one of them has to be equal to zero. Could the first factor be equal to zero?

Oh, because they're distinct, so it must be the second factor, since V and v are distinct, they're assumed to not be equal to one another.

Then we know that V started with V as equal to zero and hence they are orthogonal.

Jonathan, I thought you would have to.

Values are the same I've had multiple. That were the same, but her.

Distinct eigenvalues, distinct eigenvectors corresponding to the same eigenvalue.

Yes, we can. So like here in this particular case, like I had, we.

I mean, this was an element in you one.

I also had one, one was an element in each one.

Those are distinct eigenvectors corresponding to the same eigenvalue of one.

So it's an entire eigen space. So we get lots of eigenvectors corresponding to that single eigenvalue.

But but what I take a vector and the corresponding to the other eigenvalue, it has to be orthogonal to whatever value I chose here.

Because the sense in this case, since they're in different eigen spaces, they have to be orthogonal to one another.

But if you have. If you have to do that, you have to be.

Yes. Yes, because if they correspond to the same eigenvalue, then they're they're on the same eigen space.

Yeah. So you might be thinking of, though, that if a matrix, if the matrix isn't symmetric, the space spaces definitely don't have to be orthogonal.

I mean, we've seen examples where you might have like one in space is the x axis and the other eigen space is like the line Y equals x or something.

I mean, those are definitely not orthogonal. It's not true for general matrices that the eigen spaces will meet at a right angle.

It is true for symmetric matrices. Well, yes, that's what we're trying to prove here.

Yeah, we have not. I mean, we've just proven it here in this theorem. But.

The four symmetric matrices, this will be true. If the matrix is not symmetric, then you don't expect this to be true.

And Lisa. General I.

This proof is completely general, I didn't assume anything about the size of the Matrix.

And by and it's totally fine here. My example just kind of inspired that this might be true.

So then I tried to prove it, but in no place in my proof, like, here's the start of the proof,

and here's the rest I made no mention of the size of the matrix. So that's not necessarily relevant.

No. And it's the same thing is true when you're coming up with ideas, right?

I mean, a two by two matrix might be how you came up with the idea that something might be true and then you proved it in wild generality.

Or maybe it was.

Only two were true for two by two matrices, but your your single, your data point, your data doesn't necessarily encapsulate everything.

Yeah. So this is going to cover that as well, and that there's still going to be orthogonal,

they don't necessarily they won't be orthogonal compliments because there could be lots of eigen spaces like you could have in by a matrix.

You could have and distinct ones at most, but you could also have like one, for instance.

I mean, you could take the identity matrix.

Well, if you take the identity matrix, that's definitely orthogonal, diagnosable because it's already diagonal.

Right? You could just take the standard basis on our end, and that would orthogonal diagonals, the identity matrix.

So and that one only has the one eigenvalue of one with the algebraic multiplicity of N.

But then in that case, you get enough linearly independent eigenvectors.

So does that answer your question? Other questions?

Yeah, severe. You know.

We haven't shown that. No, no. Definitely haven't shown that.

Other questions. It's actually a lot, a lot to do to show the reverse direction.

It's I mean, it's the hardest theorem of the semester, so that's why I'm kind of spending a lot of time building up to it.

Don't want I don't want to spoil the suspense here.

There's another thing you might notice about the example that we considered

was that and this came up even on the quiz when we're thinking about dying,

analyzing matrices. There's the subtlety of whether we're thinking about dying, analyzing them over the real numbers or over the complex numbers.

Because when you're thinking about the zeros of your characteristic polynomial over the complex numbers,

this is one reason why we like the complex numbers. So much is because you have zeros for your functions.

For polynomials and the real numbers, you might not you might not have a point where you intersect with the real axis there,

so you have a little bit of a problem. But in this case, you notice we did have real eigenvalues.

That observation also persists for symmetric matrices. If you have a symmetric matrix, then all of your eigenvalues will be real.

So it's another result we can prove.

So theorem. If a symmetric.

So again, we're setting up under the hypothesis of the converse direction in the spectral theorem of going in the reverse direction,

if you start with that hypothesis, do you have a symmetric matrix? What structure do you have starting to look like?

You've got a lot of structure, so then a has all real eigenvalues.

OK. So let's prove it.

So proof. So one way that you can prove that a number is real is you can compare it with its complex conjugate.

If it's equal to its complex conjugate, then it's a real number. It's going to be our strategy here, but I'm going to take an eigenvalue.

I'm going to compute the complex conjugate of that real number or of that number and then verify that it's equal so that it has to be the same.

Oh. So that's what we're going to shoot for.

So when every time I use an over line here, a bar over my variable, that means take the complex conjugate.

Is everyone comfortable of complex conjugation as everyone remember this?

No. OK. Let me.

So if I have a no say Z is equal to a plus by this only take a second, but it's worth making sure we all agree on what we're talking about.

So here is and the complex plan we call this the real axis and the imaginary axis and Z is equal to a plus b i.

So what that means is that this length here we call this distance B and this distance over here A, so that's representing the complex numbers.

And it's sort of rectangular form could also think about it represented in its polar form.

So if you want to compute the complex conjugate, then you're reflecting across the real axis.

So that would be this point down here, so we call this zee bar for Zee conjugate.

So this is by definition equal to a minus VII slot in this distance.

This is now minus B. So Zee Bar the complex conjugate.

Of Z, an element in the complex numbers is given by.

Zee Bar. Is equal to you, a minus by.

Z is equal to a plus by. So an observation that you can then make is that.

Z is an element in the reals, so that means everything about Z is a complex number.

It's on the real axis. If its big component is equal to zero.

So if and only if Z is equal to Z Bar.

So if Z is equal to Z Bar, then reflecting across this axis gave you back the same thing.

So that meant, hey, look, you're on this axis.

So our strategy approved here so we can prove that something is real by showing that it's equal to its complex conjugate.

So standard trick and complex analysis. Oh, my.

OK. So now let's prove it. So whenever I use the bar here, that always means complex conjugate, not not the little vector arrow.

It's one reason why I don't like the vector aero notation. So what Lambda B and eigenvalue?

For asymmetric. Matrix Bay.

Or they're more likely be a corresponding.

Eigenvectors. All right.

So we then get the equation. A times B is equal to Lambda Times V.

So we're just going to kind of play around with this equation and manipulate it with the goal of

eventually showing that Lambda Bar Lambda complex conjugate of lambda is equal to lambda itself.

So one thing that I could do with this equation is I could take the transpose of both sides.

So let's try that take transpose.

Of both sides. OK.

So then we get a v transpose equal to Lambda V transpose.

OK, so we can use properties of the transpose again, so this becomes V transpose, a transpose is equal to lambda b transpose.

OK. Well.

I said the strategy was going to be to compare with the complex conjugate, so I'd like the complex conjugate to get into the problem somehow.

So why not take the complex conjugate of both sides and see what that looks like?

Hey. So now take conjugate.

Both sides were conjugate both sides. Both sides.

So then we have we transpose a transpose complex conjugate as equal to lambda b transpose complex conjugate.

OK, so now there are some concerns that you might have about how complex country, Jonathan, maybe you're going to get.

That's a great question. So there are complex conjugate of a vector is going to be the complex conjugate of each entry in that vector.

The same thing with a matrix. Then your next question will probably be what happens when you take the complex conjugate of a product?

So you might worry about what that will be.

It actually turns out to behave as nicely as you could hope that the complex conjugate of the product is the product of the complex conjugates.

I'm not going to prove that for you, but I am a complex analyst for a living, and so hopefully you'll believe me.

So what that means is that I can then break this up to be the complex conjugate transpose a

complex conjugate transpose as equal to lambda complex conjugate the complex conjugate transpose.

All right, so now let's think for a moment, what do we know about a what are the entries of like?

Your matrix, your original matrix that we started with, where did the entries have to live?

Tommy. Or sorry, you might not have had your hand up.

I shouldn't have called on you. Yeah. Yeah. They have to live in the real numbers, right,

so if I take the complex conjugate of a we started out with our matrix had to be only matrices with real entries.

OK, so here if I take the complex conjugate of a, that's just going to be a itself again.

So that's somewhat nice. So then we get the complex conjugate transpose time pay as equal to lambda complex conjugate the conjugate transpose.

And now, unfortunately, there's not that much we can really do in terms of simplifying with complex conjugation.

So we need to think about how else we might approach simplifying this.

Well, one thing that you can notice is that when you see a V transpose by itself,

it sort of calls out for multiplying by something, multiplying by V, for instance.

We did in the last problem to be kind of a nice thing to simplify.

So we're going to take the dot product of both sides with the.

So let's try that and see what happens. So now take the DOT product.

Both sides. With the.

OK, so then what will we get, we'll get the bar transpose times, A B is then equal to Lambda Bar.

We are transpose B OK.

Now we're really in business here. So what can we do?

How can we simplify this? Yes, of course.

It was a good thing to focus on the AV part. I like that idea.

What could we do with the AV part? It's Lambda V, right, so we could replace that because we remember Aves equal to Lambda V.

Well, hey, why not get rid of the eh? So this becomes the bar transpose times.

Lambda V is equal to Lambda Bar Times V Bar Transpose Times V.

OK.

Well, the lambda, again, is just a scalar that pulls up front you have lambda times v bar transpose V is equal to lambda bar times v bar transpose V.

Who is an eigenvectors? So it's nonzero. So this thing is very much like the DOT product.

So it's like, I can't it's it's zero if and only if it's the zero vector, so we know it's not non-zero vector.

So then that means that these coefficients lambda and lambda bar then have to be equal.

I'm telling you that Lambda is equal to the bar. Jonathan, how do you?

V is an Eigenvectors V and observe actor. But what about like you just feel like we also plan to get that?

But the transpose that's a good question. So just ignore the complex conjugation for a moment, pretend that V is a real vector.

So then this would be v transpose times v. So that's v dotted with V.

Right? So if you have V dotted with V, then that's like the length squared of V, right?

And that's only going to be zero zero. Right? But I don't understand how.

How to get the. OK, so let's go back over here for a moment and think about what the product of a complex conjugate is.

I should have maybe reviewed slightly more.

So if we take the product of each of complex conjugative Z with itself, so Z transpose time Z, this will be equal to a minus B times a plus by.

Right. So then when I multiply this out, it's a difference of squares.

So then this is going to be a squared plus b squared.

So if you think about what that is geometrically, what does that represent?

In this picture that I've drawn. We're on the air.

Yeah, it's sort of like this length here. The curve getting at the the square of the hypotenuse of that triangle.

So this is the modulus squared of the the same thing is going to be true here when we're taking the products of these vectors,

it's going to happen in each component because then so then it would be saying each component would have to be equal to zero.

So go. Here, hear what is multiplying, yes.

Yup, here we're just multiplying. Oh, you're right, you're right, I just want to multiply.

You're right, it could take the dot product, I suppose, if I wanted to, but I don't want to transpose.

So let's just multiply. You're right.

Got too, too fixated on my. Fix it on my dot products fix.

Now there are questions. So the upshot of this is it's telling us that symmetric matrices actually have a lot of structure, unlike general matrices.

They have to have real eigenvalues, and their eigen spaces can't just be sort of like skew in space, they have to meet orthogonal.

And so that's actually a pretty useful bit of geometry to know when we're thinking about how we might

approach or giving an orthogonal basis for my for my four hour at an orthogonal eigen basis for our.

OK. Are there questions? All right.

Let's see. So I'm not going to have time to finish the proof of the theorem today.

So what I want to do is I just want to set it up because the logic there's actually quite a lot to the overall structure, to the proof.

And so I want to set it up so that you can kind of think about it between now and Wednesday's class.

We won't take all of Wednesday's class to finish it. Then we'll prove the spectral theorem and we'll kind of go from there.

So let me go through how the proof is going to look.

It's really a quite nice because it tours a lot of the ideas that we've seen over the semester.

And it's a fundamental theorem for anybody doing a project related to principal component analysis or singular value decomposition.

Oh. So for.

So the idea is we're going to suppose. But a is a symmetric.

And by and matrix. So we're going to prove this result by contradiction.

So the way that we're going to do the thing we're going to contradict is we're going to assume that we've chosen and a war end is minimal.

OK, so we're and is as small as possible where it's not or legally diagnosable.

So let's prove suppose a is a symmetric and by and matrix.

And let's actually for contradiction, we're then going to say that a is an example of not just a symmetric one by a matrix.

But one that's not worth arguing diagonals. So where the conclusion would fail, so we're trying to prove this by contradiction.

So I suppose it is a symmetric and by a matrix, but is not going to be diagnosable.

So we're supposing we have a counter example to our theorem. And.

If we had a one by one matrix, if we had a one by one matrix, it's always orthogonal diagonals.

So it's just a one by one matrix. It doesn't really make sense to talk about analyzing a one by one matrix.

So that means that for NW will be then strictly larger than one when we're thinking

about what the smallest one could be the smallest size matrix where this happens.

So we want it to be a symmetric and by a matrix that is not orthogonal, diagnosable where an is as small as possible.

So what that means is that no matter how small and is, maybe it's like a thirty seven by thirty seven matrix,

that means every matrix from every symmetric matrix from size one by one up through thirty six by thirty six,

then would have to be orthogonal, diagnosable. This is the smallest size you can have where it's not orthogonal.

Diagnosable. OK.

So if you think about how I'm trying to set up, the proof is that I start with my essay that I say is not orthogonal, diagonals visible.

Then I'm going to take a smaller piece of a to get a smaller matrix,

something that's say, not thirty seven by thirty seven, but thirty six by thirty six.

Then, because that's a smaller matrix. My assumption here would then tell me it must be orthogonal diagonal sizable.

I thought the only diagonal size, that smaller piece.

And then I put that together with my with a vector from my original matrix to then diagonals, the original matrix giving a contradiction.

That's going to be the general structure of what we're doing. There's certainly some work in doing that, but that's the general structure.

So we should also note that if NW is the smallest, it can possibly be.

Well, one by one matrices, we say, are sort of trivially diagonal of also and has to be strictly bigger than one.

Are there any questions so far? Yes, Jonathan.

Well, yeah, there are a number of names that this kind of structure goes by,

I mean, proof by smallest counterexample or something approved by the descent.

So yeah, there's a number of different ways that you can think about this. It's very similar to approved by induction.

Yeah, it's a nice way of thinking about it. OK.

So again, I I don't want to rush this.

If I had like a little bit more time, I could finish today, but I don't want to rush this, so I really want to focus on just what the big ideas are.

And then we'll probably and a little bit early. And Isa.

Yep. Because of the assumption that that's the smallest size that isn't worth diagnosable, because then if you went to a smaller size,

then it must be orthogonal dying leasable because otherwise you would have taken that to be the smallest size.

The size of. The smallest and where this wouldn't be with diagnosable up.

So, yeah, that's a great question. OK, so now what we want to do is we want the contradiction.

So the way that I'm going to contradict this is I'm going to then construct an orthogonal basis consisting of eigenvectors.

So our goal is we want to find all the normal for the normal.

Basis for our NW, consisting of eigenvectors  eigenvectors of a.

Because once we have that, that tells us that this word applies is with arguably diagnosable and we're done well in that case,

then we're saying that we have a contradiction to be a contradiction, then we're not.

OK, so that's what we're hoping for, if we can do that. We're done.

So we want to somehow be able to do that.

So the way that we're going to do that is we're going to use the previous two theorems about the structure of symmetric matrices to then say,

we must have real eigenvalues, they must have orthogonal eigen spaces to then piece together a way of putting of diagonals in your entire space.

OK. So I think rather than rushing through the sort of  minutes left of this proof, I'm going to finish it on Wednesdays class.

On Wednesdays class. I'm also going to spend a little bit of time answering one question that I've spent

quite a lot of time answering questions about by email and in individual meetings.

And that's what are some additional fun math classes you might take.

So not maybe not everyone is thinking about that,

but I do want to give us some idea of what other options there are for people for next semester or next year.

So that's where we'll kind of go with next class, and I think that'll make a nice bookend to the semester.

All right. See everyone on Wednesday. Check one you.

