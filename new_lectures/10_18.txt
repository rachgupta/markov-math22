Hi, everyone, welcome back. I hope you all had a nice weekend, the hand out is there.
So if you need need or want a copy, it's also on the website.
So if you prefer the digital copy. Feel free just to quickly run through some announcements at seven is do on our normal schedule.
So on Wednesday we have lots of office hours today.
Got lots of great questions in my office hours before class today. So that was fun.
I think that Caleb can get some more fun questions later today at office hours.
So today we'll have a bunch through math night tonight, then also a bunch tomorrow as well, and including my Wednesday morning office hours.
So feel free to come by and ask questions. We have our fourth quiz on Friday, so keep that in mind on our usual schedule.
So every two weeks or so. So, again, kind of getting used to the questions that might show up on a midterm.
I'll also put my link up here again, I keep adding more times if you'd like to have individual meetings with me at this point,
I'm happy to open it up to anyone I've mentioned to students. Sometimes it's faster to resolve things.
And a quick one on one discussion, like a 10, 15 minute meeting,
we can often go through a number of your questions and faster than going to office hours.
So feel free if you can find a time. I just kind of tend to keep adding times a few weeks out as my schedule solidifies more.
In terms of the reading, we're nearly done with four point one, you've probably already finished your reading for that section.
We'll get into four point two and probably finish four point two today.
The I guess the only other thing that I'll announce today is I just wanted to take the
opportunity to thank all of you or many of you for filling out the early course feedback.
I think there are some really good ideas there. There are many great ideas that I wish we could implement, but unfortunately we can't.
To the student that suggested we just make all the classes longer and meet more often, I would be happy to.
I mean, nothing would make me happier than just having many more sessions.
But I suspect that there's other voices in the class that might disagree with that suggestion.
The other suggestion that was also interesting that we've discussed at length in the department that might change in math.
Twenty two in the future is whether there should be a mandatory section for this class, which a number of students suggested currently.
The I would say that that's not in the cards, certainly won't change for class this semester or next semester.
And the main way that I'm trying to provide the same resources that you would have with a mandatory section is to have a bunch of optional sections.
So the sections for this class are really the problem sessions. Caleb's problem session on Thursday, Kevans on Sunday, seeings on Thursday.
And I think those serve the purpose of just another place to see more problem solved.
Unfortunately, those are a number of suggestions that I can't implement.
There were a bunch of suggestions that I think we can implement and we will try to do so.
We'll do some amount of load balancing to do with office hours and problem sessions
to try to move them to times that it seems like people can make more use of.
And one additional thing that students asked for was just more problems.
So you might have noticed, I assume, that they did not mean more required problems.
But I'm certainly happy to provide many more problems that you can just use to get more practice.
So I've started adding recommended computational problems to the piece that's to kind of give you more practice with computational problems.
I've also started adding more proof problems to or problems that I would consider to be at level questions to the handouts,
which some days we'll get to in class. Other days I'm really just putting them on there for your own benefit.
So a number of suggestions, I think, are really great and we'll try to implement as many of them as we can,
and I do appreciate very much the the feedback. So thanks for doing that.
Are there any questions before we get started? Does anyone want to more emphatically ask for more homework problems?
So if you recall what we've been doing over the last few classes or last class, really is that we've done.
Oh yes, go ahead, Tommy. So quiz for we'll go back to the beginning of inverses,
so the last quiz really only went through the definition of what it meant to be a vertical matrix.
So like everything in Chapter two and Chapter three should be there. I believe that goes back to October 4th.
And in terms of the end of the material, probably Friday is class.
So I think that was, what, the 18th? So the 4th through the 18th.
So today's class won't be on the quiz.
The style of reasoning that we're doing here is certainly useful, but I don't think it needs to necessarily be on the quiz.
Oh, I guess. I mean, is today the 18th really mixed up then?
I'm sorry. So, I mean, not Friday is class, so the very beginning of vector spaces.
So you should know what a subspace is. You should really do some basic things with sub spaces.
You should tell me if something is a subspace. Not much more beyond that.
Sorry about that. I have small children at home, so I'm very sleep deprived, so memory formation, as all of you know,
from being sleep deprived, memory information gets harder when you don't get enough sleep. But let's dove into some math.
So if you think back to what's the big idea for what we're doing today and over the next week and a half or two weeks
or so is that we want to generalize all of the ideas from the first three chapters of this course to a wider context.
So the way that we're going to do that is we realized R.N. has these wonderful properties and we're going to say,
let's just study any object that has those wonderful properties.
So we give a name to objects that have those properties, namely when we started calling them vector spaces.
So we wanted to start understanding what we could say about vector spaces and
to be able to sort of drag along all of those ideas that we had from before.
So just to make sure that we're all. In agreement on what the goal is, the goal is that we want to extend our tools, our toolkit from our own.
Too abstract vector spaces.
So one thing that you might have noticed that mathematicians like to do is that when we have a good idea or someone has a good idea,
we like to make that thing the object of study. So here, instead of studying our end, we said, well, what made that a good idea?
Now let's just name every object that has those properties,
something we call them a vector space, and then we prove things about the vector space instead.
So we don't need to individually prove all these results for the space of and by and matrices or the space of polynomials or the space of functions.
You can just say, because they're like AURIN, many of our results will carry forward.
So that's what our goal is. That's what we're shooting for. That's our aim.
Uh. Well, if you then think back to the little bit we got to last time was we just started developing this library of examples of
vector spaces and the way that we wanted to understand those vector spaces was in terms of their constituent structure,
like what kind of sub spaces do you have within them? So when we think about R n, we had lots of really great sub spaces.
When we're thinking about lines and planes in our N, we had this nice geometric way of viewing that.
So we gave then also a precise notion of what we meant by then a subspace inside of a vector space.
Again, just trying to abstract away what we meant by a line going through the origin or plane going through the origin inside of our theory.
So maybe just as a bit of review at the beginning, let's just recall what we meant.
So a sub is. Of the age of a vector space V.
Is a subset. Such that the following properties hold.
The first property we want. H.
It to be closed under vector addition, so if you had two vectors inside of each and some of those two vectors.
We'll also be inside of age being careful that our sum is now defined by the operation
on we and then simply we would like to be closed under scalar multiplication.
So if C is one of our scalars. So in this case, a real number and use some vector in H, then C times, you should also be inside of H.
So that's just what we meant by a subspace. OK, but if you think back to the intuition we want to have is some spaces,
we're supposed to be like lines and planes going through the origin inside of our three.
So certainly in those cases we have the zero vector.
If you added two vectors on a line going through the origin and our three,
you would get another vector going through that same line on that same line in our three.
And if you scaled a vector on that line, you just get another vector on that line so you would get those properties.
So if we thought about how we actually described those sub spaces inside of our three or knw,
what was the most common way that we described a subspace, how did we think about it?
How did we describe it, subspace, what do we often try to do in order to say what the subspace is?
So if you're thinking about a plane. In our three, how what were the ways that we could think about a plane?
Savir. This a span of some number of vectors in the space, right, so we tried to get a spanning set for that particular collection in our end,
we would like to then do the same thing here because we have a vector addition and scalar multiplication.
It makes sense to define the span. So let's just do that.
So in this case. If you're given, say, some Vector's V one through VPE as elements in your vector space,
well, then we can now define the span of these vectors in the abstract vector space.
We're really to be essentially the same thing we had before,
it can be linear combinations of one through VPE, so see one v one plus dot, dot, dot plus c.p VPI where?
See, one up there are elements in your scalar field.
So it's the same definition we had before, but now the thing that's a little bit more complicated as our vectors could be like two by two matrices,
they could be functions, they could be a little bit more general objects.
Yes, there are. One of these.
So when we talk about what we want that to mean, well, we can prove that that will represent the additive inverse element.
So we can from the vector space axioms will say that that will perform the role of our additive inverse.
From the vector space axiom, so you and I have done a bunch of those together by email, so let's let's follow on by email.
So here, when we're thinking about this, this definition of the span is.
The first claim that we would want to do is to make sure that this thing was actually a subspace so the span of one.
It is a subspace. Yes.
Yes, so a subspace implies some additional properties, a subset just means that you're contained inside of the other thing.
So it's like a could be a subset of B and not necessarily be a subspace.
So one quick example. Maybe you're thinking about inside of R2. I could take, say, the unit circle and our two.
This is a subset, but not a subspace.
That would be one example for one, it doesn't contain the origin if you took any line in our two that doesn't go through the origin,
that would also be a subset, but not a subspace. So the subspace implies some additional structure beyond being a subset.
All right, so what we want to be able to do here is we want to be able to prove this from the definitions.
So we need to check these three axioms, so the first thing I would want to do is to show that the zero vector is in here.
How can I show that the zero vector is in here? Quen.
If I scale all the vectors by zero, so if I take zero vector inside of me, this will be equal to zero times the one that I thought to zero times VPI.
This is now a particular linear combination of those vectors. So now this is an element in the span of one through VPE.
OK, so we've checked the first axiom, we've shown how you can represent the zero vector inside if it works the same way as before.
So that's a good start. All right. So now to prove.
So this is a proof of one. Now, let's prove to. Take two arbitrary factors, you and the inside of the span.
Yes. With the operations that you've defined with the.
Yes, or scalars are always the real numbers, so that has to be defined for all real numbers.
So we have to define what the zero real number would be applied to a vector from the vector space axioms.
You can prove that zero applied to any vector will then have to give you your space.
So that's a great question. I mean, there are a lot of these like really great questions,
like Jonathan is bringing up that we can work through from the definitions of a vector space.
And those are good exercises to do.
Certainly they're great practice for like a quiz or an exam, but we certainly can't do all of them together in class.
OK, Quinn. We can limit.
For us are real numbers or their complex numbers, except for this one exception on your piece at.
Yeah, so now you're taking you're taking a very important field of two elements, zero and one.
So it's actually even a little bit nicer there because they're not an infinite number of scalars to check.
There's just exactly two scalars that you need to worry about.
So you only need to worry about scaling by the zero element there and the one element there,
you don't need to worry about scaling by any possible real number at all. So it means that proof by cases works a lot better there.
Early, Demidov. In fact, in computer science, that's one of the most common ones you might want to do to represent zero being off and one being on.
So, yeah, that's a very important vector space. So you can certainly change your scalars.
You just want them to be a scalar field. So good questions. So here now, what could I do if I wanted to then prove this?
It's a great quiz question. For maybe not this quiz, but maybe a following quiz, we.
Perfect right to use in here, so just use the definition of being in the span, so then, you know, there are some scalars see one through.
So this is true. Then you just need some other scalars.
So say these one for you, one plus DPE, VPE or scalars C one up there, S.P. Real's and D one up through DPE in the reals.
That's the definition of being in this set that we described. Then I think Zoe's next idea was now just add them.
Seems like a good, good idea. So then we have you, plus we will be equal to see one, the one plus do you one, the one that DPE.
All right, Vector Space Axium two I think was that I could commute vector addition so I could commute all of
these terms involving the one past all of these terms involving PvP to put them next to one another.
Similarly, I could do that with all the others,
another vector space axium so that I could factor out the vector from the two scalars and then write this as a sum.
So then just using those two vector space axioms I get C one plus D one times V one plus DataDot plus C.p plus DPE three.
And now what do you notice about the thing that I've just written here. So this will be by vector space axioms.
Yes, Tommy. And that's why I say to I wouldn't.
It's it's a good exercise for you to work through yourself to make sure that you understand which where you're using, which one at which place,
but I mean, like for me on an exam or a quiz or something, like, I don't want to penalize you because, like, you misremembered one of the axioms.
I mean, like you said three when you meant for something. I mean, that's just too much precision in your memory.
That seems unnecessary. And the numbering is sort of arbitrary anyway.
There's no, like, canonical ordering. They have to come in. So, no, I wouldn't need to see exactly which ones.
I mean, maybe it's a reasonable question on a quiz or an exam to say, like, here's the list of the ten axioms.
Show me where you use each one in this argument or something. I don't particularly like that question either.
It's probably not going to do it, but nevertheless, I could, but.
So then here at this stage, we've now just written this as a linear combination,
so then it is in the span of one through VPI, so hence it's closed under vector addition.
So, Hant. Two old.
Let's just do Prop. three really quickly. Same idea. So we take to be some scalar.
Well, then, if I take three times, you well, then just distribute the sea through the sum so that I never see time,
see one v one start at that time, S.P. VPE.
Well, again, now that is an element that's a linear combination of the one BP.
So that is a subspace. There are a few things to take away from this,
an important thing to take away from this is just how you prove something to subspace by using the axioms.
That's something that I think you're all getting comfortable doing alarming,
something you're all getting to it, you're comfortable doing through the problem set. And we'll do more examples of doing that style of reasoning.
The second take away from this as it gives us a second method for showing some things that you can now use,
either these three axioms or you could prove give a spanning set.
Once you've written it as a spanning set, then you can say, well, we've shown every spanning.
That is a subspace, so you're done. So that gives you two ways of approaching the proof of something being subspace.
So we're starting to build up a bit of theory. All right, so one of our fundamental.
Observations in. OK, I'll I'll do one of one of the comments that I think was quite good on the early course,
feedback was just asking for a little bit more computational examples, a little bit more that type questions or web work style questions in class.
So I don't want to quite skip this problem. So let me give you a concrete example of doing this.
So let me take some vectors here, let me take the Vector V and this is all going to be in the space of polynomials of degree two or less.
So V is equal to one plus two T plus T squared.
I'll take W to be the vector one or the polynomial or the vector one plus T squared, and I'll let you be the vector T plus T squared.
So this is different than the one that I wrote on the handout last time. It's not literally the same one.
I think this one is maybe better so but the same problem.
So now question a nice web work or quiz computational question.
I'd like to know whether say V is in the span of you and W.
Is there an element in the span of You N.W.?
OK, so we have to pass through the definitions here, it's not exactly the same as what we've done before,
so we really need to make sure that we're paying careful attention to what the definitions say.
Well, for you to be in this set, that would mean we want to find, if possible, to Scalars, C one and C two in the real's so that.
This Vector V is equal to see one Hymes you see two times.
Oh, OK. Well now let's just plug in what everything is.
That means you want one plus two t t squared to be equal to see one times T plus T squared plus C, two times one plus two squared.
Right. So we're now we're just asking whether two polynomials could be equal.
So we certainly a question we can do so one plus two T plus T squared.
Let's look at the coefficient of one here. So I have C two times one plus now C one times T plus one plus C,
two Times Square two polynomials are equal if and only if they're coefficients are equal.
So this then gives me a system of equations, namely it gives me the equation.
One needs to be able to see two. One needs to be equal to see one.
Thank you to and to be able to see one, two.
So what I've done is that I've read I've translated this problem into a statement about a linear system of equations,
so it's now n exactly the same sort of thing we've been doing from the beginning of the semester.
So you could form the augmented matrix and reduce and do our whole algorithm if you want, in elimination.
But can you just tell me immediately from this system of equations? It's no solution, right, so there's no solution to this, it's inconsistent.
If you formed the augmented matrix, you would get a row of zeros followed by something that's non-zero in rightmost augmented column.
So no solution. So and hence there is no coefficients.
See one and see two here that would make the linear combination of you and w so then you is not an element in the span.
Of these two polynomials. V, I'm sorry, V.
You. So the basic idea of doing all these sorts of problems, though, will be exactly the same.
We sort of work with our new definitions to translate the question into something we've done before to get a linear system of equations.
All right. So that's a computational level question.
So something that could be a nice question, too, on a quiz, for instance, to give more practice with a quiz or a piece at level questions.
Let's think about a little bit of a harder problem of how we can get new vectors
from old vector spaces or how we can get subspace from other sub spaces.
So the second question that I had for you in the last handout, James. That's a great question.
We haven't proven that yet, so that's open, but we'll we'll show how to get that.
So we we definitely that's not clear from what we've done so far, but that's that's a good question.
Certainly something for us to build towards. Yes. Yup, that's.
Have the first. Safak. By their own admission, in.
Yes, so that there's some spaces so that then we can actually use this to use the results we're developing off the top of your head,
like we just like maybe polynomial vectors, you know, we're going to at.
Well, we've given some examples of some spaces that don't happen to be vector spaces,
so like the simplest way to get things that don't work would be just take something that doesn't contain the origin,
like an R.N. and that would give you a problem. So certainly not everything.
Good questions. OK, so the second type of problem that I'd like to think about here is suppose we
have to subspace this w one and two are sub spaces of some fixed vector space V.
We thought we would just like to know what we can say about these vector spaces so we have their sets, there's subsets of.
And we then have set operations that we can apply on these to try to get new vector spaces or new sub spaces,
so in this case, the two questions to consider.
First, let's consider the intersection of the two. Would this thing be a subspace?
So why would this be a subspace? W w w e.
You know, Daniel. Right.
So in this case, it will work out. So if we have something that's in the intersection, it satisfies the conditions of both of them.
So if you take a sum of two things, it's in both.
Well, then by the closure under vector addition, for one, the sum would have to be in one by the closure of W to the someone else after two.
Therefore it's in the intersection. So you could go through the axioms and verify it.
So this is. It is a subspace.
Checking these three accidents in this case, I think would be a certainly not a quiz problem for a quiz on Friday,
but a quiz five problem would be totally reasonable. But the second.
Operation. What about this one, taking a union instead of an intersection?
Yeah, it seems like there might be something wrong here, so if you were doing this is like approve, give a counterexample on a quiz or an exam,
you might start writing, approve, and then you might get into exactly the problem that you're describing.
When you try to consider the sum, then that suggests to me that I should go back to the drawing board and consider a specific example.
So the easiest place to probably think about examples would be an or two.
So let's take two sub spaces in our two. So let's take, for instance, V to be R two.
I'm going to take W one to be the X axis. So it'll be the span of the single vector one zero.
So this is the x axis. And then let's make our life easy, so we'll take two to be the spane.
Of zero one, this is been the Y-axis. So geometrically, we can view these two faces in the following way again.
It's important for me to use the full range of my technological abilities here.
Orange toxo, that's the. I think it's especially amusing.
I mean, I was a programmer for three years in my life before going to grad school and now this is my technology, but.
Using more technology. So there's two.
And there is one. And now perhaps I'm making too much of a fuss about this, but then as Arjun pointed out,
if I take this vector one zero and this other vector here zero one and I add them together,
I get this vector here one one, which is most certainly not in the union.
So it's not closed under tradition. It's not closed.
All right, so I think those are nice practice problems.
So what I want to do now is to really get into how we can get at some sort of canonical spaces from what we've been doing before.
So we've seen already that inside of our end we can take the span of any collection of vectors and get a subspace.
But there are lots of other sub spaces that we've already been working with before.
So I'd like to give a moment to come back and appreciate those.
So. One of the first places where we thought about getting at a spanning set was the described solution sets.
So that suggests that we go back to the homogeneous system of equations and we
study that set and think about that collection that will again be a subspace.
So let's give a name to it that the null space of a given matrix.
So the null space. Ove and Ambi and Matrix A.
Is, I'll even say denoted by. Well, no, I'll just write it this way is given by.
So the two bits of notation we have for this is either end of your matrix or sometimes people like to spell out the word and U.
L l of a. For some reason, some authors will also write new L of A, which is one L.
You might see all these bits of notation I and the rest of the math.
Twenty two team won't be confused from seeing any of these different ways of expressing it.
So don't worry, if you want to use a different one, just make it clear what you're writing.
So what this is, is it's just a set of vectors x in our N such that a kind of X is equal to zero.
So it's just a set of solutions to the homogeneous system of equations. That's why we're calling them the null space of a given matrix.
So our first theorem that about null spaces will be in this new language.
It's the subspace. Again, a nice problem that could have been a peace question, so claim her theorem.
I think your book calls this a theorem, the null space of a matrix. And it is a subspace.
What did a subspace of. Perfect webspace of our end, so it's a collection of things inside of our end.
All right, so let's prove it just to kind of get some exercise, our perforating muscles here.
So the first thing to observe is that by definition, I'm taking this set of things in our N such that some condition is satisfied.
So therefore, just by what set builder notation means, this is a subset.
So note the null space is defined to be a subspace subset of our.
So all we need to do is check these three axioms, so condition one, we need to check the zero vector in here.
So since a time zero is equal to zero, that's something that we proved what matrix multiplication very early on.
And we know that zero is an element in the null space.
OK. Because we're just checking that condition to suppose.
You and we are elements in the space of a and we would like to know what happens to the sun.
Well, we check a times you plus we distribute the aid through.
So we have a times you plus eight times. Well, this is in the null space.
So this goes to zero. This is in the null space. So that goes to zero.
So then this just becomes equal to zero. Then the sum is in the null space as well.
Uh. So thus.
You plus me, there's an element in the null space that. Finally, the third one.
Let's do it just for completeness. So now suppose. See some skill or some real number and you is an element in the null space.
Very well then the computation.
If we take a Tienes see you. Well, matrix multiplication is linear, so I can certainly pull it through.
So that becomes three times a you, which then becomes C times the zero vector, which is then zero, Jonathan.
If you really want to emphasize the like, certainly we're working in our RN here, so you could put an arrow over it if you want to.
I mean, if you want to really emphasize which zero vector it is, you could even write zero sub or NP to note how many zeros it has,
especially where there's a lot of vector spaces lurking around.
It's really helpful to use this notation to indicate which zero vector are you talking about,
because there will be one for every vector space that you're studying, right?
So if you want to do this as well. I mean, oftentimes this notation is suppressed because it can become very cumbersome,
but if there are a lot of vector spaces lurking around and there is a possibility for confusion, then I would include it.
So remember, when we're writing a prove, there are sort of two fundamental goals in writing that prove.
One is we want to establish the true value of the statement that you're studying.
And second, you want to communicate that idea. OK, so you're not writing for a computer to check your proof?
We're not writing formal checkable proofs here. You're we're writing for humans to read to.
OK, so there is a little bit of room for including some indication of strategy for
what's going on for making your notation is nicely adapted for your readers.
This will be especially important as we start thinking about the final project for this class, because as I mentioned to many of you individually,
when you're writing your final project, you're not writing for me or any of the two sources you're writing for your fellow students in the class.
OK, so that's the audience that you should have in mind. So that's.
The null space today is a subspace.
OK, great. Are all solutions, that's.
Subspace is. Are all solutions at such places?
So if I study the set of vectors that solve the equation, X equals B, will that be a subspace?
Yes. Can you say a little louder?
That's right, so it would not if you're solving an equation of the form X equals B, where B is non-zero.
OK, so if you consider this is the second question on your hand. OK, so on the other hand.
If B is some non-zero vector, then the set of X in our ND such that A X equals B is not a subspace.
And the reason, of course, is that it doesn't contain zero. All right.
So that's one sort of fundamental subspace that we work with a lot here is the null space.
If we go back to a matrix, though, there's another subspace that we could certainly consider.
Oh, I think I did want to actually do a computation for you, so let's let's quickly actually computer null space.
So this is the next example on your hand out.
If we had a little bit more time and pause and let you do this computation, maybe you've already done it.
Yes. Or. Put it back up, you want to see it?
Yeah. No problem. I was probably too quick on that one.
OK, let me just quickly do an example of actually finding a null space, so this is a computation that you've actually done quite a lot before.
So let's just take a could be reasonably sized matrix, one minus four zero to zero.
Zero zero one minus five zero zero zero zero zero two.
So now the prompt is I want you to find the null space. So luckily, my matrix is very early and reduced to Echelon form.
So that makes it simple for us to read off what the solutions are. So we're trying to solve the Matrix equation X equals zero.
So let's just do that really quickly. So that means that X Factor X is an element in the null space, where does X live?
Our five. Great. So this is in the space of a if and only if.
I have some conditions on satisfied, so namely I have X one minus four, x two plus two, x one, two, three, four is equal to zero.
And then I have X three minus X four.
Is equal to zero, and then my last one, two x five is equal to zero.
So those are the conditions that you need to satisfy. Is it OK now if I lower it?
Yep, know from. So that means that X is an element in the null space of a.
If and only if your Vector X is equal to well, if I solve for X one to be four x two.
Minus two x four. Then X2 is free, then X three is five, X for, then X for an X five is equal to zero.
So then I could say this is true if and only if X is an element in the span of these two vectors, then what are they for one zero one zero zero zero?
And then my other vector is the coefficient of X for so minus two zero five one zero.
So that tells me exactly what the null space is, then the null space is just equal to this span because it's only a statement the entire way or.
The span is always a subspace yeah. That's a good point.
All right. So the other way that we talk about a Matrix equation or one of the other primary ways that
we thought about a Matrix equation was to to think about it as a matrix transformation.
So in the case of this matrix transformation, what do you get?
So what is this kind of transformation doing? Where does it live? What's the QUARTERMAIN?
What's the Kotomi in here? There are three, right, the domain is our five, the domain would be our three.
So we know that the range or the image of the corresponding matrix transformation will be some subset of our three.
How could you figure out exactly what it is? How would you figure out exactly what it is?
Well, I. That would certainly work, that's a great way of doing it.
How else might you do it? Mm hmm.
So that would be another way that we could certainly do it is we could think about just what this linear transformation does to like E one,
two and three, for instance, like our four and five.
So all of those domain variables or domain vectors, Xs.
What does it mean if I take a time to Vector X, what am I doing with these columns?
Arjun. So I'm performing a linear combination of those columns,
so then one way that I can read off from the beginning is that the image of that
transformation is then going to be the span of the columns it will give you.
The same answer is if you augmented by a general vector and actually solve for it, as we've been doing before.
But in this case, we can actually do this in a simpler way that subspace,
because it's the span, it will be a subspace we call the column space of a given matrix.
So note definition. The column space.
Animatrix. Uh, I should say a matrix.
Uh. Is the span of the columns.
So that's what we mean by the column space of a matrix, so in this case,
we can actually find the column space, your matrix with the Nope by C o l of A.
So this will be equal to the span of all of those columns. So I have one zero zero with my next column.
Is that a negative four. Negative four zero zero.
My third column is then zero one zero. My fourth column is too negative, five zero.
And then my fifth column, the last one zero zero two. OK, so it's the spane, can you tell me about all these columns that I've written down?
Zoe. They're literally dependent, right?
This is a very redundant list, I don't need all of these, for instance,
adding in this negative four zero zero once you already have one zero zero, really added nothing.
So I can certainly delete that one. Adding zero one zero does add something.
But then once I have one zero zero zero one zero to negative five zero can be obtained from those.
So I can certainly delete these two factors. So it's worth noting this thing does span all of our three.
So what do we usually call a matrix transmission of this kind?
What we usually call it KeySpan, the QUARTERMAIN, so it's subjective is this thing or interactive?
So someone said, no, why is it not? Why is it not interactive, really?
There's multiple solutions, they will zero right there.
There's more than one thing that goes to zero, so if the null space gets big, that's telling you about the failure of injectivity.
In some sense, the null space is measuring the failure of injectivity. So you want the null space to be very small in order to be injected.
So, again, these sub spaces are telling us about what we had before as well.
So they're very useful. So here I have this column space.
I should remark that it will it's defined in terms of being a span.
So we should note that the subspace. So maybe they should more properly be called the corollary, the column space of a matrix is also a subspace.
And what is this is subspace of. The subspace of our GNH or is it a subspace of our in the case of an MBA and Matrix?
What am I right? OK, so the picture that you should have in mind is we have our domain copy of our KNW, we multiply by our matrix,
our M by N Matrix will output things in the domain of our M somewhere inside of this, you have the zero vector.
Things that get map to the zero vector will be your null space.
Gets sent to zero. On the other hand, if I want to find what the column space is,
the column space will be taking a applied to the AMANE, which will, of course, contain zero.
So that is the column space of air.
So when you're thinking about the null space in the column space, they live in different places.
The null space is a subspace of the domain. The column space is a subspace of the domain.
OK, so this was our three over here and this was our five.
It's perhaps interesting when you have a square matrix, so they'll both be out in.
Great. Oh, good, good, good, good.
I'm used to looking up at the clock and being alarmed, and today it's a pleasant surprise.
Maybe it's the good luck that I've gotten from the student that suggested that I make the classes longer.
I appreciate the suggestion. All right.
So the key thing here, if you will call back, it's easy to get lost in the weeds when we're doing this.
So let's try to regain the overall thread. The overall thread of what we're doing is we're trying to generalize things to abstract vector spaces.
Right. In this case, we took a brief diversion back to our GNR and we studied matrix transformations
and we realized in that case there were some nice subspace is for us to study,
namely the ones coming from the range of your matrix transformation and the ones coming from the null space,
the ones that go to zero of your matrix transformation.
So it suggests that we try to generalize these sub spaces now to the case of an abstract vector space.
So what I need in order to do that is I need something like a that's going to be going between abstract vector spaces.
So the way I'm going to do that is I'm going to define what I mean to have a linear transformation from one vector space to another.
And then I can generalize these objects. So definition. So just like we had there, I now want to study a linear transformation between vector spaces.
We let V and W the vector spaces, not necessarily our M and our N.
So certainly their sets, so we could define a function going from one to the other,
but most functions won't have nice properties necessarily, they'll just be a random function.
So we want this function to respect the vector space structure.
So that means it should play nicely with both vector addition and scalar multiplication, as it did in the case of Orient.
So then we want this to be true such that, well, what does it mean to respect vector addition?
It means that if I took t applied to a vector V to W,
that it wouldn't matter if I first added the vectors and then applied T or if I
applied T to each vector and then added them in the corresponding output space.
What you'll notice was exactly our condition for linearity before we're abstracting
out this fundamental property of what a linear transformation was from our interim.
But now between vector spaces, the interesting thing to think about here is where is this addition coming taking place?
Where is this addition taking place? Nothing here is in our in.
It's envy, right? So let's be really, really careful, this is addition in V,
so you will actually see this notation sometimes if the operation that you're using, there are a lot of vector spaces lurking around.
You want to emphasize which operation it is. This is the addition in V.
What about this addition? Where is this taking place?
And W. So this is Ed. This is what I mean by respecting vector addition, I could first add the vectors and then apply it,
or I could apply T to each vector and then add them in the code domain.
Both will work and give you the same answer for linear transformation. We also want to respect scalar multiplication times.
You should be able to see times to give you. And that should be true for all to see scalars, and you can be in my domain vector space.
Yes, Xavier. Earlier. Spaces.
Any vector space, any vector space whatsoever, if I take the span of elements inside of that vector space, then I'll be getting a subspace.
They know they use the vector operations inside of us, so that's a really actually an interesting point.
I like that question, let me explore that a little bit more so what I'm thinking about the span of one VPI,
these things are inside of my vector space, right. So this will then be equal to see one V one.
So this scalar multiplication is occurring inside a V and then I'm adding A, B, C to be two plus the vector addition inside of C, P, VPI.
So all of these vector sums and scalar modifications are occurring inside of your vector space.
So they might be these nonstandard operations. They don't necessarily have to be the ones coming from our end.
So it's a great point. So let's here, if it's hopefully clear from context what the addition should be and we usually don't make a big fuss about it.
Yes. Oh, sorry.
That was thank you. Thank you. Good good catch. Good catch.
Yeah, that wouldn't make sense at all. There's no way for us to do that.
They have to be our base scalar field because there's no way to multiply two vectors together necessarily.
I mean, our vector space wasn't defined that way. You could a new kind of structure where you have a multiplication and that's a nice project idea.
I mean, certainly those structures show up and have nice applications, but it's not a vector space.
So the integers are an interesting idea here,
because the integers then we we don't have multiplicative inverse is there versus over the real numbers we do.
So we don't usually take the integers. Other questions, those are great questions.
All right, so thinking back to this setting, we had these two subspecies coming from a matrix transformation, we had the range or the image.
Well, that will work just the same way.
We now could take the range of an image of a linear transformation so we could take the range of T this will be a subspace.
Is this a subspace of V or W? What is it?
Where does it live? Derby, right?
It's in the domain, we also take the set of elements in the domain that get sent to zero.
This is usually called the kernel of transformation. So the colonel.
Of tea, sometimes written k e r of tea is then just by defined to be the set of all V in,
we use a different letter, the set of all X in V such that T of X is equal to zero.
So it's just like the null space, the case of the null space at exactly corresponds.
The difference is that we talk about the null space of A that's only applied to a matrix,
the kernel of T is now a notion that can apply in more generality. So it can apply I can compute the kernel for any linear transformation whatsoever.
Jonathan. Yes. Other questions.
Tommy. So, yes, we're trying to generalize what we did with the space to get out with the kernel of a linear transformation would be.
So if you had your tea to be equal to the linear transformation from our end to R.M., then the kernel would be the space of the associated matrix.
So, yeah. All right, so we have these two new sub spaces.
If you're a linear transformation t was injected, would you expect the kernel would be big or small?
Small, right,
it's measuring the amount of stuff it's sent to zero if a lot of stuff gets sent to zero and we're seeing the failure of injectivity again.
All right,
so what I want to do for my last little bit of class today is I want to just get through as many examples as I possibly can in the next 12 minutes.
So we've really accomplished all of the learning objectives that I've set out for you today.
This is just me trying to present problems that are like peace problems, that are computations that are fun to play with.
We'll see lots of these examples building over the next few classes in both sophistication and abstraction.
So it's nice to play with them early on.
So if you look at the handout, I think I have three or four more problems, they're all basically of the same variety.
They're good ones to practice on. So let's just try one now.
So let's take a transformation t going from some vector spaces that we're not used to working with,
so I'm going to take the space of polynomials of degree and or less than.
And I'm going to send this to the space of polynomials of degree and minus one or less and minus one.
So this is supposed to be a function that takes a function as an input and an output is another function.
So it takes a polynomial. It outputs a polynomial.
So where this is going to be given by T of the input polynomial T, we'll just go to the derivative of your polynomial.
OK, so what this does is often called a linear operator going from one space to another.
So the inputs are functions are polynomials, and I'm going to output something by differentiating it.
So this gives us perhaps a different perspective on an old friend, the derivative.
So the first thing we should think about is this, this function going to be linear or not, we have a precise definition of what linear means.
This is a vector space. This is a vector space. This is a function.
So we can ask whether it's going to be linear or not. So our definition of linear means we have to check two things.
Let's see if it works. So now KWEM.
To use linear. So now let's prove it.
So to prove it, we take some arbitrary vectors in our vector space. Let's take polynomials P and Q So to be polynomials of degree and or less.
Now, let's plug them into our function.
Be applied to the sum of these two polynomials, P and Q well, that says just differentiate these two polynomials, the some of them.
Well, going back to Calc one, we have some rule for differentiation, so this becomes the some of the derivatives.
So this is the same roll back from one. So then we've just verified.
Well, one more step, I suppose this is equal to T of P plus T of Q, so there we've checked that it respects tradition.
All right, let's check scalar multiplication. Scalar multiplication.
So now let's see if your real number. And he b, some polynomial.
Well, that if I take t applied to the polynomial scale by C.
Well, this just says take the derivative of the scalar multiple times, this polynomial sees a scalar.
So by the constant multiple rule from Calc one, this is C times the derivative of P.
Which is Sea Times T applied to P, so here we use the constant multiple rule.
From Callick Lanegan.
So then the derivative is a linear transformation between vector spaces where the vector spaces are now, the spaces of polynomials.
So this is giving us now a different perspective on calculus, which is a very powerful one.
Uh. OK, but for a linear transformation, we had these funny spaces that we attached to your transformation,
if you just thought about this function from one space to another. Do you expect that it's going to be injected?
Why not? So that would mean you'd have to give me two elements of the domain, the map to the same thing in the code domain,
what would be two polynomials that map to the same thing under this function, James?
Different constants, right?
If I take the polynomial constantly equal to one, the polynomial console equal to to differentiate both of them, they both become zero.
So this function definitely wouldn't be in a. But we can compute the colonel if you want and we can compute the image.
So let's actually do that. All right.
So we know that he is linear, let's find the colonel, let's find the, um, image.
All right, so let's suppose we have being some polynomial in the space of the pan,
so polynomial degree and or less so that means that P of T will be equal to ANot plus a one T was a two T squared plus that of two and T to the end.
All right, that's just what my general polynomial degree analyst looks like,
it's also a nice, true or false question that I've used on quizzes before.
If you add to polynomials a degree and together, do you necessarily get another polynomial degree?
And that's one to think through. So here degree and or less.
All right, so now let's think about what he does to this.
OK, well, if I differentiate this polynomial, I know it's linear now, so I can certainly just apply to the mills.
So when I differentiate the constant, that term drops out.
So I get a one from this term, but then I get plus two, a two from the power rule times T plus DataDot up to ten times A and T to the end minus one.
So we can right there see the element in our co domain. Because it's of degree and minus degree and minus one or less.
So now. So T of P is equal to zero if and only so now this is the zero in my Kotomi in space, so that's the zero polynomial.
So if you're the zero polynomial, what needs to be true about all your coefficients? They all have to be equal to zero.
So that means we get. One is equal to zero to a two is equal to zero down to 10 times A and is equal to zero.
So those are all my coefficients then have to be equal to zero. So what's left of all of those things have to be equal to zero.
What's left? Nothing is not an artist left, right, so an art is then free.
So all the other variables are basic or are determined, rather, and this variable and not is then a free variable.
That one we actually can plug in anything we want.
So then we can even say ust p e is an element in the kernel t if and only if p o t is equal to a not.
It's just equal to a real number, it's just a constant function. So that tells us that the kernel is just equal to.
All constant functions, so the kernel of tea is equal to the set of all polynomials.
I'll just read it this way. All right. This is equal to a not whether or not there's an element in the real's.
Well, this thing is just an at times one, so you could also rewrite that as the span of the single polynomial one.
So the colonel is just the span of the constant polynomial, one, every other constant polynomial would be a multiple of this one.
So this is then encoding that first observation,
I think that James made the failure of injectivity because our colonel is big, having multiple things in here, Colonel,
is observing the failure of injectivity for your transformation,
which tells us back again to something that we've known for a long time about differentiation,
that there are lots of functions that could share the same derivative. Any questions on that?
So, again, it's kind of a different way of thinking, Marco, when you say the.
Like the doctors there, like. Yes, I mean, nontrivial, and it is still big in the sense that there aren't really many things in it,
so it would still it would still fail to be objective.
That's right. All right, let's see, what about the image of this linear transformation or the range, what would you tell me about the image range?
Can I get any polynomial degree and minus one or less? Jonathan?
Yeah, how? The.
A. OK.
Sort of an awkward angle, Jonathan, can you see this? I like I know.
OK, yeah, kind of cut off at the angle there. Can you tell me what the input would be?
What input could I put into this to get that output? And.
This is a not yeah. One of the.
My guess is they it's not X plus one at.
Oh. Yet it's hard to see across the room, I really was trying to write it really big, but it's hard to see, especially with your angle there.
But so we want the power rule to come down to cancel all these terms out.
Right. So then when the power rule comes out, down with the two, it'll cancel with this, too.
And I can keep going all the way up just a. Differentiating so a nd minus one.
And then I want the end to cancel T to the end.
So now if I differentiate this thing, well then I get my energy. I wanted the 2s come down so I get a one t here.
If I keep going all the way up the end comes down councils with this n and then I'm left with A and minus one T to the end minus one.
So this shows me, given an arbitrary element in the code domain, I can find an element in the domain that maps to that.
So then T is subjective. Said another way that the range of T is equal to the span of not just one, but one T up to T to the end minus one.
So any polynomial of degree and minus one or less we can obtain as the output of the derivative here.
OK, so this thing will be actually subjective. Every possible polynomial degree and minus one the last is obtained.
All right. It looks like somehow I managed to go over time again. I apologize for that.
I'll make sure to give back that at some point this week. All right.
I'll see everyone on Wednesday. Mike, Mike, check.
