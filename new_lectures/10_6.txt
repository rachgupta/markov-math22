All right, let's go and get started. So just to quickly run through some announcements, so we're all on the same page with deadlines and so forth.
So the things we have coming up, the things we have coming up.
So problems at five. I didn't get through all the material I wanted to last class.
So as I mentioned during class move the deadline to Friday, that shouldn't be.
There's only a tiny bit that you need to finish that piece yet from the material from today.
Of course, if you were doing the reading ahead, you've already seen the theorem that you need to use.
But nevertheless, I thought it was only fair to make sure you had a bit more time to digest that material before the problem.
That was. Do we also have our third quiz on Friday?
So the same structure as all the other quizzes. This goes back to the material on linear transformations.
So the material, especially from the last exam, in principle, the quizzes can cover anything over the semester.
But practically speaking, it makes the most sense for us to emphasize the most recent material.
So thinking about everything from Monday's class back to the material from the midterm.
So that's quiz three. That'll be on Friday. So our usual structure, 15 minutes at the end of class.
Several students emailed saying that they thought that there were a lot of things due on Friday.
So I felt like that was a fair argument. I was convinced.
So I don't really if you're done with reflection, certainly turn them in.
But if you need more time to give a really fair, complete reflection, then certainly it's fine to take until Monday to do that.
So I extended the grade scope window for the midterm corrections and reflection until Monday.
Again, that's optional. Strongly recommended but not required. Problems at six will be due on Wednesday, sort of on our normal schedule.
I have not yet posted that to canvass, but probably around, say, 5:00 p.m. today.
I'll get it up. Are there any questions about logistics? Yes, Tommy.
So it's starting from material after the midterm, so the midterm essentially went through and the minimal way possible,
I went through linear transformations just in the fact that I asked you to define what a linear transformation was,
but basically everything else since then, I have not assessed you on.
So that's what the quiz will cover. So from inverse until Monday's class, really?
Yeah. Other questions, concerns, anything that I can help with at the moment.
Are we good at anything? OK, great, let's get going.
So we have a fair amount of material that I'd like to get through today, if at all possible,
but I also want to make sure we go through this at the appropriate pace to maximize understanding for the class.
So let's just take a moment to catch our breath.
From what we've been doing last class, what we've been doing is we've been thinking then about when a matrix,
an NBN matrix is going to be convertible. We had sort of those three fundamental questions after we defined what it meant to be inevitable,
namely that there exists some matrix C that when you multiply on the left and on the right, both result in giving you the identity matrix.
And then we started proving theorems about what that gave for us, what it told us about solving systems of equations,
what properties of inverse matrices we have for free coming from that set.
Namely, if you have a Matrix convertible, then the inverse matrix is also convertible.
The inverse of the inverse is the same matrix. Again, results like that.
Then what we started building towards was a simple class of matrices that we could easily invert, so then we could try to build from that.
And the idea there was that some matrices that are easy for us to invert,
how we respond to doing elementary row operations on, say, the identity matrix.
Those are matrices that we call elementary matrices. So the nice thing about those is that we can undo elementary wrong operations.
So then we can just do the corresponding matrix that will then undo that row operation to get the inverse of that.
So, for instance, if you want to undo the elemental operation coming from switching to Rose, we'll just switch them again.
And you get back to the original thing so we can write down the inverses of those.
So that led us to a way of thinking about convertibility, of a general matrix by thinking about it in terms of zero reduction.
So yet again, another problem in linear algebra that reduces to zero reduction.
So we were in the midst of proving the following theorem when I was running out of time.
So let me just go back to this theorem. So the theorem we wanted to prove was that A is convertible.
If and only if A is equivalent to the identity matrix,
so our probably our most important equivalence relation this semester is then thinking about equivalence.
So that gave us also a computational tool, an algorithm for actually computing an inverse as well.
So when we were doing this last time, we had already proved one direction of this.
We just had the one piece left.
So this direction of the proof from last time, we did actually manage to finish that one.
But we had this other statement of equivalence that we needed to handle.
So we need to prove right and left. So, again, we just want to work through the definitions.
So starting suppose we have a is equivalent to the end by an identity matrix?
Well, that means that there's a sequence of elementary row operations taking a to the identity matrix.
Our new way of interpreting those elementary operations is in terms of multiplying by elementary matrices.
So then that means that there exists.
A sequence of elementary matrices set one up through E. P. If you want some number of them, these are elementary matrices.
So that. Well, we start with our Matrix A and that would be equivalent to E one timezone,
which will be equivalent to E two times E one times a day, which will be equivalent to eventually up time down to E one times.
And the whole point is that once you've applied enough of these elementary operations, we get the identity matrix.
So that should be equal to the identity. So what we've done so far is we've established that there is a left inverse right.
So right here that I can multiply by some matrix, the product of all of these elementary matrices, multiply that by a to get the identity matrix.
So there's something I can multiply on the left to get the identity.
So I think, as David pointed out, last class.
Well, what about multiplying on the right? How do we know that that would work there?
Because the definition of inevitability meant it had to work on both sides.
The same matrix had to work on both sides. So here we need some way of reasoning about that.
So we're not quite done.
So we need to somehow work around that to recognize that whether a time's up through E one would also be equal to the identity matrix,
that's not something we've established yet.
So we want to think through how we could either work around that problem or prove that very result, so that's the thing that's missing.
Well, if you think about what we have here, so let's just take this last identity we have.
EPI down to E one times A is equal to the N by an identity matrix.
Well, let's think about what this means for a this is an equation, a matrix equation,
so I could then solve for a here so I can all of these elementary matrices by themselves are convertible.
That was an observation we made for any elementary operation we perform. There's a corresponding elementary rule operation to undo that operation.
So then I can multiply on the left by the inverse to get them onto the other side of the equation.
So then I would get well, multiplying.
Both sides on the left by Eppy have epi inverse times to say the end my own identity matrix down to one inverse.
So that's then what A is. This is of course just the identity matrix, so I don't need to keep writing it.
There won't do anything.
But now I've got the product of a bunch of inverses of matrices, but we've proven something about the product of a bunch of inverses before.
How could I rewrite that expression using the properties of inverses that we have from last class?
Let's be equal to. Arjun.
Right, because inverting the product reverses the order, so then moving them in just becomes this each of those products is again convertible.
Each of those factors is convertible because they're elementary matrices.
So that tells us, again, what a mess, but that also tells us that A is the inverse of some matrix.
So if A is itself the inverse of some matrix, that means that A is then inverted.
All right. So that's. We know from the properties of inverses.
From last class. That a inversed will then be equal to this thing inverted, so eppy down to one or inverse.
Inverse, so I'm just inverting this expression from the properties we observed last time, this is then just equal to e p e one.
So then the inverse is just what we are hoping for, namely the thing that we multiply on the left to get the identity matrix.
So the way that we're able to work around that property to get the other piece, that multiplying on the left will also be convertible.
Convertible was through using the properties of matrices. OK.
So that's. A is convertible.
And a universe is just given by EPI down to E one.
Tommy, we just want to play. Multiply the eight that we found here.
Yep.
Yeah, if you if you took this thing and you multiply it on the right and showed somehow that it's the identity matrix, that then you would be done.
Yes. Yes. It's a good question.
So you want to think back to why did the order change when we were thinking about taking the inverse of an expression,
maybe that's a worthwhile remark to go back to. So recall the following.
So if we wanted a inverse, this is equal to B inverse A Andrius, your question was why?
So let's think about why for a second. So I want a matrix ab that when I multiply it by this thing, I'm supposed to get the identity matrix.
This is what we want. So if I plug in the inverse, a Anvers here by associativity B times, B inverse becomes the identity matrix.
So then then the middle terms will go away. Then I'm left with eight times a inverse, which then becomes the identity matrix.
So the order switches so that they will cancel out.
If this were A and B inverse,
I would have no way of combining these together to move them past each other because matrix multiplication and in general is not commutative.
Does that answer your question and the same thing is true in the other order, if I take the inverse, a inverse times, Abe?
Well, again, this is the inverse times A Anvers, A and B, and then, hey, great, that's the identity matrix.
So this becomes B, inverse times B, which that becomes the identity matrix again.
So then we have it. So the reason why the order switches is to get those products to cancel out.
Other questions. So the other key takeaway from this is that this sequence of elementary row operations applied to a gives me the identity matrix.
Well, we can also think about what would happen if you did the same sequence of RO operations to the identity matrix itself, then what would happen?
Well, if I thought about this expression right here, I know that I can multiply.
If I want to multiply the identity matrix now by each of these, I would like to see what happens for that particular case.
Well, if I multiply A by all of those were operations, I get the identity matrix.
So then you'd want to think about if I just took Eppy through it one times.
The identity matrix. That's the inverse. So note. The key observation, a inverse is equal to epi, down to E one applied to the identity matrix.
So all this is doing is it's taking the identity matrix, it's applying the same rope first rule operation you did to a then the same second one,
the same third one, the same fourth one to the fourth one to then get a inverse.
So this observation justifies the remark that if I take a and augment by the end by an identity matrix,
when I reduce this to the identity matrix that's applying the same row operations to the identity matrix, which then gives me an inverse over here.
So this is the remark that justifies why doing the same operations to the identity matrix produces the inverse matrix for us.
Yes. Hmm.
So the reason why we're able to conclude that this that property of inverses is both statements, right,
though that claim was that if A is a convertible matrixx than a inverses convertible and a Anvers inverses equal to a,
we're sort of using both pieces of that property right here.
So we're using we're concluding that A is convertible because we know that Eppy through E and is convertible and A is the inverse of that.
So we're using both properties of that piece of property be from last class or no, is that property A?
I think probably A, probably B was the product. So here the first one, so this is both of these statements are coming from that same property.
So I'm not concluding a convertible from this from the property of inverses.
I'm concluding both this and this. So maybe maybe you're right and that a better expression would be and.
Instead of just there, that maybe makes the logic clearer. Mike.
So as long as you're still doing the product in the same way, so if you're just using associativity and it will work out the same way.
So here the practically speaking, like when I start from my matrix, a, I don't know what the last row operation is going to be,
so I can't it's hard for me to read off from just what a is, what EBP is.
But I probably know what the first operation I want to do is to get rid of, say, the entries below, the topmost pivot, leftmost pivot.
So practically speaking, it's hard for me to know what Eppy is before I would know what you want is.
Also, practically speaking, you would never write down what all of these elementary matrices are as intermediate steps.
This is more just like the justification for why this approach actually works.
So here I'm sort of encoding in the notation in the reduction we've been doing before,
a framework where you apply the same operations to a to the identity matrix.
And we saw him actually doing that last time, maybe I should do one more of those to show what can go wrong.
So we actually computed an inverse last time. Let's now actually consider one where it doesn't work.
This was the next problem I had on the hand out. So one minus two minus one, minus one, five, six, five.
Minus four, five. So a common prompt could be here's a three by three matrix, find the inverse if possible.
So a good Cui's level question. That's not a hint for Friday, by the way, but just for future kids, it say find the inverse, um, if possible.
OK, well, I mean, the algorithm that I've just described would say, take the Matrix, whatever it is,
one minus two minus one, minus one, five, six five four, negative four five augment by the identity matrix.
Now, I just want to do the row operations that would turn this into the identity matrix.
OK, well, let's try it. I can't certainly tell immediately from looking at it whether it is convertible or not, so let's actually do it.
So here the first row, one minus two, minus one one zero zero.
So now I want to get rid of this one. So I'm just adding the first row to the second row.
So I then get zero, then three five.
One one zero. Now I'm multiplying the first row by negative five and adding, so I'm going to get a zero, then I get ten, minus four is six.
Then I'm multiplying here. I get ten multiplied here by negative five.
Negative five. Zero one. OK, we could do one more row operation if you want to, but what do you already notice?
When? It's not going to reduce the identity matrix right here, I'm going to get three zeros here because throe is a multiple of the second row.
So that tells me that A is most certainly not equivalent to the three by three identity, any matrix.
And so then what does that tell me? It's not so and hence by the previous theorem.
They is not a convertible, a inversed does not exist.
There's not inevitable. So this gives us a relatively efficient algorithm for computing the inverse, it's just a bunch of real operations.
It's, again, just the usual row reduction procedure that we've done for many other problems.
So it's actually a pretty computationally efficient method.
And because it's an if and only if it's an exact characterization of matrices that are convertible.
So in some sense, it's a very precise answer to our original question as well.
OK, so we're now in this really, I think, nice spot to stop and.
Summarize what we've done so far this semester, so if you think about what we've done,
at least in the linear algebra portion of this course, it's really been about giving lots of different perspectives on similar questions.
So even when we thought about inverses, we connected that back to the first day of the semester and we talked about solving systems of equations.
So this is all one very interconnected story that we've been talking about.
We're getting more and more perspectives on the same problems,
which gives us then a lot of a lot of power in the applications that we can then pursue from that,
because each of these different perspectives gives us slightly different ways in which we can make gains in our understanding.
So there's a theorem, probably.
Arguably, the most important theorem that we'll talk about this semester that summarizes all of those different perspectives,
so this is the convertible matrix theorem. So this is the theorem.
Where if you don't remember what to cite, citing the convertible matrix theorem is probably a safe bet, so it's often a useful one to know.
So it's a really fun theorem and we'll keep adding to it over the course of the semester as well.
So in some sense, you could think about 20 to A as really being about building out the convertible matrix theorem.
So let a B and and by and Matrix.
Then the following our equivalent.
So remember that abbreviation, the following are equivalent, and we're going to give a long list of different statements that are all the same.
So the first one, A, The Matrix, is convertible.
So that's our first statement. Now, everything that follows will be equivalent to this.
Well, the previous theorem I just proved was B A is equivalent to the end by an identity matrix.
That was our first theorem. So we've even just going back a few minutes in terms of review.
Another statement that's equivalent to that is that A has an pivot's to an end by a matrix that has a pivot in every row and column.
So that's another way that we can check whether a matrix is convertible, if you have a four by four matrix,
you have four Pivot's, you are done, you know that it's going to be convertible debt.
Another way for me to think about a matrix, having an pivot's is you can think about how many solutions you have to the Matrix equation X equals zero.
In particular, you should have only the trivial solution. So going back to homogeneous systems of equations has only.
The trivial solution. So there are no others.
So another way of thinking about that is you have no free variables, for instance,
so what can you tell me about The Matrix if you have that this Matrix equation is equal to it has only the trivial solution.
What's true then about the columns? Gwen. The columns are then linearly independent,
so that's another condition if the columns of your matrix are linearly independent, your matrix will also be convertible.
So if the columns. But they are linearly independent.
So the columns being linearly independent, if we thought about that as representing a linear transformation, what did that tell us?
Yes, Cameron. They span our end.
That's certainly true. So that would be then thing, but whether it's a subjective function.
So that's great. We'll put that on our list in just a moment. I want to keep them in the same order as a state of the theorem.
Tell me, what else would that tell us about it? Injected.
So we prove that the columns of your matrix are linearly independent with the columns of the Matrix associated,
your linear transformation or linearly independent, then the linear transformation is injected.
So that was an if and only if statement that we proved just before the midterm.
So if you have T going from Aaryn to our NP given by T of X is equal to eight times X, then T is injected.
OK, so that's, again, just about the same things we've been talking about then this is more to Karen's observation.
We can also think about how this relates to what the column span.
If you have a pivot in every row, they span out. And so the columns of a span are at.
So really, we're seeing a lot of different perspectives on the same thing.
Let me call this one H, I'm going to want to keep it in the same order as the way I'm stating it on the Panopto so that it's easier to refer to them.
So that'll be H. The columns of a span R and G will then be for all be an hour and X equals V is consistent.
So we can always find some solution, no matter what the right hand side is, I.
Well, if you can always find such a B where this matrix equation is consistent,
that also means the corresponding linear transformation is subjective, not just inductive.
So if t again, same one going from our end to our end giving.
By T of X is equal to a times X, then T is subjective.
So one thing you're noticing here is that t being injected implies that T would be subjective in this case because it's an end by N Matrix.
Not true in general, but at least here it's true.
We can also think about going back to the definition of inevitability, one thing that we were concerned about is here.
This implies that you have some matrix, see, and when you multiply on the left and when you multiply in the right, you would get the identity matrix.
What if you only had one of those?
So, for instance, J is that there exists some Matrix C, so that in this case, C times A is equal to the identity matrix.
So this is exactly that statement that you might be looking for if you're upset that having a left inverse means that you would have the same matrix,
would give you a right inverse, K will be the same corresponding statement for the other side.
There exists a Matrix C so that a time C is the identity matrix.
So if you're thinking about stating a definition for inevitability,
the definition of inevitability certainly requires that your matrix multiplies on both sides to give you the identity matrix.
It is now a theorem where after we prove this,
it will be a theorem that we're going to go to some amount of work to prove to show that having one side or the other is enough to get both sides.
So if you state as a definition just one, you're totally negating all of our work.
So it's really why don't we do it, Mike?
I you can sit right here if you prefer.
I mean, I'm just saying that there exists some matrix that does it and then I'm going to prove that I work on the other side, too.
So, yeah. Tammy. They will be the same.
You need the same matrix on both sides to get invisibility. Remember our definition of invisibility that exists exactly one matrix.
See where is the identity and the identity? So you need the same one.
Good questions. OK. And the last one really follows from the properties of inverses, again,
that if you think about the transpose and the matrix itself, they have the same inevitability conditions.
So a transposes convertible. So in some sense, this is really just a summary of everything that we've done really from the beginning of the semester.
It connects back to solving systems of equations. So right here, when we're solving X equals zero homogeneous system or X equals B,
all of these are then giving you exactly how they are giving you different versions of the same thing.
So it's a nice, I think, summary of everything that we've done. On top of that, the nice thing about it is that most of this we've proven before,
nearly all of it is just theorems that we've already had, and it's the kind of recording it in one place.
So proving this theorem really doesn't require that much work because it's more about just
observing that the individual pieces have already appeared in your notes in various places.
Tommy. Yes, yeah, so, yeah, we've got another one that we could add, yep.
Good, good point. There's also I mean, so there are a few more that you could add to being bioactive, for instance, now she could appear,
but I think by the end of the semester, we'll get to something like nearly through the alphabet, four different things that are equivalent.
So. It also means that when you're given a problem about checking whether a Matrix is convertible,
you have a lot of different conceptual ways of thinking about it.
So, like there are now a lot of power in your approach thinking about whether the columns are linearly independent,
whether they span our end, whether certain matrix equations are consistent or not.
So, like, there's a lot of different ways that you can think about that given question. OK, um, well, let's see, what do I.
How much time have I used? Twelve thirty five.
Perhaps it's worth summarizing some of the proof. So.
Again, most of this work is things that you've already done, so we need to somehow show that these are all equivalent.
Showing certain ones are equivalent to one another is a lot easier than others, certainly.
So we can think about what's maybe an expeditious path through all of these implications.
It's also a perfectly reasonable exercise to just take two random ones and see how you would prove that they're
equivalent without necessarily going through exactly the chain of equivalencies that I'm going to describe.
So A and B, we've just proved our equivalent. So that's done.
There's really nothing you need to do their. Perhaps the ones that require the most thought are over here, Jane.
So if I know that A is convertible, well, then I would have a Matrix C that works on both sides.
So in particular, I get one that works on the right or the left, rather, for J.
So A implies J.
Well, this is just by definition, convertibility gives you that you have a two sided inverse.
So then in particular you get a one sided inverse. So that's just by the definition of inevitability.
So there's not much to check their. OK, well, let's think about what Jay might imply.
So, again, there are a lot of different ways that you could go through this last class we proved.
Well, OK, let's think about this just from the beginning, if I had a matrix, see that I can multiply on the left to get the identity matrix.
If you had, say, the equation X equals zero,
you can multiply both sides of this equation on the left by C and then the left hand side would be X, the right hand side B, C times zero.
Multiplying any matrix times zero will give you zero because it's a linear transformation.
So then you would get X is equal to zero. So you've just satisfied that this is true.
So this would imply ID sense.
X would just be equal to sea time zero, which is equal to zero, which was essentially the theorem that we observed last time,
that when you had an inverted matrix, that we had a unique solution.
The only reason why we have to be a little bit careful is because we're only assuming a one sided inverse.
But if you go back and look at that argument, you're really only using multiplying on the left there.
All right. You could go through this a little bit further to. So C, D implies C, for instance, let's think about which that would be so here,
if this equation has only the trivial solution, then I would like to connect this back to having an pivot's.
OK, well, if I have X equals zero has only the trivial solution, then how can I connect that back to having an pivot's?
Yes. Right.
So then we know we can't have free variables, so since. We have.
No free variables. That means we have a pivot in every column.
It in every column we have in columns.
Every column and. We have an columns.
So since we have columns, then we know we have an Pivot's. Does you have.
And Pivot's. OK, so we've proved so far, amortized J J implies D, D implies C, if you keep going, C implies B why would that be true?
Why would that be true? I had to put a reason here urgent.
That's exactly right. So then the registration form has to be the identity matrix,
and this is essentially then by definition coming from where the leading ones would have to be.
We have a leading one in every row and every column. So therefore it's an end by a matrix.
So the registration form has to be the identity matrix and then we've already proved the implies.
So that was the previous theorem. So so far now we've completed a cycle, it doesn't give us everything,
but then since we've computed it completed a cycle of these implications that does give us that A DC B and A or all equivalent, Jonathan.
J implies D, so J implies D, what was J so J was that I have this matrix C that I can multiply on the left by A to get the identity matrix.
So what I do, I want to know how many solutions this equation has.
So I multiply both sides by C. S.
So then three times A becomes the identity matrix, so the left hand side becomes X, C times zero zero zero.
So then I get access to be zero. A good.
Guy. So we have at least some of them are equivalent.
Gwen. Missing what?
The compliancy. Pivots in every column and we have.
Thank you. Other other tables.
I shouldn't make the same joke, but I guess it's not a typo if it's on the board. Um, it wasn't funny the second time either, apparently.
So I just keep trying to make the same joke and see if it's ever gets a laugh.
Um, yes. Oh, yeah, it should, yeah, the following our equivalent, thank you.
This should be equivalent. Thanks. Good, good catch.
I don't know what I was thinking about their. Other other typos.
Have all of you read my proofread my book next semester so that you find all these typos?
All right, so we've had we've established a bunch of these. I don't know that it necessarily makes sense for us to go through all of them one by one.
The arguments are really mostly just tracking down how we've done them before with previous results.
So I think what I'll do is I'll leave the rest of these implications mostly as an exercise for you to think through.
How do they follow from either probably one step from either a definition or a theorem?
I think that's a good way to test your understanding of these.
But generally speaking, that's how the proof will go, is you'll try to establish another cycle going through these.
So I'm going to leave the rest. Or an exercise or exercises.
Certainly, if you don't believe any of them or one of them strikes you as particularly challenging and those are great to bring up in office hours.
Yes. Jay, what?
Oh, the one that Jonathan was asked about this one, so if I know there's some matrix here,
see that I can multiply A by on the left to get the identity matrix.
And I want to know how many solutions this equation has.
We'll take this equation, multiply both sides by sea that will then still give me a true statement.
So then I have seen times a times X is equal to see times zero,
so then that tells me X because the C and I will cancel to give me the identity matrix.
So X is just equal to zero.
So then, yeah, that's proving that then the argument going through there that shows us then we can get X has to be equal to zero.
Yes. Yeah, because I knew you started with this thing and you then concluded that X has to be equal to zero.
Jonathan. Of like on the. That they see I am then.
So one way that we know it is through we've proven through these chain of implications,
we're proving that if we knew Jay, we would then know that we could go through that where you have a two sided inverse.
So if your question is how do you see it directly without going through the sequence of steps, it's a little bit harder to write down.
But one advantage is that we've already proven a bunch of these things. So you can start with, say,
having that one sided inverse J then you can conclude how many solutions does X equals zero have well, has only X equals zero.
Then you can go from there to how many pivots do you have then if you have any pivot's we can then say,
well then the reduced echelon form has to be the identity. So the basically the route and proving it is to go back to we've completely
characterized invert ability based on being role equivalent to the identity matrix.
Are. The if, uh, if A is convertible, then yes, maybe.
Yes, a convertible inevitable, not always, but if a convertible, then that's true. Other questions.
Other questions. If you are also worried about the D or the J implies the implication,
the argument essentially appears in the previous theorem from last time showing that if the Matrix A is convertible,
then you have a unique solution to X equals B. Other questions.
Now is the time, are we good? Yes.
Through right now, we have established that a DCB or all equivalent, so if you could connect any of the others back with those, then you're done.
So you definitely don't have to go directly to a because if you can say connect them to be.
We've already established BNA, so then you don't need to necessarily do that work again.
So use the results we've already established. Yes, Sergeant.
So if you wanted to show that Kay implies A, it would be very similar to how we show that J implies A,
we would think about how we could use having a right inverse to solve a particular
equation and connect that back to whether you have a pivot in every row,
for instance. So you can connect that back with other statements that appear in the list.
But now we at least have the left one. The right one is similar.
OK. Other questions.
All right. Again, I think if you want to talk about any of them in particular, there's certainly there's lots of great quiz questions there.
There's lots of great questions, lots of great practice problems, lots of nice things to ask about in office hours.
So feel free to bring them up.
But I do think they're of the variety now where connecting them back to the previous things you've seen in the class should be doable.
Let's see, I don't know, maybe I should skip this example, but I'll do it anyway.
Let's take one concrete example before we move on. Why would we care about this theorem?
So one question that questions, do we have a question?
We got. I'm happy to answer if there is a question.
No, no one wants to volunteer. OK, OK.
So one common problem is to determine if a matrix is convertible or not,
so the most powerful tool you have now in your toolbox is determined is to use the convertible matrix theorem.
So certainly you can answer this question by just reducing the matrix,
but it's worth giving a little bit of thought to see if you could be more clever before you do that.
I mean, to see if you have theoretical tools that you can apply before just kind of bashing the problem with computational tools.
So here, if I wanted to think about whether this matrix is convertible, how might you see that without just reducing Tommy?
There's a pivot in every column. The columns are linearly independent. So, I mean, there are a lot of different ways you could see this.
So we have three pivot's. The columns are linearly independent if you want.
So by the convertible matrix theorem, we know that A is convertible, so we don't need to just immediately go to long strings of calculations.
I mean, oftentimes you can. Think about the question before just diving in and computing lots of things.
All right, when we were thinking about matrix multiplication, what was the other major perspective that we had on matrix multiplication?
What did we use that to represent? Yeah.
Linear functions. OK, so we are the linear functions, right, so we had the linear functions,
that was the main point is that we wanted to be able to represent linear functions by studying the associated matrices.
Here we've got a major theorem that's telling us a lot about the matrices.
So think about what that says about several students had panic looks on their faces when I said that and I wasn't wasn't the reaction I was expecting.
So I was wasn't sure what to make of that. So one thing that's nice, then, is to explicitly make that connection again.
So back when we were talking about functions more generally in the semester,
we had a notion of and if and only if statement for an inverse function to exist.
What did you need to know about the function in order to have an inverse function? Just generally for a function.
So that it was subjective, an objective, right, so you knew that a function had an inverse function if and only if it's by a.
So we could think what that would mean in this context, that it's telling us a bit about the same thing.
So I think this is a nice theorem. I was going to put it on your piece, but I'm going to do it anyway together.
It's a nice problem for us to talk about.
So here we have a linear transformation t going from our end to our end, the square coming from multiplying by some matrix.
So where T of X is equal to eight times X, so is the associated matrix.
So for this thing to be convertible, that means that there is some function now going the other direction to inverse,
and we call it actually s for the moment. So we say t a function is convertible.
This was the definition that we've not actually mentioned in class,
but it did appear in the reading for the day and functions if and only if there exists now a function?
S going back from from our end to our end, such that two conditions are true.
What two conditions do I need to know about? S in order for this to be the inverse function?
Tommy. So that's what I need in order for it, for one to exist,
but what would I need to know directly about this function as in order for us to be the inverse function?
Yeah, it's unique. How would I how what else can I say about it highly?
Yeah, so T of s so the composition T of S of X should just be equal to the original input and S of T of X should be equal to X.
So this is just the definition of being the inverse function so that we can compose in either order.
So this probably also makes sense with insisting that when we're doing matrix multiplication,
matrix multiplication is supposed to represent composition of functions.
So this is then representing composition where you first do X and then do T, this is T and then s.
So it's just like when you're doing AC and C A in both orders, we want both to be the identity.
So that's just kind of remembering back to general statements about functions, so this works even if t weren't necessarily a linear function,
but on the other hand, if T is linear, so it is linear, so it's associated matrixes, a, what would you expect the associated matrix of S to be.
And worse, it seems like a really good guess, right, so let's just make sure that math makes sense and actually prove that.
So that's what this theorem says. So if he goes from orange or in his linnear.
With Associated Matrix or Standard Matrix A. Then we have tea is convertible.
If and only if the matrix, the associated matrix, is in vertical.
Furthermore, the inverse of the inverse function of tea will have associated matrix a inverse.
So this was going to be on your piece, but I thought it'd be more fun to talk about it together.
So let's actually try to prove this theorem, I think it's actually a really good theorem,
like it's at the level of things that result that I would like you to be able to prove with some effort.
So thinking about the structure of the argument, we need to.
So that right implies left and left implies right.
Do you have any feeling whether which direction might be easier? Jonathan.
I think right, which implies left will be easier, too. So let's do that one to get it out of the way.
I mean, if you don't have a strong feeling, it doesn't particularly matter which when we start with, we've got to do both.
So let's suppose a convertible. So if A is convertible, what do you know from the definition of inevitability?
Jonathan. Yeah, and because we know she is unique, let's just refer to it as a right, so then we know a Anvers exists.
And a times, the inverse is equal to why and which is equal to a Anvers times a.
So what would what do we need to do now in order to finish the proof for this direction?
What do we need to find thinking? Kind of what we need to follow, Roy?
Right, so we need to find the function. We need to find this s how do you think we could define a function?
S that would do what we want? Yeah.
Perfect, right, so we could define this new function as so define.
Our function that we're looking for this purported inverse function. By s of the input, X does equal to a inverse times X.
Now all that's left is we just need to check that this thing really does what we want.
So we just compose the function together and both orders and check it.
So no. So if we do tee composed with us of X, this corresponds to a time a inverse of X, which is just of course equal to X.
Similarly, if I have S composed with T of X, this will then be a inverse times a X, which of course again is just equal to X.
So thus. T is convertible.
And. Well, that's really all we wanted to prove here, so that's Ts and writable.
So that proves one direction. What about the other direction?
Maybe this is a good moment to pause. Why don't I pause for two minutes?
Let's try to think about it. How would you prove the other direction? What would you do?
Try to think about it just on your own for a couple of minutes. Could you come up with some concrete ideas for what the proof would look like?
So it's really important to just try to think about some concrete ways that you
would start this if this were a proof problem on a quiz or an exam or a piece,
that what would you try? All right, why don't we discuss so what could we try?
What how could we at least get started if we're just kind of following the definitions?
What might we do or. I think that's a great idea, right?
We at least get that, so let's let's say suppose to use convertible. Protests invert.
OK, so TE's and Roadable, what does that tell me, what am I allowed to do from that?
Let's see anyone that hasn't answered yet. Everyone's hand goes down.
Yes. So then there exists some function as going from the domain of T to the domain of T, so that so.
T composed with S of X needs to be equal to X and s composed with T O, X is equal to X and that needs to be true for all X and or N.
All right. So now what about the following idea for finishing off this proof?
What if I just said, well, I need now to show that the matrix associated to T is convertible,
so what if I just called, say, C to be the matrix associated to S?
And then these two compositions would then tell me that a time C is equal to I and C times A is
equal to I and then conclude that A is convertible and call it a day box up my proof and go home.
What would be wrong with that, Sam.
Oh, yes, there's a problem, the function is not linear, we don't necessarily have an associated matrix, so the whole thing would fall apart, right?
I mean, that would just be a total disaster.
So we first have to make sure that this function, as we've written down here, actually is an inverted function.
And then we can write down the associated matrix and make the rest of that go through.
So let's make sure that it's actually linear. I really like these problems.
I don't give enough of them, I don't think where you have to critique the reasoning. So I should give that as a question on the next one.
So claim. US is linear.
Oh, boy, how would I actually prove that some functions linnear.
What do I need to do here? Mike? So those are great points, right?
So one thing to think about, though, I proved earlier in the semester that if you have a linear function, it must take zero to zero.
But I can definitely think of lots of functions that take zero to zero that aren't necessarily linear.
So the converse of that statement isn't true.
I know you were then giving me additional properties, but we want to make sure that we just go back exactly to the definition of linearity.
So what is exactly the definition of linearity say? Is it enough to prove this for a single instance of X, Y and Z?
Great set to make this completely correct. I need to make sure this is true for all X and Y and C, it's not just once this happens,
but it happens all the time and it needs to work for all scalars.
See, this is precisely the definition of linearity for us.
So that's what I need to check. I need to verify that that's true.
What about if we paused again for one minute to say, how would you prove these two things?
How would you prove these two things? So we've gotten further along in our proof, but what makes these somewhat subtle to prove?
So let's just pause for a minute and then we'll come back together. So how would you actually prove these two things?
So we'll see how to fix it so we can substitute that becomes a legitimate explanation.
But then there's another piece of genius so we can put. I think she understood and I can see if picks because she and I are.
The would be she she is so she could.
What's up, Cameron? All right, let's come back together,
so hopefully that was just a moment for you to think about how this would actually go if you wanted to break down this argument.
I think a diagram here helps in thinking where everything lives. So we know T is linear.
So somehow I'd like to get T into the problem. Right. So I'd like to somehow be able to use T inside of this problem.
Well one thing that I know is that if I had these two elements, X and Y in the domain of S and the domain of T,
I could then get the corresponding elements over here and the domain S of X.
And the corresponding elements over here, this is just a schematic representation of what this thing actually does, sends them over to the other side.
Now, those are things that are in the domain of tea that you could then sort of think about using tea.
So in particular, I could think about what?
As part of this thing of t t of this element, does so t of this element would then map back to x, t of this element would map back to Y.
That's the definition of being in an inverse. So I can then use that to write down what my argument would be.
So let's suppose that then. I don't know, let's give a name to these elements over here.
Let's call this one you and this one then we.
So let's define. You to be as applied to X and V, to be as applied to Y, then note if I applied T to both of these things T of you.
What will this be equal to x their inverse functions and T V will be equal to.
Why? OK, so let's now actually use that so note, I don't know then if we consider as of the some X plus why this is S of T of you plus T v.
So my goal in doing that from a Problem-Solving perspective was just to get T into the problem.
Now, what do you know about teh. What could I do next?
Yeah. Maha.
That's a good start. But just going back to linearity, like if I know T is linear, what am I allowed to do here?
I can then combine them together, right, so I could rewrite this is then S of T of U plus V.
Using linearity of tea. Now, how can I simplify this expression?
That's Zoe. That's just equal to you plus V by definition of an inverse function.
So since it's an inverse function,
this is just you plus V now just go back to what you and V are you and V where defined to be supplied to X and supplied to Y.
So this becomes X of X plus Y.
So right there is a proof that the first condition holds that we respect vector addition.
Scalar multiplication behaves in a similar way. Maybe for completeness, let's actually include it, so now we just need to check.
If we looked at as applied to sea times, the Vector X, well, again, let's just plug in what I've labeled that to be.
So that's as applied to sea times T of U.
Again, by the linearity of tea, I can move the sea in, so this becomes as applied to Tea of Seiyu.
So this is by linearity. And then now buy the properties of inverse functions, this is equal to see times you buy inverses, inverse functions.
Functions and then finally, you was equal to as of X, so this is a C times of X, which was the property we wanted to show.
So thus. S is linnear.
So now we've proven that all linear functions between from our end to our M even have an associated matrix, so thus we know that we have one.
So let me be the associated matrix now.
Or the standard matrix. Of course, we've just proven that it's linear, so we know one exists, we've proven that's true.
Again, just going back one step, how do we find the associated matrix if we wanted to find it?
How do we find the associated matrix of a linear transformation? James.
Yeah, exactly, so we find where it goes to, where it's an easy one, that's the first column.
Where does it send you to? That's the second column up through it. And that's the end column.
Yes. Question in the back. Yeah.
So we're not proving here, we're not using that, it's showing that it's by directive, right?
We're just going to show exactly that an inverse matrix exists. So we're not going through that particular property of bridge activity.
But that will follow from the convertible matrix theorem as well.
So then in this case, because now we want to conclude that A is convertible, so we want to prove that AB is equal to I and BA is equal to EI.
So then note.
They're as composed with T of X is equal to A times, B applied to X, which is just equal to X. We know that T composed with S of X is equal to today.
Uh. Uh, this this is, uh, sorry, uh.
Tea first. There we go, a, b, x times X, so therefore in both orders, eight times B is the identity.
So that's. A times B as the identity matrix and B times and hence and a convertible.
So I particularly like this proof because we were only going through the definitions at every stage,
we were doing this very much from first principles to get out exactly what's going to happen.
OK, Tommy. So here, if you wanted to know the associated matrix to this one, as James pointed out, I could plug in one.
So if I plug in one, it's just equal to one. So then the associated matrix of this thing is the identity.
Yes, the columns are all equal. All right, so I'm out of time today, so let me shift any questions to after class or office hours or email.
Next class, we'll pick up with giving a numerical test for inheritability and determinants.
So that'll be Fris class. Hey, what's up?
