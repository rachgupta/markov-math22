In terms of announcements, I did post the problems at 10:00 as requested, so the main portion of problems at 10:00 is, again, your project proposal.
There are mainly two things that I'll be looking for as a part of your project proposal as we give feedback.
So the first thing is there is genuine evidence of thought that you've put some effort into your final project and what it will be.
The second thing that we'll be looking for is that you have a group of two to three people to work with.
OK, so if you don't yet have a group of people to work with,
a number of your classmates have been emailing me to try to find a group of people that you might work with.
And I will do my best to match people based on the interests that you tell me about.
So if you want to email me over the next few days and just say, oh, I'm just looking for some partners for my project, just feel free to do that.
And I will sort of match people as I have people that seem like they would work well together.
OK, other questions on that. So keep that in mind, that's the largest part of peace at 10.
There is one problem on that 10 that I really quite like, but it's one.
So it's not not a super long problem. That is the microphone going out.
Is it going out again? That's good. OK, so the other announcements.
So Midterm two is, of course, coming up on Monday,
six to eight p.m. you should fill out the out of sequence form if you have an academic conflict with the midterm.
We also have quiz five coming up on Friday. So Quiz five covers a subset of the material for the midterm.
A common question that I've been asked many times is whether the midterm is cumulative.
Again, it sort of has to be cumulative because we we use the beginning of the semester.
That material still much in answering the subsequent material.
So it's not really the case that I can write an exam or you won't use anything but, you know, five or six weeks of the course.
That being said, I won't specifically be asking you lots of questions from that first bit of material.
I could say a little bit more about the midterm as we're still kind of arguing about what questions should appear.
But I think it's probably fair to tell you now that I did post a bunch,
I think 12 or so practice problems for the midterm along with the hand up from Monday's class.
So if you look at the eigenvalues handout from Monday, you'll see a bunch of problems that I quite like on there.
And in fact, you'll notice some problems appear multiple times on their sort of variations on similar problems.
As a hint, I'll say that those problems tend to be important if I ask them multiple times.
So make sure that you're comfortable with those. Those problems are kind of a wide range of difficulty levels.
And from what I would consider to be very straightforward to problems, that would be, I think, a good question.
So on the more difficult side, so don't think that I'm expecting that you would get 12 questions like that and that would be a midterm.
I'm not writing it with the intention that that's a practice midterm. I'm just giving you a bunch of problems that I like.
Look. The same room as last time.
Yeah, so what was that?
See, I was either B or C, I don't remember off the top of my head, does anybody remember it was B, OK, so the same room as last time.
I will post it on the page, but it is the same room as last time.
In terms of preparation for the exam and the quiz, since the quiz is a subset of the material for the exam,
I would spend as much of your time as possible doing math.
OK, it's really tempting to, like,
watch a lot of videos or to spend a lot of time reading books or reading solutions that have been posted to various things.
That's not a very effective way to learn.
I'm not going to assess you on this exam on your, like, retention of like what did I say on the solutions to problems at 2:00 or something?
It's not a memorization course. It's just really not. So you want to practice the things that I'm going to ask you to do.
What am I going to ask you to do? I'm going to ask you to solve new math problems.
So you want to be practice solving new math problems. So the review problems that I posted are all problems that I think for all of them.
They're not problems that I've given you before. So those are, say, 11 or 12 new problems for you to practice with.
But as you're preparing,
I would very strongly recommend that you allocate your time as much as possible towards actually doing math rather than just like watching your
friends do math or watching Khan Academy videos or whatever other thing that you might do that's very passive like to do is they like to recopy.
This is very time consuming, and for most people, including myself, I can shut off my brain and recopy text without thinking about it at all.
So that's not going to be a very effective use of your time if you're worried
about preparing for the definitions question or to be question one on the exam.
It's, again, a nine point question.
So it's not something that you should sleep on, but it's a question that I don't think that you want to spend most of your study time preparing for.
So it's sort of something that you need to know, but not something you should spend hours and hours preparing for.
Quinn. You can I think that's fair.
Oh, I was going to say this, too, I had a question on set cardinality,
which is fair game for this exam on there, but it was killed from the last draft of the exam.
So I thought I would just tell all of you that you don't need to prepare for that topic.
So if that's something that was particularly interesting to you, I did post like I think three review problems on set cardinality,
they're fun to do, but maybe that's kind of leading you away from the questions that are actually on the exam now.
Yeah. You know, it's interesting, one draft of our exam,
there was a long discussion on our team about whether one of the definitions should be to define a vector space.
And the team was very conflicted on this. I'm of the school of thought that asking you to write TMS is not a good use of your time.
It's something you should certainly know, but probably not what I want you to spend 10 minutes writing.
So I don't think it's a very efficient use of our limited time together.
It's the same idea.
I mean, in principle, for question one, I could also ask you to like state major theorems like state the ranked naledi theorem or something like that.
But I'm pretty clearly not going to ask you to state the convertible matrix theorem.
I mean, it's it's just it's too much time to write.
So I don't think that that's, again, a very effective use of my limited time to.
To learn about your understanding, linear algebra, so I'd rather use questions again,
it's going to be seven questions like last time where at a variety of difficulty levels.
So when you're thinking about going through the exam, I would make sure that you allocate your time appropriately.
Don't spend an hour on a single question. OK. Spend your time on the questions that seem most straightforward and easy to you.
And I'm sure that the first question on the exam or the second question on the exam should be or will be very,
very routine now, routine things that I've asked you to do so.
Just like on the last exam, the first question is to solve a system of equations,
that's one of the core things that you've been asked to do this semester.
The first question on this exam, again, will be, two, to work with one of our core computational skills that you need have to come out of this class.
OK, then the questions will sort of build from there.
But again, as we're writing the exam, like the last exam, more than half the points will be from computational questions.
So you should be comfortable with all of the Web work problems.
You should be comfortable with all of the computational questions that I've assigned as recommended problems from the textbook.
And if you can do all of those, you're already setting yourself up for a relatively a pretty good score then going into the
sort of second half of the exam where the questions get a little bit more theoretical.
So I would keep that in mind as you're preparing, what is the list of 10 or 12 reasonable computations I could ask you to do since the last last exam,
there's not that many different computational skills that we have. There are some like variations on how we can ask those questions.
But a part of demonstrating deep understanding is to recognize that that's really the same question.
OK, so that's personally how I would prepare.
But again, I'm happy to talk with anyone.
How you might approach studying for the exam question. I'm sorry, I can't quite hear you.
A very bad hearing, can you say it again? That's a good question.
So the question is, well, I posed solutions to the practice problems that I posted. So why do I hesitate?
In principle, I'm very happy to post the solutions to the problems that I posted there.
They're all questions that I like very much. The reason why I hesitate is because it becomes very tempting when solutions are posted to
just read the solutions and then you'll gain very little to no benefit from those questions.
So. I'm if I post the solutions of those questions, it will be closer to the exam to try to encourage appropriate use of that resource.
Because it really is a poor use of your time to just spend a lot of time just reading solutions over and over again,
because I'm not I'm really not going to ask you to just memorize a bunch of proofs and then reproduce them.
I don't see how that really assesses your understanding of anything other than your ability to memorize things.
So I will post the solutions, but it'll be closer to the exam itself.
So hopefully that doesn't just encourage people to just wait for the solutions.
They were on Monday very lightly so in principle, but I wouldn't expect them to be a very large portion of the exam, so.
Yeah. In terms of the timing.
The timing is always sort of a tricky thing. I suppose what I would try to do with the timing is I would approach the problems,
the review problems that I've posted individually as if they were an exam problem.
So like with the quizzes, give yourself, say, 15 minutes on each question, because on the exam itself,
I'd probably give yourself 15 minutes on a given question before you would move on to other questions.
So just in terms of timing, in terms of strategy, so you make sure that you have time to look at all the questions.
So I would probably use that as a gauge. Some of them will be challenging to do in 15 minutes.
But I would again probably use that as an excuse to practice, like letting a problem go, going on to another problem and then coming back to it.
Because what you will find is that you're you will make progress on a problem when you're working on
other problems to solve a problem that you don't have any idea how to do when you come back to it.
You then might have some more ideas on how to approach that given problem.
So it's usually a very inefficient use of time to spend a lot of time on one question,
unless you've already solved to the best of your abilities every other question on the exam.
OK, so I guarantee for every student in the room, you will see a question early in the exam or you're like, oh, I know how to do that.
So do that. OK, please, please, please. I implore you, don't go to the question you think is the hardest.
Every single time I've given an exam in my life,
there's always a student that goes to the hardest question and tells me I spent an hour and a half on this question.
But then you left all these other questions blank. I mean, I know you know how to do these other questions, so please don't do that.
OK, if there's a question, it's hard on there that you don't know how to do immediately.
Leave it until the end. Come back to it. Don't just sink an hour into it.
I know that's hard, because when you see something like that looks fine, I want to do that one.
It's hard to let it go, but on an exam of two hours, you need to make sure you're prioritizing, telling what you know, OK?
Yes. Oh, yes, they're supposed to be up already.
I think that's just sort of an administrative oversight. I'm happy to post them.
Yeah. If I don't have them posted by the end of the day, just email me and I will post them.
I mean, the case. I'll have the solution. So I don't know why they're not posted, but sorry.
That's my that's my my fault. Other questions.
Any other concerns before we really get going here? So I do very much.
Today, even material is not on the exam, I want to be clear about that,
I was actually just talking to one of my other colleagues who said that at this point in the semester,
they have about twenty five percent of their class regular regularly attends class.
And I was like, no, it's more like 90 percent or 100 percent of my class that attends class.
And so I, I very much appreciate that this this would be a lot less fun if you were like two people here.
So thank you for coming. All right. Let's try to emphasize again some big ideas of what we're doing here and building some intuition.
So if you think about what the goal is and what we're doing,
what we want to do is we want to find a nice coordinate system to work it and we want to think about what that means to be nice.
So if we're given some transformation tea from a vector space V to a vector space W, we want.
To find, quote, nice. Bases for V and W.
V and W. So that The Matrix.
Obce relative to these bases.
Is simple. And again, I've used to undefined terms here, what do I mean by nice and what do I mean by symbol?
So those are of course, great questions, but that will be become more clear as we keep going.
So let's just be a little bit more precise about what we're saying here.
So what do we mean by the Matrix t relative to these bases?
So if we have A bases, B, say B, one through B and it's a basis for V and I have a basis C,
C, one up through C, M, they're not necessarily the same dimension for W.
Well then we introduced this notation. I think we use this notation T and then we put down here the bases that the relative relative to.
So this is supposed to be the matrix of this transformation.
So the way that we found that was we took T and we plugged in the first base vector over my domain B one.
And then I expressed that output relative to the basis on the code.
And then I just did that for every one of my basis vectors.
So this is the thing that I want to be simple.
I want to choose be one through the end and see the the coordinates on my domain so that this matrix would be very easy to analyze.
OK, that's the whole motivation for basically the rest of the semester.
When we think about chapters five, six and seven, is that big question of how do you find a nice basis?
And there will be different uses of the word nice there, depending on what you want to do.
OK, so depending on maybe the application that you have in mind. So I think this was Arjun's question last time.
How can we find. A nice basis, again, where I'm being sort of vague about this use of the word nice.
In the standard basis, seem nice, why am I being so I mean, down on the standard basis that was perfectly nice in chapter one, why now?
Are we not calling that a nice basis?
The reason is because the basis wasn't necessarily reflecting the operation that you wanted to do, it might, but it doesn't necessarily.
So instead, what we want to do is to choose our bases or our bases to be in tailored for the thing you're going to study for your tea.
This is a fundamental thing when you're doing, say, in quantum mechanics and physics.
One forty three is you want to choose a good coordinate system to work and to make your computations tractable.
All right, so let's consider some examples to try to make this point.
All right, so let's suppose again, let's go back to Chapter one for the moment, let's take the transformation from our two to our two.
One of my favorite operations is orthogonal projection.
So let's take with our own projection onto the x axis. So just projection onto the x axis.
X axis, so we can then, of course, we've done this before we think about.
Yeah, I draw a picture.
Yeah, good question. So if we have say, oh, and I should use a color to what it's really fancy here.
Good point, because. So we take our standard basis vectors, say E one and E two, and then I apply it.
So then I say, well, what does orthogonal protection do to these two things?
Well, it sends e one to orthogonal projection onto the x axis.
So it just gives me back the same vector. So I'm just projecting down.
I'm taking only the X coordinate. If I take E two and I project down to the X axis, then I just get the zero vector.
So this is T of E two. So that allowed us to conclude last time that the standard matrix or now the E
matrix of your transformation t will then just be the vector one zero zero zero.
So that seems like a pretty nice representation of our transformation. Right.
How much simpler could your matrix get than just one zero zero zero?
But unfortunately or maybe fortunately, your operations aren't always just defined in terms of your access.
So you can also consider, say, orthogonal projection onto a different line.
So let's consider this other example. Now, t is going to be projection.
Onto the line, why is equal to three X?
So I could do the same thing we did in terms of Chapter one, if I wanted to find the standard matrix,
I'd find what happens to E one and E two with respect to this line.
Let's do something a little bit different. So if I have this line, Y equals three X here.
And I'm projecting onto this line, suppose that I took this vector here.
And maybe I'll take this vector, this red vector, I want it to be on the line, so I'll take this vector V to be equal to then one three.
So now what happens if I compute T a v? If I project onto this line, Jonathan.
It's just going to be V, so that seems like as simple as you could get that it didn't do anything to just give you the exact same thing.
What's another vector that would be easy to describe in terms of this operation?
That seems like a good idea to do what we're doing, orthogonal projection, so if I take a vector that's orthogonal to this line.
What could be a vector that's orthogonal to the line? Could I take for that?
Yeah. Three, negative one.
So that's what I said now, if I compute t of me, I get the exact same vector again,
that will be equal to V and if I compute if W that then gets projected to the zero vector.
So T of w then also seems particularly easy, in fact, zero is just equal to zero times W and V is just equal to one times V.
So now what if I took as my basis? That seems like a bad color scheme.
Basis should be blue, can you see the blue, can you see the bullet in the back is OK?
OK, so now what if I took as my basis these two vectors, V and W, are they linearly independent?
You know that. I'm not parallel,
it's an hour our two we have two vectors in our two is the basis theorem tells us we have a basis for our two because our two is two dimensional.
Now let's compute the B matrix of T.
The matrix of. So in our notation, the B matrix of T.
So we usually write this as t sub, so by definition, this is supposed to be T of your first base is vector B one,
then written relative to kwatinetz B and then T of B to written relative to coordinates B,
I personally find whenever you're doing problems like this, it helps us to write out the general thing for building memory.
So in this case, by first basis vector is V, so maybe let's do this a little bit slowly.
T V relative to base B and then I have T of W relative to Base B.
Well, TV was just equal to V itself, so then I just want to know what is V relative to Base B,
and then I want to know what is zero relative to base B?
OK, well, how could I write V is a linear combination of the basic fact.
What I take. One zero, right, is itself a basis factor, so that makes it kind of nice.
So that means the first column is one zero. If you want to write the zero vector, linear seemed to be just isomorphic, so that has to be zero zero.
So the B matrix to study this transformation in the sort of nice coordinate
system is identical to studying this one in the original coordinate system.
So what's nice about this is that if you choose this basis that reflects the
geometry of the operation or the operator or the function that you're studying,
you can then do a much simpler calculation that you can understand it in an easier way.
So I claim that this is much simpler than if I just tried to compute tee of E one and T of E two,
which would give me the slightly less nice looking matrix.
Yeah. A.
Yes. No, you'd have to describe everything in the B coordinate, so if you want to use this network,
things written relative to W and V, so if you're just given an arbitrary vector in the plane with respect to,
say, the standard coordinates, you then first want to express it with respect to new system, then you can multiply here by this matrix.
This will output something relative to the nice coordinate system.
So if you want your answer in terms of the standard coordinates, then you would need to do that.
So maybe that's a good point to follow up on is how would we then get the standard matrix?
So question, how can we find.
The standard matrix of teh. So remember, the standard matrix in our new notation is T relative to the standard coordinates.
So there are a few ways that you could do this, you could do it just directly by plugging in one and two like we did at the beginning of the semester.
But we can also then think about doing this in terms of a change of coordinates. So t of.
Sub e, the standard matrix, which we are often calling a at the beginning of the semester, well, we want to know how does this relate to T sub?
Well, this thing over here does the same operation, but it does it relative to the B coordinates.
So if I'm inputting, then something relative to the standard coordinates,
I could first transform to this from the standard coordinates to the B coordinates.
So from the standard coordinates. To the coordinates.
And then over here, this outputs something relative to the coordinates, if I want to be,
then relative to the standard coordinates, I then just transform back.
This matrix was particularly easy for us to represent.
This one was just B one and B two. So then in this case we would then have the Matrix going from B to standard.
And I said VW. So this is one three three minus one times, then one zero zero zero times, then one three three minus one inverse.
So another way that you could do this, you could just multiply this out conceptually,
all you're doing is you're taking something relative to the standard coordinates,
transforming to the coordinates, doing your operations and the coordinates, then transforming back to the standard coordinates.
OK. Luke. Yeah, you'd have to use your sea like Euclidian geometry to figure out what the you want to need to adapt to.
So it's a nice exercise and like playing geometry if you want to do that.
So the thing is, I think this goes back to a bit of Arjun's question from the last time, like here,
we used our knowledge of what this operator was doing to really figure out what the nice coordinates are.
So what we want to do have something more precise than that.
So what got us here is we're looking for Vector's particular directions where when you plug them into your function,
to your operator, to your matrix, it outputs a scalar multiple of that.
So it just gives you a scaling of that particular direction. That's sort of the simplest thing that we could hope for to happen.
So those things we define last time, those particular directions as then eigenvectors and eigenvalues Tommy.
But. So I'm being deliberately vague about what I mean by nice,
because there will be different notions of what you want to have happen after you change coordinates,
but typically the first thing we're going to want to do is to express your operator, your matrix in a diagonal form.
That's sort of in some ways the nicest thing you could hope for,
because it's really easy to multiply diagonal matrices together to analyze diagonal matrices.
So, yes, in some sense, our goal will be finding a diagonal basis.
That will be one one way of thinking about the word nice or simple.
And then a follow up question to us is then thinking about like, can you always do this?
Is it always possible to have a nice basis for your operator or for the data that you're studying?
OK, so just to recall then, how do we encode this idea of nice?
Well, it's going to be input factors, specific input factors from the domain that just gets scaled by the function.
So they don't use the complexity of the function. They just are scaling of that particular function.
So that's what we're going to mean by an eigenvector. So an eigenvector.
I should all tell you that I often use this abbreviation because the word eigenvector is too long,
so an eigenvector V of an and by and Matrix A. is a non zero vector.
Such that. A times V is equal to lamda times V for some scalar lambda.
So that's what an eigenvector is. The lamda, the amount you're scaling by, that's called your eigenvalue.
So Lamda is called.
Eigenvalue. Because eigenvalues are such good friends of mine, they don't mind it if I call them values.
Also because it saves some writing. You will be good friends with them, too.
So we call LAMDA the eigenvalue caught the eigenvector of you.
So one thing that seems a little bit interesting and is a common mistake that
often happens when people define eigenvectors is to leave out this word non-zero.
Why is that important? Why would that be important?
Yeah. It's very close, though.
You mean? Any eigenvalue, right, so you're exactly right.
So if I allowed V to be zero, well, then any lambda could work here when I take V to be zero.
That would always be true. So that would mean any real number would be an eigenvalue and that would be silly.
So that's why we can't do that.
So it's really actually important that because otherwise you're just you're just seeing that a linear function, 10 zero zero.
OK, so now the motivation for what we've been doing is thinking about then Eigenvectors and Eigenvalues is trying to give us a nice basis.
So kind of building on Arjun's. So that leads us to say maybe two more follow up questions.
This is becoming a research program, getting our grant ready.
So how can we find. Eigenvectors.
For those of you that are interested in computer science or scientific computing,
you might also be worried about the question of how can you efficiently find them?
Because if you're working with a very large data set, it might not be practical to do this with certain methods.
So if you have a data set that might consist of millions of entries, then you want to have a computational efficient method for doing this.
Not just some method, Luke. The eigenvalue can be zero, the eigenvalue can definitely be zero.
Right, the eigenvector always needs to be non-zero, eigenvalues can be zero if the eigenvalue is zero.
So if LAMDA zero, that's then telling you that there's a non-trivial solution to a V equals zero.
So then what could you tell me about the Matrix? A if you have a zero eigenvalue.
Roy. Yeah, so then that eigenvector would be then an element in the null space of a so then if you have a non-trivial element in the null space,
what can you tell me about The Matrix? Has a determined and then equal to zero, which tells me, what about the Matrix, what else?
Maybe this goes back to your question before about the inverted Matrix Theorem, right.
So one thing you can also observe is that all of the properties of the convertible matrix,
in particular having a zero eigenvalue could be another statement that you could add to the inverted Matrix Theorem.
Your matrix is convertible if and only if you don't have a zero eigenvalue.
The second question here that you might think about is how do I get Vector's?
Help. In finding a nice basis. So this is maybe to Tommy's question.
So, again, kind of thing. But what do you mean by that word nice. What does nice even mean here?
So as Tommy pointed out, one nice thing to have happen would be to get a diagonal representation of your matrix.
So maybe question three hour stretch proposal would be then can we diagonals any matrix?
OK, so now let's do some examples.
So let's just do a quick computational example. This is actually the first one on the handout for today.
So the first bit of today's class was kind of just transitioning us last class.
So if I take the two by two matrix, a one, three, four, two, and V is some given vector one minus one.
The question is then how could I tell if V is and I get. How would I check?
But what I do know. Herbert.
So if I multiply it eight times, V and what would I be looking for? Perfect, right?
So we want to know those eight times we just become some scale multiple of the right, so we just compute it and see what happens.
So if I compute this, this becomes one, three, four, two times one minus one.
So then I say, well, what does this become? I have one minus three, so minus two.
And then I have four. Minus two is two. So how does that compare to be.
How does it compare to the. The.
Tommy. Well, bye bye, negative two. Thank you so negative two times the vector one, minus one.
So what that computation just did for us is we just verified that negative two is eigenvalue and it has eigenvector.
One negative. So thus. V is an eigenvector, we just verified the definition with corresponding eigenvalue.
Negative to. All right.
Well, let's think about this a little bit more generally,
so let's try to extract from this some general way of thinking about getting eigenvalues and eigenvectors.
So if you note, if you start with a V is equal to Lamda V for some non-trivial V, well,
we could manipulate this equation to then get a V minus Lambda V is equal to then zero.
All right, so we have a view on the right, so we might as well factor that out.
So this then becomes a minus lamda times the end by an identity matrix.
Times V is equal to zero. So that means that a V was a non-trivial solution to your first equation.
That also means that V is then in the null space. Of a minus lamda times the identity matrix.
So that's particularly nice because the null space is something that we're very good at computing, especially in preparation for an exam.
The audio just caught up. Is it back? OK.
All right, so. So we're very good at finding a basis for a null space, so we could definitely do that.
So in particular, what this is telling us is that if V is a non-trivial element in the null space, then V is also a solution to B equals Lambda V,
so that tells you that V is an eigenvector if and only if if is an eigenvector, if an only non-trivial element in the null space of a minus lambda.
So that means the null space is then kind of keeping track of the eigenvectors for us.
It's telling us exactly how to find them.
So we're going to give a name then to that space where we put all the eigenvectors together along with the zero vector,
and we're going to call that the Igen space. And again, because we're good friends, they'll say space.
So the set of solutions to the equation, a minus lambda times, the end by an identity matrix, the equals zero is called the Igen space.
So it's the space of eigenvectors corresponding to the eigenvalue of Lamda, the igan space or more space.
Corresponding. To Lamda.
So we often use the notation capital e sub lambda for the space, so then you should note.
If I write E sub lamda, that is the null space of A minus lamda times the end by an identity matrix so that at least in some ways
gives us an answer to the question of how do we find eigenvectors is we compute a particular null space.
Yes, James. Lambdas fixed, yeah, so we're saying the eigenvectors corresponding to a fixed eigenvalue lamda,
so we're going to have an igan space corresponding to each eigenvalue. Good question.
Yeah, it's not an eigenvalue if a minus Lambda Eye has no non-trivial solutions,
so then we just say Lambda is not an eigenvalue because the only thing that would be there would be the zero vector and zero vector.
Can't be an eigenvector. Good questions.
So perhaps worth noting lambdas than an eigenvalue, if and only if so maybe this is worth noting, I'll write it down.
Lamda is an eigenvalue. For a then will be if and only if the null space of a minus Lambda Times, the identity matrix has non-trivial elements.
Non-trivial elements. So as Xavier pointed out, whether or not the null space has non-trivial elements by the inverse of a Matrix theorem,
we're just asking about whether a minus Lambda NT is convertible or not.
So maybe I'll include that statement here. So that's true. If and only if a minus lambda comes in by an identity matrix is not in vertical.
If it were convertible, you'd only have the trivial solution. So you're looking for lambdas where I'm not convertible.
What we how do we have to check we can check that cell.
Yeah, yeah. That's a really great question, and that really gets at what we're going to be thinking about next class.
We know the dimension has to be at least one, but we're going to see that the dimension can be bigger than one.
So it doesn't have to be just one. It could be larger. The examples we've seen so far have only been for two by two matrices.
So it's been exactly one and one, maybe four or five more minutes.
Then I'll give you an example where the dimension of the Eigen space can be bigger than one corresponding to one eigenvalue.
That's a great question. One more step here, though.
I'm just trying to be sort of opportunistic.
This wasn't the direction that I was going to lay out these ideas, but since people brought it up, I might as well do it now.
How what other ways can we think about the testing, the inevitability of this matrix?
James, we can think about the determinant.
So this will be true if and only if the determinant of a minus lambda times, the end by an identity matrix is equal to zero.
So this is a way of finding the eigenvalue, since our first way of finding the eigenvalues,
you can look for the lamda that make this determined and equal to zero.
OK, so this thing we'll call the characteristic polynomial of a.
So this is the characteristic. Polynomial of your matrix.
So the determinant of a minus lambda. So we'll see that coming up again in a moment.
But because. It was right there, I thought I'd mention it.
All right, so now back to Saul's question about the dimension of these Igen spaces,
the dimension of the dimension of an alien space always has to be at least one because, you know, you have a non-zero eigenvector.
That's the whole point. So let's now think about what could happen more generally.
So let's consider this other example. I think this is example to on the hand up for today.
So let's take for a three by three matrix for negative one six two one six two, negative one eight.
And then I'm going to take lambda is equal to two.
And then what I want you to do is I want you to find the Igen space corresponding to the value of two,
because I think this is a good thing for you to know how to do for the exam.
Why don't you take a minute and actually compute a basis for the null space?
So I really find this Igen space.
All right, let's come back together, so just quickly running through this question, so if you want to respond to the eigenvalue two,
by definition, that means that you're trying to find the null space of a minus two times the identity matrix.
If I subtract two from the diagonal of a, then I get this matrix.
Then if I reduce it, I get two rows of zeros. So I get this.
So then that tells me I can then write my best back in terms of a linear system of equations,
what's binding equation to X minus Y plus six equals solve for one of your variables and then say express
your arbitrary element in your Igen space as a linear combination of one two zero and zero six one.
So therefore we know that the igan space corresponding to the Eigenvalue of two is inside the span of these two.
There are linearly independent. So then we know that this is actually a two dimensional Eigen space, Luke.
We got. The vectors you've got here. Yes, that's always the case coming from the properties of the reduced echelon form.
Yep. Where the leading ones are so because what that's telling you is that the number of free variables here exactly corresponds with the dimension.
So this is a good question. So here we're seeing the dimension of the Igen space corresponding to the eigenvalue of two is equal to two.
So this goes back to Saul's question of why can the Igen? More than one doesn't have to be, but it can be.
All right. So we've done a bunch of computations now.
Now, let's actually try to do some theory to make our lives a little bit easier so we don't always have to do these calculations.
Yes. Yep, more than one linearly independent eigenvector, yes.
Yeah, there will be one eigenvalue corresponding to that eigenvector, yeah, so yeah, you can't have one nonzero,
you can't have one eigenvector living in two different eigen spaces corresponding to different eigenvalues.
I think I've actually given that as an example of one before. In the past, I, I won't put that on this exam because it's not a part of that material.
But Tommy. What yeah, the basic factor could be different.
Yeah. Yeah, yeah, I don't have to literally use one to zero, like, for instance, I could have used two four zero.
That would also be an eigenvector then. Yeah. So you're really interested in how many linearly independent ones can we find?
Because it's a subspace we could even add to eigenvectors or scale it and get another eigenvector corresponding to that same eigenvalue.
Good questions. OK, so when The Matrix has a particularly nice form, we can often read off what the eigenvalues are.
So in particular, if your matrix is sort of upper triangular or even more especially diagonal,
then you could immediately say what the eigenvalues will be.
They'll just be the diagonal entries. So let's prove that the eigenvalues.
Of an upper triangular. Matrix, hey.
Are just the diagonal entries. OK, so let's prove this.
All right, so proof, let's start with a being an up or triangular matrix, so let a B an upper triangular matrix.
Remember, this means we just have zeros below the diagonal. Well, then, if I looked at a minus lamda times, the NBN identity matrix, well,
subtracting Lambda Times, the NBN identity matrix only changes the diagonal entries.
So then I can basically say what this thing is going to look like.
It's going to look like a one one minus lambda down to a nd minus lambda and then oops, those are all zero.
And then the entries up here are just what they are.
Because they're unchanged. So that's what my the form of my matrix will look like.
Well, we've already observed in this calculation right over here that we have trivial solutions to the equation.
A minus lambda AI X equals zero if and only if the determinant is equal to zero.
So then I just want to compute the determinant of matrix.
So again, maybe it's important to recall. Lamda is an eigenvalue.
If and only if by the inaudible matrix theorem. That the determinant of a minus lamda times the end by an identity matrix is equal to zero.
So now we just need a computer determinant. Previously we've observed that the determinant of an upper triangular matrix is easy to compute.
So since. A minus lamda times, the lamda times, the identity matrix is still up or triangular.
Then we know the determinant we proved of a minus lamda times the identity matrix.
We'll just be the product of all of these diagonal entries. So then it will just be a nd minus lambda down to a number of these in the right way.
One one minus lambda down to A and minus lambda.
So this maybe more clearly shows how this is a polynomial in LAMDA, so the entries of A are all fixed.
So when you multiply this out, it'll be a degree and polynomial in lambda and you want to find the zeroes of this polynomial.
So the fundamental theorem of algebra, for instance, tells you that they're in Zebra's, which will think about over the next few classes as well with.
So that means the determinant of a minus Lambda I and is equal to zero if and only if one of these factors is equal to zero.
So if and only if Lambda is equal to a one one a two to up to a and and whoops.
Yes. And which was what we set out to prove that those in values.
So just by way of a quick example, if you happen to have a matrix, a with, say,
one two three zero four five zero zero six, you don't even need a computer determinant.
You don't need to do any work by this theorem. The theorem.
Then we know the eigenvalues of a. They are just these diagonal entries are one, four and six, then once we found what the eigenvalues are,
you can compute the corresponding igan spaces for each of those eigenvalues.
James. Oh, that's an interesting question.
So that's a really good question. So I like the idea of synthesizing back with the things we've seen before.
So let's just take a small matrix and in particular, maybe an upper triangular one that see what happens.
So suppose I take this matrix A is equal to zero one zero zero.
What are the eigenvalues?
Just zero, right, just the diagonal entries are just zero, so it also makes sense with this being a matrix that's not inevitable.
It's the eigenvalue we just have the eigenvalue of zero repeated twice.
The number of times it's repeated as a zero of the characteristic polynomial is called the algebraic multiplicity of that zero.
So this is called a zero value with algebraic.
Multiplicity to. So we'll introduce that terminology formally next time, but we might as well mention it now.
So now to James question. What if we did an elementary operation? One of them is interchanging Rose.
So then I could get another matrix. A will be similar to equivalent to the Matrix zero zero zero one.
What are the eigenvalues of this matrix? Zero and one, so I have now a different eigenvalue.
So the eigenvalues are not preserved under elementary operations.
That's an important thing to note. You can also do an elementary.
Of scaling their entry, like if you scaled this by anything, then you'd also be scaling the value of that.
So only changing things. Good question. I think I have a similar to that as a true or false question later on in here.
Oh, OK, we did that. Great. Oh, that's a fun one.
Have time for that one.
Let me do one more reputational example to finish things off, and I might have time to prove something, but I'll do it next time, so let me know.
Previously, when we've computed Igen spaces, the eigenvalues have been given to us.
So I just want to give an example of where we also have to do the step of finding the eigenvalues.
So I don't know which number this is maybe six or seven on the handout,
but a little bit later on in your hand out, you take the Matrix three, two, three, eight.
So if there's two by two Matrix and I want to find the eigenvalues and the Igen spaces.
So it's a little bit different than what we've done before. And then now I also need to find the eigenvalues first.
OK, so the only way we really know to find the eigenvalues if unless the Matrix is in a special form, is to compute the characteristic polynomial.
So let's just do that. So the characteristic polynomial.
We'll just be equal to the determinant of a minus lamda times, the two by two identity matrix.
So that means I'm computing the determinant of three minus lambda to three and eight minus lambda.
So it's just a two by two matrix. So we can just multiply this out.
This will be three minus lambda times eight minus lambda minus six.
So we get a quadratic polynomial so we can then multiply this out and factor.
And in the Xeros, so when you multiply this and factor, I get the two eigenvalues of two and nine,
so I'm going to get LAMDA minus two and LAMDA minus nine.
So those are my two eigenvalues, the zeroes of the characteristic polynomial.
So now we can do what we did before the first one.
Value two. So that's a minus two times the two by two identity matrix that I take, subtract two from the diagonal.
It looks at one two.
And then this row reduces to one two zero zero.
So now I'm just completing the null space of this particular matrix,
so that means the igan space corresponding to the Eigenvalue two is the null space of a minus two times the two by two identity matrix,
which will then be the span of what is that, minus two one.
So it's a one dimensional igan space. How do I know that I was going to get a row of zeros here?
We had to simplify to get a variable, so then we can also do the same thing for nine.
So if you do that for nine, maybe just for completeness sake, you'll then have the Igen space corresponding to the eigenvalue of nine.
So this is a null space of a minus nine times the two by two identity matrix.
And this just comes out to be the span of one three.
So, again, we get what the Igen space is in this case, so a fun exercise maybe that I won't do for you right now.
But just to check, you understand it's now take this vector in this vector,
those two eigenvectors, and write down the B matrix corresponding to that basis.
If things are working out well for us, then it should be a nice expression.
OK. All right, so I think I have I think I have time to do this.
So the very last thing that I'll do today is you might wonder about yes, go ahead.
Mm hmm. So I didn't quite hear it was the question to why didn't I just convert this to be an upper triangular matrix?
So the reason why is because doing elementary operations changes the eigenvalues.
So if I did that, I would have to keep track of how the eigenvalues might have changed by doing those elementary roll operations.
So you don't necessarily want to do that just without being careful in your approach.
Yeah, good question. Xavier. So, yeah, it's going to it's going to change what the eigenvalues will be.
It's going to change the. Yeah, it's going to change things.
I mean, this is sort of like when we talking about column spaces, right?
When you do elementary row operations on and you look at the column space for your matrix,
it's going to change the column space after you've done those elementary operations. Yeah, good questions.
Yeah, what does the. It means the number of linearly independent eigenvectors corresponding to that eigenvalue.
So what that's telling you is whether or not there's sort of enough independent directions where you behave in a good way.
So that eigenvalue is telling you when you move in the direction of that eigenvector, you're just scaling by that eigenvalue.
And so if you have a bunch of different independent directions, it's telling you like how much can you move?
How much freedom do you have in moving in those spaces? So if you have enough of them, that will then mean that we can find a nice basis.
If there's not enough, your matrix, you often call it defective.
I don't know why that's not a nice thing to say about a matrix, but that can happen, unfortunately.
And then the situation where we can't diagonals is a matrix. And so our grant proposal will probably be rejected.
All right, rather than rushing through this proof in the last three minutes, I think that I should probably just end a few minutes early.
So we'll continue picking up on the theory of eigenvalues and eigenvectors and specifically how you can use them to give a nice basis next class.
So I'll see you on Friday.
