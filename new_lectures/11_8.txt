All right.
So in terms of quickly going through some midterm too, we have tonight six eight p.m. in science Center Hall B piece at 10 is due on Friday.
Again, piece at ten is one question and the project proposal.
So make sure that you think about the two things that I'm looking for in the project proposal that you've put
some thought into what your project will consist of and that you found some people to work with on your project.
If you're still looking for people that you would like to work with, or maybe I've heard of some groups that have broken over the weekend.
Unfortunately, it happens. It happens.
I'm sorry it happens. But and if you'd like some help being connected with other students in the class that might be interested in similar topics,
I think it's probably reasonable that we set a deadline for that by, say, the end of the day tomorrow.
So anyone that emails me by the end of the day tomorrow looking for people to work with on their project,
I will make the my best effort to connect you with other students that have similar interests.
Currently, very few people have taken me up on this site necessarily have a lot of people to match at the moment,
but if you feel like that would be helpful, please feel free to reach out.
So we want to finish up. Diane Analyzation today, and this is really an important topic and in principle encapsulates a really intuitive idea.
So the idea is basically that. Some coordinates, some ways of expressing something are easier to work with than others.
This is often the case in applications that you might pursue and in pure mathematics, to be honest,
where choosing a good coordinate system makes the rest of your analysis much easier.
And so what we were looking for to sort of choose a nice coordinate system was one that consisted of eigenvectors.
So eigenvectors were representing directions or inputs to your transformation that behave in particularly simple ways under your transformation.
OK. So it's something that if I plugged it into my tea, then tea would be very well behaved under this transformation.
So to summarize, if we're given a linear transformation T.
Going from say, well, we can just say from our end to our end or our end at the moment.
Where?
Since it's gone from R to R M, it's given has an associated matrix in the standard coordinates eight times X, then what we would like, then we want.
A basis. Be.
So that. Your linear transformation T with respect to that basis, the B matrix.
This diagonal. That's what we want.
OK, so that's this whole idea behind this picture, I keep drawing over and over again because we have T in this case going from our end to our M,
so it's a little bit less general than the setting we were in the previous classes.
We have our end to our end. This is coming from multiplying by your Matrix T now you want some basis b where
you move into that basis so that we multiplying by your p inverse matrix.
So he inverse oops, he inverse. So where p is just your base factors.
Then down here in this new coordinate system, the Matrix is easier to work with.
So this is what I'm calling T sub being what your web work is often just calling the Matrix B.
From that, we can see that he and the associated matrix at T in the standard coordinates a are related by being similar to one another.
So namely, if I wanted to know. Well, alright.
Uh, which way you want to run it right? This be.
So if I wanted to study B, the B matrix, that would be the same thing is if I went the other way around the square.
So you change to be in the standard coordinates to change in the standard coordinates, you multiply by the matrix p.
Now I'm in the standard coordinates, so I'm up here so I can use a to get an output in the standard coordinates,
and then I multiply by inverse to get back into the B coordinates.
So then we get that A and B are similar. So A and B are similar matrices.
So they're expressing the same linear transformation with respect to different coordinate systems.
You can make a problem seem much more complicated than it really is by working in an awkwardly chosen coordinate system.
That's true just geometrically. Again, if we go back to that example for my first day where I'm spinning around.
If you choose one of your coordinate axes to be different from your axis of rotation,
then things will just be much, much messier and make the analysis more difficult.
Matthew. One of them does use P, so p this matrix.
Goes from the B coordinates to the standard coordinates.
So Note A was with respect to the standard coordinates, was calling that my standard matrix the top line of my diagram here.
This world is all in the standard coordinates.
So the idea is you want to move out of the standard coordinates to now this nicer coordinate system or hopefully nicer coordinate system.
So in order to do that,
I do the inverse matrix to get out of the standard coordinates to get into the standard coordinates and multiply by the P matrix.
OK. So to go this way, I use Inverse to go. This way, I use P.
That's why when I was first going up, I used P here because remember you always go right to left.
Jonathan Husband. Why?
Because the definition of being a similar matrix is that B is equal the inverse ape.
So we we've defined the word to similar to mean that they have this kind of a relationship.
So I'm just kind of using this vocabulary to remind you of that terminology.
But if you're thinking like the word similar is a term that mathematicians overuse dramatically.
Somehow, we don't have big enough vocabularies, so we use the words similar to mean lots of different things.
In this context, what you really mean is that this is reflecting the same linear transformation, but in different coordinate systems.
And then hopefully in some coordinate system, things are simple. OK, great.
So for us, the way that we went about trying to find this matrix p like this nice coordinate system as we looked for directions,
where are your matrix? Was simple?
So like in that example of me spinning around one direction where things seem particularly simple is if I took a vector in the axis of rotation,
then it's preserved under this rotation. It's the same thing. That means it has an eigenvalue of one.
So that seems like a particularly nice direction to use as a basis factor.
Same thing. If I were thinking like reflecting across my piece of paper, I want to reflect across my piece of paper with some vector like this.
Well, if I'm orthogonal to my piece of paper, well, then I have an eigenvectors value of negative one because it just reflects it below.
If I take a vector in the plane,
then I just get an eigenvalue of one or an eigenvalue for each of the two independent directions that you could have inside of that plane.
So that gives me an eigen basis.
I have the two directions inside the plane to linearly independent directions inside the plane and the direction perpendicular to the plane.
That gives me a nice eigen best basis to work with.
So here what we want is to then just show that we have enough nice directions.
If we have enough nice directions, then we take that to be our basis. So that's going to be the diagonal ization theorem.
This is going to completely characterize when a matrix is diagonals visible.
So an end by end matrix, hey, is diagonals of all.
If and only if. We have an linearly independent eigenvectors, a pause and linearly independent.
Eigenvectors. So that we can find enough of them so that they can form a basis.
OK. So we can also make this more precise.
When we did this last class for two by two example,
we found that if we took a excess of the eigenvectors and then we computed the beam matrix relative to that eigen basis,
this diagonal matrix, it wasn't just diagonal, but even more than that.
The diagonal entries were themselves the eigenvalues. So we can include that as a part of our theorem.
So moreover. If we have a diagnosable so as equal to PDP Inverse, where the is diagonal.
Then that's true if and only if the columns.
Of P R eigenvectors of a R eigenvectors.
A and B diagonal entries of D are the corresponding eigenvalues.
Yeah, are. The eigenvalues, so we can give a very nice description.
Of what that's going to look like, Laura. It's a fair question.
So if I move the PEA over to the other side of the equation, then I would have p inverse.
A multiply by P on the right is equal to D.
So it's. If I it just depends on how you're expressing this relationship, like what you want to be, is P going from is p your basis of eigenvectors?
And then in that case, going back over to this picture over here, if p is your basis of eigenvectors B v one,
the V and then you want it to be reflecting of going from your nice basis,
the one where your thing is supposed to be, your matrix is supposed to be diagonally represented,
where your transformation is diagonally represented to the standard coordinates.
So then if you think about here like what this relationship represents,
multiplying by P first because you always go right to left p first, then transforms.
From the standard coordinates to the oh sorry, from the B coordinates to the standard coordinates,
then A works in the standard coordinates, then P Inverse transforms back.
So then all I'm doing with this relationship is I'm just solving this one for a instead of D.
So when I solve this for a, I get PDP inverse. So I just want to be consistent about what P does.
P will be the matrix that transforms from the new coordinate system back to the standard coordinates.
OK, so d will be working in the new coordinate system.
So I want P to be the one that undoes that and puts me back in the standard coordinates because that's where a is supposed to live.
Senator. Was that similar to your question, Jonathan?
I was going to have this type of life or our. Oh, oh, thanks.
Thanks. PR eigenvectors, a very good call.
Thank you. Yup. Or.
There, linearly independent, yeah. Yeah.
I mean, here, I don't have to say it again, because already I've said that P Inverse exists to the columns will be.
And yeah, so I want the columns to be linearly independent eigenvectors. All right.
OK, so let's prove that there. So it's an if and only if statement.
So we need to prove both directions. So just thinking about the structure of our proof.
So we start out with our and linearly independent eigenvectors. So let's take this one up through the PN, the linearly independent eigenvectors.
OK, so this is my hypothesis over here. I have.
So if I have an linearly independent eigenvectors on our RN, I can use them to form a basis for our end by the basis theorem.
So then let's just name that basis. Let's take script B to B v one up through the end.
So then the P matrix. We won V n will be the matrix that transform from coordinates to the standard coordinates.
That's what The Matrix always does, it takes your basis with respect to.
Oh, I'm sorry this one. This one should be embraces. OK.
So let's give a name to the eigenvectors since or the eigenvalue since we'll need them as well.
So define. Lambda one lambda and has the corresponding.
Eigenvalues. This does not mean they're all distinct.
So lambda one through Lambda NW, they could have a repeating values in that list, but they're all corresponding to V one through V.
OK, so I'm not assuming that they're all distinct. Finally.
The claim was about this diagonal matrix where I put the eigenvalues along the diagonal, so let's just do that.
So now, if I'm going to prove the other direction A is diagnosable,
so the definition of being a diagonals will matrix means that you're similar to a diagonal matrix.
So that means you need to construct this p matrix in this DX matrix where a is similar to those.
So now we have our purported diagonal matrix DX.
We have our purported Matrix P, so it's just a calculation to see if then A is equal to p dpi inverse.
So let's compute so. Or is a vision instead work with eight times P?
First, so if I think about eight times P, so you'll note eight times Per.
So if I look at what eight times is, this is a times the one through the end.
Well, now, by the definition of matrix multiplication, I can multiply through all of the V the A's, so this becomes a times v one a times b n.
That's how matrix multiplication works. But now we went through Vienna eigenvectors with corresponding eigenvalues.
So then this becomes lambda one, the one up through lambda and the RN, that's again since we went through Vienna were chosen to be eigenvectors.
All right. So now I would just like to know what this is. So this says the first column is scaled by lambda one.
The second column is scaled by Lambda two up to the nth column is scaled by Lambda NW.
So one way that you could express that relationship is you could then say this is equal to P.
We want through the N times the diagonal matrix lambda one found through lambda and.
Just equal to top times, D. So now we have the relationship, AP is equal to PD.
So if I then just supply both sides by inverse, which again, you know, Inverse exists because they're linearly independent,
you have and linearly independent vectors and are in the inverted matrix theorem then tells me that P Inverse exists.
So since he is inverted, all.
Vertical. We get. But A is equal to P the P inverse.
So hence, it is diagnosable. So there is a proof that if you have an linearly independent eigenvectors, then you will get that matrix is diagnosable.
So we need to prove the other direction as well. Is that part okay?
What questions do people have anything? OK.
We're technically approving the moreover part, because the more of a part, it really implies this part.
So yeah, I mean, the moreover part is really just a more precise version of that because what I did here is I just took the moreover claim,
and that's how I set up the calculation. So.
And the top part of the more I more of a claim, a being equal, the PDP inverse, that's the definition of being diagnosable.
So yeah. All right, so we should prove the other part.
So now we do want to prove we want to suppose that is diagonally visible,
so then that means that suppose a is similar to a diagonal matrix, so A is equal to p the p inverse for some matrices and.
So let's give names to these matrices, and we'll optimistically label them as the one through the end.
This point, I don't get vectors, but I hope they are by hoping again.
Maybe my favorite proof strategy, proof, positive thinking.
Oops! I say that as a joke, but I think there's actually a fair amount of truth to that.
I mean being it's like solving hard problems and working on hard problems,
you really it's important to remind yourself like how how much skill you have in this subject now that you've really worked hard, you've done a lot.
And so it might not feel like it because we're always working on the edge of what we know, like those are the problems we're working on.
But you've truly put a lot of work into this class. I know that I've seen many, many of you in office hours.
I see you before class working on the problems. I get an incredible amount of email asking really good questions.
It's I know you all know this material. Well, OK, so it's.
It's important, though. I mean, it's fun to joke, but it's important to remember to have a positive attitude and to remember that, like bring that in.
Well, it's a lot easier to do hard problems when you go in, like knowing I've solved hard problems before I can do it again.
Uh, yeah, that's that's a good question.
I don't know.
I mean, it's a I think it's a reasonable thing to think about what works for you, what motivates you, and it is different for different people.
I mean, I think that one of the things that I enjoy most about teaching and working with students is recognizing that each student is an
individual and learns in different ways and ask questions in different ways and trying to figure out what motivates different students,
like how to get people to work sort of really at their peak capacity.
And like to really understand things in a deep way. And um, I personally think that it's really,
really important to take time even like further on in your career when you're well past this course, when you're well into your college career,
when you're well after college, maybe you're in grad school, maybe after grad school,
you're all famous professors or something that your you take time to remember how like, how far you've come to because.
Good. It can truly be discouraging if you're always at the very edge of what you know.
But if you take some time to look back and I think hopefully maybe over Thanksgiving break,
you'll all get some time to reflect more over the semester, you get some quiet time.
Take that time to really think about how much you've learned this semester. I know it's been hard.
I know it's been a long semester for everyone.
It's truly been challenging, but you've learned an incredible amount, so you should take pride in that and you should definitely appreciate that.
So it's important to take time to appreciate how much you've learned to.
All right. Well, let's prove this before I start crying. So, so we have this statement and approach it in the same way.
Some approach it in the same way as I'm going to compute a times piece here.
And I want to essentially use the same argument to conclude that I have a bunch of eigenvectors.
So now we know. But eight times P is equal to P times D just by solving that first equation.
OK, that's what we know, because a is equal to PDP inverse.
OK, well, now this side of the equation that tells me that I have eight times the one up through van and I have the one,
you could summarize this proof by just me saying I don't want to invert any matrices.
So that's maybe the guiding principle behind what's going on here. So now when I multiply these together, I think this goes back to.
This Luke's question, someone a long time ago asked the question about what happens when you do elementary matrices on the other side of the equation.
So now I'm going to be doing column operations on this matrix, and this matrix will become a times b one up through a times v end.
And then here we have lambda one v one times lambda and the RN.
So now two matrices are equal if and only if their corresponding columns are equal.
So this tells me a v one is equal to lambda one v one down to a v n is equal to lambda, and we end.
So then this tells me that the arc is an alien vector.
With Eigenvalue. Lambda decay.
OK. Which was what we set out to do that if we took the columns of this corresponding Matrix P,
we could show that there eigenvectors in the diagonal elements would be the corresponding eigenvalues.
So now we have this theorem. Yes, soccer. Oh, it's a good question.
That's a good question. Does anybody have a suggestion, how can we figure that out, whether they're linearly independent?
Let's see, maybe, Tom, you've answered a lot of questions. We'll give someone else a chance.
How would you know that if you want to be in, I think your point is exactly right, I probably should say something about it and my proof.
Why would they be linearly independent sailor? Yeah.
That's exactly right, since we started with a risk matrix, the inevitable matrix theorem tells us that there linearly independent.
So it's a good question.
OK, so our main theme about idealization is that if you end linearly independent eigenvectors, then you know that your matrix is diagnosable.
I want to skip my two examples for just one minute.
What if I knew I had a matrix and all of the eigenvalues were distinct,
I think this is the last question on my handout, but I just want to ask you that question.
It's a nice profit problem.
It, of course, won't be on the exam tonight because eigenvalues and eigenvectors would only show up in a very minimal way, if at all.
But here, how would you know that if they're all distinct, if all these eigenvalues happened to be different, you have different ones?
Why would you then know that you have to be diagnosable? James.
Perfect. Yeah, take one.
I bet they're coming from each of those and distinct eigenvalues, we proved last time that then they would all have to be linearly independent.
Now we have and linearly independent eigenvectors of the diag analyzation theorem tells us that they will be a basis and eigen basis.
No, that we can. We would be diagonals able. So that raises an interesting question.
That means that if I have any distinct eigenvalues, I would always be that.
If you're looking for a matrix that's going to be done in Liverpool, no limit to be matrices where you have a repeat.
Then it raises the interesting question, I think of how important our repeated eigenvalues.
So in many linear algebra courses that you might take,
there will now be a big fuss about repeated eigenvalues and why you should study repeated eigenvalues.
I claim that's not super important for the following reason.
Suppose that all of your data is sort of experimentally determined.
So in that data, so all the columns of your matrix are like data points from some measurements that you've done.
You have these and measurements all living inside of our.
Well, the generic situation from your experiments is that they're all going to be linearly independent.
The reason for that is because your experiments are only up to some amount of certainty anyway.
There's experimental noise.
So if you have a slight perturbation, like within the range of noise of any of your data points, then you've just moved it.
So that's not a linear combination of the previous ones anymore because to be
a linear combination of the previous factors is sort of a special condition.
That's very precise, a precise relationship.
So the generic setting is that you would have indistinct eigenvalues and that you would have something that's diagonals able to work with.
So this is actually the most relevant situation.
The thing that's most likely to happen in an application when you have a real data determining what's going to happen,
whether you're going to be diagonally visible or not.
Because even if you did have, say, a repeated eigenvalue in your calculation coming from that real data set,
well, that was only up to some experimental accuracy anyway.
So it's probably not even the case that it was genuinely a repeated eigenvalue.
They might just be close. So then you can still use this theorem of making them all diagonal visible.
So that would be a reasonable project if anyone was interested in developing the theory for repeated eigenvalues.
And what you could do, they're subject to not necessarily being able to diagonal lines.
But that's not something I want to go into in this course.
So what I want to do now is I want two examples, one example where you can diagnose and one example where you can't diagonal eyes.
And then I think with my last, I don't know, I'm hoping my last, say, 15 minutes for the day.
I'll then show how this looks.
In the case of working with some differential operators, which is then parts of that will overlap with the exam material for tonight.
So I thought that might be a nice bit of review at the end. So let's go through these two examples.
And then I'll hopefully have time for these exam relevant examples at the end.
So let's take a to be the three by three matrix one three three minus three minus five minus three three three one.
So I want the question that we're interested in is there a basis in which this matrix would be represented as a diagonal matrix?
So is it diagonals visible? Is a diagonal sizable?
So the only way that I've really approached diagonals ability before is by spinning around.
That's how I saw a lot of math problems as I spend. Also helpful.
So here what? I'm looking at this. It is not at all apparent to me that this represents any sort of geometric transformation.
So there's not a good way for me just to guess eigenvectors or guess eigenvalues.
So I need to just go back to sort of the drawing board here, and I need to actually carry out the signalization algorithm.
So this is going to be three steps.
You find your eigenvalues, you find the corresponding eigen spaces and then you apply that idealization theorem if appropriate.
So our strategy? What I and I got values.
To find alien spaces.
Spaces. Three. Apply the diagnosed Asian zero.
OK, so let's start with one. So we want to find the eigenvalues of this particular matrix.
The only way that I've taught you to find the eigenvalues of a matrix is through finding the zeros of the characteristic polynomial.
I think, was it Joel, someone asked before? What about the situation?
Why are determinants even useful at all? They seem computationally really inefficient.
So our only strategy for finding the zero the zeros of the characteristic polynomial will be
through studying the determinant and through studying the zeros of particular polynomial.
So this is a rather computationally cumbersome thing to do.
So that's not what you'd really want to do experimentally if you're again working with a very large dataset.
It's not a practical approach to just compute a polynomial of degree 10000 and try to find all the zeros to factor it of a polynomial of degree 10000.
So again, it's a very nice project idea would be to think about what are algorithms
computationally efficient algorithms for finding eigenvalues and eigenvectors?
Profoundly important problem. So here we will use the example of the strategy for small examples and still
theoretically useful of finding the determinant of the characteristic polynomial.
So in this case, then I'm trying to find where I am finding, I suppose, the determinant of one minus lambda three three minus three.
Minus five, minus lambda, minus three, three three, one minus lambda.
So this is a calculation that I'm sure that all of you can do, but I don't think we need to go through it together in the moment.
So this becomes minus lambda minus one times lambda plus two squared.
For her, there is the repeated eigenvalue, so this is a particularly special situation, it's not a generic situation.
Maybe I should switch boards. Sorry, that's warm, I shouldn't cover that after I finished writing.
OK, so just to summarize, step one, we found eigenvalues of one and an eigenvalue of negative two.
And again, I mentioned this before, but I'll mention it again.
When you have an eigenvalue appearing some number of times in the characteristic polynomial,
you refer to that as the algebraic multiplicity of your eigenvalue. So the algebraic multiplicity of the eigenvalue of one is one.
The algebraic multiplicity here, the algebraic multiplicity.
Is to because it appears as a factor, twice in the juristic polynomial.
So step two is then we want to find the corresponding null spaces.
So certainly something that you're all prepared for and comfortable, comfortable with for tonight would be finding a basis for a null space.
So this is a good example for us to work through. So we take each. Eigenvalue and we compute the corresponding eigen space.
So the EIGEN space corresponding to the eigenvalue of one is the null space of a minus the three by three identity matrix,
which is then the null space of the matrix. Zero, three, three, minus three, minus six, minus three, three three zero.
Now we Robredo's, so then you reduce the matrix and then you end up with one zero minus one zero one one zero zero zero.
Which makes sense that it had one free variable. In fact, one result that we're not going to prove, but it's nice to notice,
is that the dimension of your eigen space is always at most the the algebraic multiplicity of zero.
So because you have a algebraic multiplicity of one here,
you can have at most one linearly independent eigenvectors vector corresponding to this eigenvalue.
OK, well, now in this case, we have the one free variable. So, you know, the nullity is one you're expecting one basis factor.
So then again, we can do the usual thing too. Then, right, this is then equal to the span of one.
Minus one one. For instance.
So there is my basis there for then I can space corresponding to eigenvalue of one.
So I have one I can Vector, and I want three.
So the other two had better come from the other eigen space or I know I'm just out of luck.
So now we take the other eigenvectors eigenvalue route.
So the eigenvalue the eigen space rather is the null space of a plus two times the three by three identity matrix,
which then this will be the null space of the Matrix three three three minus three minus three minus three three three three.
And there's a nice matrix to reduce early on a Monday afternoon,
so that's then just equal to the null space of the Matrix one one one zero zero zero zero zero zero.
There are two or three variables now. It's good because I was hoping that I would find two linearly independent eigenvectors.
So now I can then write this as the spin.
Of the two vectors, I guess I could take, for instance, minus one one zero and minus one zero one.
Those would be give me a basic. For this particular agent space.
So now I can apply the tag analyzation theorem.
Maybe I'll leave up the original matrix. So now step three, we want to apply that idealization theorem.
So the Digi Analyzation Theorem tells us that a is going to be equal to PDP inverse, where PND are determined vectors.
So now by the diagonal ization Theorem A is equal to p the P inverse where.
D. Well, it's going to be the diagonal entries will be the eigenvalues, the eigenvalues in this case were one and negative two and negative two.
So it's repeated. That's why it shows up twice on the diagonal. And he was supposed to be my be my matrix coming from my baby eigenvectors.
So in this case, I need to write them in the same order that I've written the eigenvalues.
So the first one is one minus one one. That's Nagan vector coming from the eigenvalue of one.
Now these two are coming from the other eigen space. So then I have minus one one zero minus one zero one.
And the calculation that you can check, but I'm not going to do it for you, is you can compute this matrix times this matrix times p inverse.
And it will indeed be equal to a. And that will generally be true.
Jonathan is one of the second around the order of this agile life.
Yup, yup. So like, for instance, like I could scale this one, it's still an eigen vector.
No problem. All that's going to change his plea here and the inverse here, because it's on both sides, it changes accordingly.
So yeah, there's not a unique choice here. They just have to be three linearly independent eigenvectors.
The thing de here is sort of unique up to the choice of the eigenvalues, the order that you've written them down and.
Of course. Sure.
So here I'm looking at the whole space. Of one one one zero zero zero zero zero zero, so that's like this.
Matrix.
So there's not much more that I can do with The Matrix, the augmented matrix, so instead what I turn it back into a linear system of equations.
So that tells me X plus y plus z. Is equal to zero.
So that tells me that X equals or maybe Vector X is equal to x y z.
So maybe solve for X. When we saw four, so then if we solve for X here, I would get minus y minus z y z.
So then that's equal to y times the vector minus one one zero for z times the vector minus one zero one.
So that. Good question, Tommy. So things like.
Yes. So that would be something else you could say that's a little bit of work to prove it, but yeah,
you could say that the the in order for a matrix to be diagonals of all the
algebraic multiplicity must be equal to what's called the geometric multiplicity,
which is the dimension of each eigen space. The sum of the each the dimensions of the eigen spaces must add up to RN.
So, yeah, that's a nice way of thinking about it also could be a nice project since I'm not going to do that in class.
So, um, let's do one more example, and then I have these two differential operator questions that I'll do at the end.
All right. So let's do one more example, this one is I can do a little bit more efficiently.
So now let's take another three by three matrix again, given the context that's just coming from numbers in The Matrix,
we have no geometric context to go from to kind of guess things. We have to use more algorithmic methods.
Two four three minus four minus six minus three three three one.
So again, the same prompt diagonal lines, if possible, find a basis in which the matrix of a will be diagonal if possible.
So we go through the same steps as before.
Step one, we find the eigenvalues, the only way we really know to do that is through the characteristic polynomial.
Again, there are better methods, especially if you are thinking about applying this in sort of a context, maybe with a large dataset.
It's nice to learn those methods that their.
So here, if I look at the characteristic polynomial, this will be the determinant of a minus lambda three.
It's actually a nice product from last year, one student went through like actually finding an eigen basis for an extremely large
dataset from a biologic biology context of looking at like genetic sequences.
And so it's a really interesting to see it play out that way. So here, if I die, analyze this one again, I won't show the exact calculation here.
So we get this is our characteristic polynomial after you factor it.
So that means the eigenvalue. For one and two, again, the second one, the two has algebraic multiplicity to.
One only appears with one factor here. So it's the multiplicity one.
So now, step two, we need to find the EIGEN spaces. And again, it's the same thing.
We just need to find bases for the north faces. It's really a critical skill.
So here, let's do the first one. If I take the eigenvalue, land is equal to one,
so then the corresponding EIGEN space will be the null space of a minus the three by three identity matrix.
Well, saving a little bit of time. I'll just tell you, this comes out to be the vector one minus one one.
Lambda has equal to two again, I'll just save a little bit, uh, uh, who did I make a mistake here?
Well, I'm just going to.
So then the eigen value corresponding to to the eigenvectors with the null space of a minus two times the three by three identity matrix.
And the point that when you compute this one, you will actually only get one free variable, so you'll end up with minus one one zero.
If I remember, right?
So that means that we can only find two linearly independent eigenvectors here instead of the required three to pull off diagonal ization.
So the point wasn't really to see the exact reduction here, so I'm not going to go through all of those steps for you.
But because we only have then to linearly independent eigenvectors, then that means that a is not diagnosable.
So then by the dying analyzation theorem.
A is not diagnosable, there is no basis, there is no basis, no matter how clever you are, where your matrix would represent it as a diagonal matrix.
Tommy. That's a good question.
Hum, hmm. But I mean, in terms of the geometry of a.
So it would be telling you about the number of directions in which your matrix is operating and sort of just by scaling,
and that there's essentially only two of those that operates just by scaling.
If it were, say, a larger matrix,
it could be representing that maybe you have something else going on where maybe you have like some kind of a rotation.
Like when I was thinking about my example here, which will come up next class when I'm rotating around,
I have this nice eigenvectors coming from my axis of rotation. But anything in this plane is not just a scaling by a real number.
So then we would want to think about like, how could we diagonals the rest of the transformation?
What would I do with the rest of the coordinates?
And so what's going to come up there is that with the way that we usually express rotation is by multiplying by complex numbers.
So what we really then have in that context of rotation as I spin around, that's how I think about complex numbers is I just get really did.
So. Let's try to repeat the same joke, but sometimes it'll work.
Clearly, if you're thinking about what that would mean is it's telling you something about a number of nicely behaved direction.
The other side of that is that can't diagonal lines.
There are other things that you can do like you could ask whether you could put it in a different diet.
But maybe it's close. Like what kind of matrix would you?
It doesn't happen again. They can offer triangular matrix doesn't seem so bad.
I mean, especially if maybe you can get a bunch of zero, maybe. What nice.
OK, so those are my two examples of one you can diagonals, one you can't diagonals,
so we know that the story is that we can't always diagonals, but we have distinct eigenvalues.
We can't. And that's sort of the one that's most relevant to experiments and data sets anyway.
So I thought what I would do with the rest of my time is go through two examples of working with differential operators,
so a differential operator, you'll recall, is just a linear transformation on function spaces.
So we've been doing a lot of that already, so we'd like to know, and this is especially a context that comes up a lot in, say, quantum mechanics.
So let's take a relatively simple function space the space of polynomials of degree two or less.
So I'm going to define T to go from the space of polynomials of degree two or less to the space of polynomials of degree two or less.
That's what I mean by a differential operator.
It's a linear function on a function space, so it functions and it outputs functions in this case, polynomials.
Jonathan? And space is a vector space where the elements are functions.
So here, like the elements inside a two, these are just polynomials,
like when you talked about the space of continuous functions from zero to two pi like Oh, sine X is now a vector and Cosine X is a vector.
So it really showed the for abstract. So we need to say what this thing is going to do.
So it's a polynomial p and what it's going to do. Is this going to output the polynomial p plus T plus one times the derivative of that polynomial?
So that's just some rule, just like any other function you've seen before in your life, it just does something to the input.
So in this case, it takes the input and then adds the derivative time something.
It's just some example. So the first part is let's take a basis.
One T n t squared.
So this could be a basis for both I and the domain, so it makes sense, and this is a question that I could certainly ask you on, say the midterm.
Here's to find the B matrix of T, so then find.
T relative to the standard base.
The next question I want to ask here is, can I diagonals this operator, which is at the heart of a lot of questions in quantum mechanics?
Yeah. I just mean that it's a function between it's a linear transformation, which phases.
So it's like the derivative is an operator that eats a function, it outputs a function.
So in this case, I'm eating a polynomial of degree two or last thing, a polynomial degree us, so.
So recall, the way that we find the B matrix is we plug in the base factors and we express them in terms of those coordinates.
So here let's compute what tea is applied to one.
Well, that's equal to one plus tea plus one times the derivative of one, which is just zero.
So then this just comes out to be the polynomial constantly equal to one.
Then my second basis factor was tea. So then I take tea, so I'm just plugging it in to this expression.
I'm just following this rule. Plus tea plus one times the derivative of tea, which is just one.
So now simplifying I get to tea plus one.
Now, my third one. T of T squared again, all I'm doing is plugging it into this funny looking operator.
I shouldn't call it funny looking. That's not nice. This. Nice operator.
So I plug it in and I get T squared. Plus T plus one times two T.
So there's the derivative. And then when I multiply this out, I'll have T squared plus two T squared plus two T.
So I get three t squared two plus two T.
So those are my three basic factors when I plug them into T. But now I can't take those as the columns of a matrix because they're not columns,
they're not column vectors, they're not elements in our three.
So what I need to do is I need to express each of those in coordinates relative to the B coordinates.
So in this case, just relative to the standard coordinates.
So if I do that, then I get T of one relative to the system, we'll just be the vector one zero zero.
Because I use one of the first bases factor zero of the two others that take T o t i express
that as a linear combination of those three basis factors and I would get one to zero.
And finally, T of T squared. Written relative to the B coordinates, and I'm taking this thing.
Well, I use zero of the one part, I use two of the tea parts and I use three of the tea squared parts.
So this becomes zero to three.
So that means the matrix, the B matrix, the matrix of T relative to IS B will be equal to one zero zero.
One to zero and zero to three.
There is the matrix. So now I can study this three by three matrix in order to understand my linear operator.
So in particular, I could ask you questions like, Is this injector?
Is it subjective? Is it is it subjective, is it interactive?
Can you tell me? Jonathan, the.
Yeah, so the columns are linearly independent, there's three of them, the convertible matrix theorem applies.
And so we know that it's going to be both inductive and surge active.
It's going to be a bioactive transformation. OK.
Now we shouldn't make this relevant to spend talking about, too.
So now my next question is, can you diag, is this differential operator?
So that would mean, can I find a basis of eigenvectors?
So here to find eigenvectors and eigenvalues this particular matrix, the matrix attached to it?
That's right, perfect, right?
So we already know it's going to be diagnosable because we can we know it's upper triangular base, it's upper triangular,
so we can read off the eigenvalues very quickly because we proved that the upper triangular matrices,
the eigenvalues will just be the diagonal entries. So then the eigenvalues.
R one, two and three. Then we just observed we didn't necessarily write out the formal proof observed, if you have can use,
you can eigenvectors corresponding to each of these eigenvalues, they will be linearly independent.
You have three of them, so they will form an eigen basis. So then the diagnostician theorem tells us that this will be diagonal sizable.
So maybe just for kicks. Why don't we actually do it?
So if I take this Matrix B matrix here, a so if I take a minus the three by three identity matrix.
So now I'm just subtracting one from the diagonal, so I'll get zero one zero zero one two zero zero two.
OK, so now I can read off with the null space of this matrix is the first variable is free on the second to determine.
So this will just be the span of one zero zero.
Now, remember that, yes, I'm sorry, my. Why are we able to?
I mean, the values of this. I don't know of T yet, because the eigenvalues are going to represent tea with respect to and coordinate system, right?
So the eigenvalues don't change when you change coordinates.
That's sort of a fundamental property of the eigenvalues, even if I change the coordinates, the eigenvalues will still be the same.
So that's sort of an invariant quality of the transformation.
So if I change this to consider the sea matrix of this relative to some other basis, the eigenvalues will still come out to be one to three.
How? We know that you could look at the characteristic polynomial and look at them
with the zeros of because any matrix that's related by a change of coordinates,
it'll be like a is equal to PBP inverse. So then you had two problems that problem where you compared the determinant of a with the determinant of.
I think in that case, it was CBC Inverse. But you show that then those determinants were the same thing.
I don't. I'm sorry, I'm not following. I understand.
I don't. So what I'm saying is, is that here the eigenvalues of tea will be the same and any coordinate system.
OK. Because remember, let's just take a brief diversion here.
So if we have a matrix, A is similar to B.
Let's just pause for a moment. Then a is equal to p be p inverse.
Now I claim that the eigenvalues. Of A.
Are the same. As the eigenvalues of the.
OK. Well, think about how you compute I. Euros.
Characteristic polynomial of B. Oh, we can.
Let's spread it out. So if I take the characteristic polynomial of a.
This is, by definition, the determinant of a minus lambda times the NBN identity matrix.
Great. Right? OK. So if a is similar to be you, what can you tell them, what can I replace it with?
Well, then I can just replace this with p p p inverse minus lambda times the n by an identity matrix.
Oh, no, this looks terrible. There's a pea here, I want there to be a pea over here.
How can I get a pea over there where I can rewrite this identity matrix?
Blindness, lambda times, p times, p inverse.
Now I could factor out the pea on the left could factor out the P inverse on the right.
So. Jonathan's going to make me rewrite my next piece so I can have this problem on there.
But. So P B minus lambda times and by an identity matrix times p inverse.
Right. Because I just factored out the pea on the left. The Universe on the right.
Now, the determinant is multiplicative, this part you've actually done, you really,
really did this as your piece, but the determinant is then multiplicative. So that was like a Chapter three.
I like this problem, actually. He.
But oh, the one, yeah, you're right, you're right, you're right, you're right. Thank you.
Thank you. I went on autopilot. You're right. Thanks. Well, now these are just multiplying real numbers together,
actually graded this problem on the piece that so I saw like 200 people telling me an explanation of this problem.
So then these are all now real numbers. So then I can compute this real number, pass this one,
and then I could use the multiplicative it again to rewrite that as the determinant of p times the inverse.
Well, then that becomes the determinant of the identity matrix. That's an upper triangular matrix.
So then that will just be one. So this becomes the determinant of B minus lambda times i n, which is the characteristic polynomial of lambda.
So then if this thing is zero, then this thing is zero. So the eigenvalues are the same in any coordinate system.
So this is a quality of your transformation, not of the basis.
OK, that's what makes it really useful in trying to find a good basis. Yep.
But that doesn't always happen to me, so I'm glad I'm making sense.
So here are the determinant of this quality, right? I have these three matrices together.
I use the multiplying activity of the determinant to then rewrite it this way.
Then these two cancel. So I'm just left with the determinant of B minus this, and then I'm left with what is the characteristic polynomial of B?
So that means, oh yeah. So then that means that the eigenvalues of B will be the same as the eigenvalues of A.
So that means the eigenvalues are telling you something that coordinate independent.
So that's what makes them useful in finding good coordinate systems is because
it's telling you something that's fundamental about the transformation itself.
That's the eigenvalues, yes, then you find the corresponding eigenvectors. Yeah.
All right. Let me look at the clock here. Oh, it's not some time, so we're turning to that answer your question.
Sort of. OK, well, we can come back to it to future.
Well, I think now I well, maybe I'll leave the question up. Sure. OK.
When we when I get the like emails from canvas telling me about the analytics on the videos,
then I'll get like a lot of people watching like certain minutes of this video.
But this are the next point actually comes right back to, I think, Jonathan's question.
I think this all started by saying,
why did the eigenvalues of T tell you about the eigenvalues or the eigenvalues of the matrix tell you about the eigenvalues of the operator t itself?
And the reason why that's true is because these values here are just obtained through a change of coordinates.
And so we we did over there as we established that the eigenvalues themselves don't change.
When you just change coordinates and we can actually observe this,
maybe a calculation will help in convincing us suppose that we took one of these things.
So like the eigen vector corresponding to this needs to be a polynomial.
What polynomial corresponds to one zero zero in the standard coordinates?
What so then, the EIGEN space corresponding to the eigenvalue of one is the span of that polynomial what?
So this is an eigen vector for the matrix. One is an eigen vector for the the operator.
So this is often called an eigen function to really emphasize that the element here is a function.
So we can actually check it. So T of one. So if this is going to be an eigen vector, it should be lambda times T of one.
Right? That's what it would mean. So you are to some scaled version of your original thing, what's lambda in this case?
One. So what were you hope for is that T of one will just be equal to T of one,
which if we plug that in tier one, when we compute, it's just equal to the polynomial one.
Right? So when we plug in of one, uh. So we want to have one to be equal to just times, want some and one when we plug it in.
This was the one plus tea plus one times the derivative, which is zero, which is equal to one.
So this is some input polynomial that is a scaled version of that same input.
So just by way of terminology, again, if you take physics 140 three, you'll see a lot of this.
This is what's called an eigen function. I can function is just fancy for eigenvectors in a function space.
Let's do do I have time for another one? OK. Yes.
Let's see. Oh yes, the second the second eigenvalues actually better illustrates the point.
So let me do one more. Let me do one more, and then I'm probably going to run out of time.
So if I take a land is equal to two, so then I'm looking at a minus two times the three by three identity matrix.
So again, remember there is my A. So you get minus one one zero zero zero two and we get zero zero one.
So now the eigen the null space of this matrix is a little bit more interesting.
So now it's the span of the vector one one zero.
That's not the eigen space, because remember, eigenvectors need to live in the domain of your operator in the domain of your function.
So what's the corresponding polynomial to this thing? Jonathan?
One plus t, so that means the Eigen space correspondent. But.
So now let's check that let's go over here and see if that actually comes out to be what we hope and I can function should be.
So now if I compute T of one plus T, this will be equal to.
Well, this function again one plus T plus one plus T times the derivative of one plus T, which is just one.
So that becomes two times one plus T. So what does just?
We're only scaling your input so that it's an I can function again.
The third one, I'll just tell you. And the third.
You will end up with the eigen space corresponding to the span of one plus two T plus T square.
So now, just in my last little bit of time here, let's compute the The Matrix for your transformation relative to these basic factors.
So now let's call it the basis of one one plus T and one plus two T plus t squared.
So this is an eigen basis. They're linearly independent. We can find now T relative to C.
Would you expect this matrix to look like? What do you hope for?
You hope it's diagonal, right, because that was the whole point of getting an eigen basis. So by definition, this will be two of your first one.
One written relative to the sea coordinates T of one plus T written relative to your sea
coordinates and T of one plus two T plus T squared written relative to your C coordinates.
OK. Well, the first one just becomes one again, we checked that one.
So we want to write one as a linear combination of these. The second one we just computed over there T of one plus T was two times one plus T.
The third one, if we computed that one,
which I'm not going to do because of the running out of time will then just become three times one plus two T plus t squared.
So now, if I wanted to write, one is a linear combination of these three, it's one 00 phone, right?
Two times one plus two is a linear combination of these. I take zero times this one two times this one, zero times this one,
so I get one zero zero zero two zero and the third one is just three times the third basis vector, so it's zero zero one.
So indeed, it does give me a diagonal representation of this particular operator.
OK, so I think I'm out of time. I can.
In fact, when a minute over, I guess we're going a minute over the third.
The last one on there is, I think, another one that's nice to play around with just to make sure that you understand how to do those.
The solutions are already posted to canvas.
If you want to see my solution to these particular problems, but otherwise feel free to ask about them in office hours.
I'm happy to chat about them and I'll see everyone later this evening.
All right. So you sort going to.
