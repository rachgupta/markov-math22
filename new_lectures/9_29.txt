So just a few quick announcements.
I'm sorry, the math department, we're having a lot of trouble with our copier, so I don't have copies of the hand out today.
Hopefully it'll be back to normal on Friday. The so the handout is on canvas if you want to work with the digital copy there.
But otherwise, through today's class, I'll try to be more careful about writing down the problems from the handout
on the on the board as we do them in terms of what's coming up in the class.
I did email everyone to let them know that piece set for essentially you have an extension on that one until Friday if you want to.
If you have one person, if you've already put it in, and certainly you can use that extension at a different point in the semester.
So just maybe that gives you a little bit of flexibility in what's coming up.
I've already posted problems at 5:00. It's a pretty similar length to problems at 4:00.
So the workload, I think, should be relatively stable through October.
So keep that in mind as you're allocating time.
Good. If you're in terms of the reading, the reading for this class, I think is really helpful and important to do, to deepen your understanding,
to see sort of another perspective on the material, to really make sure that you fill in any gaps that maybe weren't clear from Class Anarcho.
Not. OK, then I maybe I didn't get it posted yet, then I'll post it when I get back.
I teach until 3:00 today. When I get back to my office around 3:00, I'll post it.
But it is a similar length to be set for. I think the reading for the next few chapters is particularly useful.
So chapters two and three in the textbook tend to go by pretty quickly.
I think it's worthwhile to at least make sure that you're skimming that material to make sure you're comfortable with all of it.
So today, well, roughly speaking, get through to point one to point to Max Klatt, to point to, well, next class, we'll do our last poopy day.
Then we'll do two point two and two point three on Monday. Are there any questions with.
Sir. Oh, yes.
So I'm going to do my best to have the midterms graded by Friday. That's.
That's the goal, that's the goal.
So I don't know if that makes people happy or sad, but as a team, I think it's better that we get the information out as quickly as possible.
So we will have those ideally by Friday, barring catastrophe.
So, Jonathan. So I don't think that you should and I don't agree that it was OK, so the practice exam was longer,
and when we gave that as an actual exam, the students also found that to be quite a challenging exam.
To the most challenging question on that exam for most students in the past was
the question on equivalence classes from talking to students in office hours.
That seemed like the most worrying question that people were anticipating.
It did not appear on the exam. OK, so I don't know if that helps at all, but.
The intention was certainly not to write an exam that was wildly different from the practice exam.
Our intention is to write the practice exams to be very similar to the exam.
The questions on the practice exam were questions that we would have been happy to give on the actual exam.
The questions on the actual exam were questions we would be happy to give on the practice exam.
So any exam you take,
I think is going to feel much more stressful than an exam you take on a practice exam that you take because you're taking it in a different format,
OK. So I realized that I think.
I realize that that's sort of that it feels very stressful to take.
Exams for the first time in more than a year again, and so just getting back into that,
it's going to feel a little bit different and a little bit uncomfortable, a little bit unpleasant.
But I mean, I think it's.
It's a it's important to remember that we want to use the exam, we want to use the quizzes, we want to use the problem sets as a learning opportunity.
OK, so one, if you have, say, a bad performance on one exam, we've all had bad performances on one exam.
We've all had exams where we do really well on the exam, even though we maybe didn't study as much as we meant to.
I mean, it doesn't always. One exam.
Well, Sam. Is we have problems at work problem.
To try to get a more complete picture understanding of. The.
I don't think that that dictates a particular outcome in this class, I think any grade is still possible.
I think that every semester we see some students do really, really well on exam one and then it falls off the rest of the semester.
We see students that do really, really poorly on Exam one,
and then they do really well the rest of the class and get an A and we're so happy and excited about that.
And so we're less excited about the first outcome where people do well and fall off because we really do want you to do well.
OK, we're we're we want you to learn this material.
And it is a very deep way so that you can use it and move on from it and use it in a way that's your course.
So I think I really don't want to have an adversarial relationship between the two of them.
So we're not like, oh, we got you on this one. We're just trying to present problems where if you don't understand that problem,
well, then you learn that problem, you learn that material and we move on from there.
OK, that's why I'm giving you quiz problems to give you that feedback where you can say how to do that.
Let me think about how to do that so I know for next time so I can deepen my understanding so I can move on from that.
That's the same motivation for the midterms, is to give you this sort of diagnostic feedback, maybe in how you're in the class.
Sometimes in particular, it could be approaching the exam for many of us.
I'm sure that you were all very strong test takers at some point in your life,
but we're all pretty out of practice and taking exams and especially timed exams.
And so there might be some amount of thought that needs to be given to just timing during the exam itself,
like making sure that we're spending less than five minutes at the very most on the definitions at the very beginning.
I was just grading that question earlier this morning.
For many students, people were writing way too much for that question, way more than just the definition.
And so not only is that not the definition anymore,
because you're including more so it's actually a detriment to your grade, but it's also more taking more time.
Move on, just the same thing I wrote on the solutions of the practice test,
the practice that started with question one being the exact same question as on the actual exam,
a few of the other questions were very, very similar as well. The separate question, I just changed the sets.
So hopefully that was also a question that people were relatively comfortable with.
But the format of the second exam will be similar.
If you don't think it's helpful. I'm happy to not provide a practice exam, but we'll do if we do write a practice exam for the next exam,
I will do my best to make sure it's wildly harder than the actual exam.
Yes, I'm grading a fair amount of the exams.
Caleb's also grading a fair amount of the exams, so you should be nice to him too.
So and yes, I am grading a lot of them.
I think I have six hours and already of grading, so there'll be quite a few more hours over the next few days.
Yes, Luke. It counts towards your paycheck, right?
Yeah. You want more of those?
One reason I'll just tell you my own thinking about this and I mean people can convince me otherwise in this particular point,
my feeling towards extra credit is that it's I like giving you problems.
I think it's fun to talk about math and show more math problems. So in some sense, I'm happy to give zillions of extra credit problems.
It becomes, to a point, unfair, I think, because you don't all have the same amount of time to work on those extra credit problems.
And so I don't want to go overboard with that just because some students this might
they might have one hundred hours that they want to allocate just to this class.
But most students don't have that ability.
So I'm reluctant to give tons and tons of extra credit just because I think it can quickly become overwhelming.
And I also don't want that to be the way that people kind of get through the class is just by doing tons of extra credit.
I want to focus on the core learning objectives. OK, other questions.
Have I quelled the uprising, is it going to be OK if I turn my back?
All right, so I think this is honestly a very good time to start catching your breath with the material.
If you've been feeling uncomfortable with the linear algebra, the next two chapters are really a place where most students do find a time where
they both integrate their knowledge from the things they've been seeing before,
and they get much more comfortable with the linear algebra going forward.
So let's have a bit more of a relaxed day about the Matrix operations and Matrix arithmetic.
So this is starting Chapter two, so let's just give a name to a particular matrix.
I want an M and Matrix, so let's give a name to the columns, a one through A and this will be an M by and Matrix.
So it's there's some standard notation that has not really come up for us in this particular class,
but just to make sure that we all are comfortable with this,
we often need to refer to the various entries in the given matrix and we use the indexing oh by column.
So I'll take a one one to refer to the entry in column one and one.
Similarly, I'll take a one two to be the entry in row one column to out to the entry in row one column.
And so this is sort of standard notation for how we can refer to the particular entries in this matrix.
Similarly, going down, we have a one to A and so this is to refer to the entries in a given matrix.
Sometimes we'll use the shorthand, a J like that to just show we're indexing the entries.
If I want to refer to a specific entry in The Matrix, I'll often use the following notation.
So if I take a matrix and I put an IJA here, this is just the entry of that matrix, so it's just a bit of notation that we're using here.
So maybe to make sure it appears in your notes, note a IJA is the entry.
Of a in row. I can call them.
Jay. All right, so just as a quick example, there are a lot of ways that we can work with matrices just the same way we've been working with vectors.
So, again, let's go through this in a little bit of a relaxed way. So let's take The Matrix.
One, two, three, four, five, six, going across the rows and let's take the Matrix B to be two, three, four, five, six, seven.
So there are several things that we can do that are probably not too surprising,
I can define the sum of these two matrices to just add them up opponent wise the same way you would add two vectors together.
So namely, this will be the one one entry of this matrix will be one plus two to the one two entry to three three plus four four plus five,
five, six and six or seven.
So then we get a new two by three matrix just coming from the corresponding entries.
Similarly, as we scale vectors, we fail every entry. We do the same thing to a given matrix.
So we also have two times the Matrix B will then just be two times all of these entries.
So four, six, eight, 10, 12, 14.
This gives us now a bit of matrix arithmetic that we do.
We have now two Matrix operations. We can add two matrices of the same size together and we can scale them like a given scalar.
There are many properties of Matrix arithmetic.
We should observe them so that we can use them.
So we don't have to reprove these results, so let's just take A, B and C to be matrices of the same size.
Furthermore, we'll take two scalars or a. then a whole bunch of things are true if I add A plus B,
this is the same thing as adding B plus A matrix addition is commutative.
Similarly, matrix addition is associative.
So if we have A plus B and then you first do that and then you add C, this will be the same thing as if you took A plus, B plus.
OK, three, if I take a and I add to a matrix consisting of all zeros, just like if you took the zero vector, then this would just give me back at.
So all of these results should be results, they should be comfortable proving they're mostly just kind of notation.
So for we have distributive city.
So if we have our times, the sum of two matrices, this will be the same thing as our times, the first matrix plus our times, the second matrix.
So scalar multiplication distributes across Matrix edition.
Similarly, if I have scalar additions are plus ass times a day,
that will also distribute in exactly the way that you would hope that this becomes R times A plus s times.
Right. The last one that we want to record is just this associativity of scalar multiplication.
So we have our times as times a it doesn't matter if you first did as times your matrix and then an R times your matrix,
you could also do our Tansy's as real numbers and then use that to scale your matrix.
Same thing.
So so far, the takeaway that you should be seeing from this, hopefully, is that nothing too surprising is happening yet where they're being yet.
So I'm going to do something kind of. Through exercise.
By that, I do not mean you need to do this as an exercise to turn in by any means, but you should be comfortable approving any of those results.
OK. So there are other Matrix operations as well, some probably even familiar with the most important one is going to be matrix multiplication.
So it's kind of funny that we've been this long into a matrix linear algebra class and we're only talking about matrix multiplication now.
But that's OK. We've been working on other things.
So I really want to make sure that you understand where matrix multiplication is coming from,
because for those of you that have certainly seen matrix multiplication before,
it's a little bit of a strange rule, the strange way of multiplying two together.
So the question I would want you to have in the back of your mind is why is it that strange rule I'm going to think about from our perspective,
why it is what it is? And then for those of you that maybe even in your final projects, there are other ways of defining matrix.
Multiplication are useful in other contexts, including in machine learning,
where there are other ways of multiplying matrices together that become very relevant.
But the most natural one is the one that we will consider. So the starting point is going to be taking some vector X in, say, r n.
And then I want to apply some matrix to it, say some Matrix B, B will be an M by NT Matrix.
So where will the output of multiplying by B live? What space, if it's NBN?
Or em, right, the number of rose, so multiplying by this matrix B,
so we think about that as a matrix transformation or a particular instance of a linear transformation.
We showed last time that all linear transformations have an associated matrix and we can explicitly find it.
But now I want to do another linear transformation. So I want to compose together these two functions.
So here we're going to do another one multiplying by the Matrix A going into Arpit.
So this would then be a times. B of X.
So what are the dimensions of Matrix A. What kind of matrix is that, if it's going from our M to our P?
And so this thing A is a P by M Matrix.
He em and this one is made by. And.
OK, so if we're thinking about this in terms of linear transformations,
in terms of composing together two maps, we then get the composition which goes from here to here.
So this is what I want matrix multiplication to represent. I want that to be taking a B, whatever that is, times X.
That's the way that I want to define this. I want to define it so that it represents composition of linear functions.
So we know every linear transformation has an associated matrix, the associated matrix of that linear transformation.
I'll call the Matrix a B of multiplying those two matrices together.
So what dimension should this have given that it's a linear function going from our NP to our P?
And so this is done up by an matrixx.
Baby. So now the goal is I want to know exactly what this matrix is or represent a linear transformation
so we know how we can use the properties of linear functions to find what the associated matrix is,
the goal. So we want to find the associated matrix.
Well, let's just sort of do this piece by piece, so if we had this B times X, so that's something we've defined already.
So that represents the linear combination of the columns of B, using the weights coming from X.
So this is then X one times the first column will start at that plus X and times the end column.
So that's just the definition of how we defined the matrix vector.
OK, well, now we could do the same thing of A times, B of X, because that's how we were getting at this composition.
You first to be then you do a to the output of that. So now we can plug in what this means.
So this is a times the linear combination, X one, V one, plus DataDot plus X in the N.
OK, well now you have a linear combination of some vectors, B, one through B n so from the properties of matrix times vector matrix times A vector,
we can distribute this across and pull through the scalars using linearity if you want.
So this becomes x1 times A B one plus plus X and a B it if you want by linear.
I'll put parentheses in just to make.
So now this thing, so a big one that's going to be giving you some vector and R.P., so we have done some linear combination of vectors in RPE,
how could we how do we often represent that so linear when we have a linear combination of a bunch of vectors?
How else do we think about that? Using a matrix.
That's exactly right. So we could think about this is representing the Matrix where a B one is the first column,
Abnd is the end column and X one through X, N are the weights.
So this would be a B one is your first column through a B and your end column times the vector X one through X at.
So exactly tells us what we get for the columns when you multiply the matrices together, so thus we now define.
A times, B, multiplying these matrices together or rather eight times bigger.
So remember, that's between doing B first and then A, multiplying eight times.
B, we define this to be the matrix A times the first column of B up through A times,
the tenth column of the state again tells us we're going to get and each of them are going to then have the number of same number of rows as a.
So we get them up by and matrix as we were expecting.
So if you're thinking about that question of why do we define matrix multiplication and sort of a funny way,
why don't we define it in the same way as like vector addition or Matrix edition where we do it component wise?
The reason is because we want to function.
In order for it to represent competition of linear functions, to doing one function and then another, you need to define it in this way.
So the definition is forced upon us by what you want it to do.
All right, so let's do again a quick example, make sure we can actually multiply some matrices together.
So we have the Matrix two, three, one minus five, Matrix B will be four one three, negative two six three.
So I want to compute then A times B, so this is the result of this should be a two by two matrix multiplied by a two by three matrix.
So the end result should be, uh, give us a two by three matrix.
So here by definition, this is a time your first vector B one eight hundred second vector B two eight times your third vector B three.
So now each of.
We're talking these as your wait on these two particular columns, so that tells me my first entry here two times four plus three times one,
my second entry here will be four, one times four plus negative, five times one.
Similarly, we'll do this row by this column now, so then we'll get, um.
Let me just put in the entries, zero three twenty one minus nine.
So the first question you should ask yourself is this corresponded to first doing this and then doing it.
So what is the.
What would this correspondent like, why can't you computer?
Yeah, exactly right. I mean,
think about what a does A takes an element in or two and it spits out an element in order to be takes inputs and r3 and spits out outputs in order to.
So there's not a way to match up the inputs with outputs in this order.
So here I can't multiply these together because it doesn't fit the definition of matrix multiplication.
So this is then not possible. Tommy.
Thirteen. Let's see for this entry I'm doing one times three.
Yes, 13. Thanks. Yes, yes.
Are willing to change. So here, when I first do this, I take some vector that has N components, I multiply.
And of Muslims are taking a linear combination with the outputs are going to be an R m of those end columns to then get.
So this thing is. So it has em in there and I'm looking that into a major.
Where of the 10? And Carl.
I'll put on each of those columns in order to get it, and he.
Sorry if the audio is cutting out there, I shouldn't just go that way. Other questions.
So make sense. But.
All right. Oh, we now know the definition of matrix multiplication, so we and we know situations where it's not going to work,
where they don't line up, when you can't actually define those two matrices together.
Just like for Matrix additions, we needed the matrices to be of the same size in order for this to make sense.
So. Add together any random matrices or multiply together random list,
there's some care that needs to be given to make sure that they're representing what we think they represent.
OK, so a few observations. So one thing that's useful often is to note how we get these entries, so like this particular entry,
the one one entry was coming from the row one of a times, the column one of B, so that pattern persists and we can observe that.
So, for instance, if I multiply eight times, B, well, this will just be say,
the one one entry of that matrix will just be the one one entry of a times,
the one one entry of B, but then I want to go across the row of A and down the column of B, so this will then become a one two times B two one.
So now I'm in the second position in the first row of A and I'm in the second position of the second of the first column of B out to the end.
So this will be a one. An and B and one.
More generally, the same pattern holds for the eigth entry.
So now I just need to make sure I'm going across row I so it'll be A I one B one J plus dot dot out to A I and the and J.
So that's just recording how we do, how we can get a specific entry in the Matrix when we multiply.
And we can see that exactly showing up, say,
in this example where I'm multiplying across the first row of a down the first column of B for that one one entry.
So the same we would like the same thing we had before, four properties of Matrix, edition and scalar multiplication.
We would now like to have some properties of matrix multiplication. Lenegan.
Nice to have it. We don't have to reprove them all the time. So, for instance, matrix multiplication is associative,
this immediately follows because we can identify matrix multiplication with
composition of linear functions and we know composing functions is associative, so you can rely on previous knowledge that you have.
So this is the same as A, B, C, so matrix multiplication is associative.
It's also distributes across Matrix Edition. Again, assuming that these are all of sizes where these things make sense a, b,
c so we can distribute the AI through, we can distribute on the other side.
So if you had B plus C times A, this will be the same as B times A plus C times.
We could also say scale our given matrix or our product of our matrices or our times a b so it'll be the
same thing as if I first scaled A or if I scaled B as multiplying and B and then scaling the result.
So this is the same thing as R times A times B, which is also the same thing as A times r b.
There's also an important special matrix that we can multiply by to get exactly the same thing,
just like when you have functions and you compose them together, you can have the identity function, Jonathan.
I don't feel like. So what I'm doing and see is I'm multiplying this some on the right by a matrix.
So I want to be able to distribute it through. So the first term is coming from putting it through.
So I get by. The second term is coming from distributing it through and getting s.A.
These are different matrices, yeah. So I need all of the mattresses here to be matrices where it makes sense to do matrix multiplication.
So they have to be compatibly shaped. So one special function that you've seen before is the identity function.
So everybody is equal that spits out the same output, the input into it.
What is the matrix of that linear transformation? If you just take the linear transformation?
Tiebacks is equal X. Yeah. Can you tell me what the identity matrixes?
Uh. Nice, right?
So if we wanted to find the associated matrix to this linear transformation, T of X is equal to X,
how would we do that if we were just sort of following what we've done so far in class?
How would we do that? How do we find the associated matrix to a given linear transformation?
Arjun. One.
So that's exactly right, right, so we just plug any one, two, three, four and five up to 10, if it's the identity,
the identity transformation, it's just going to send each of those to the exact same output.
So that means your matrix will then be the one e one, two, three, four of the end.
And that's exactly right that we're then thinking about and we're calling it the identity matrix.
So it's representing the linear transformation that doesn't really do anything. So here if I had this identity matrix,
so loved ones along the diagonal and zeros off the diagonal and you multiply by the matrix, a, what do you expect to get back?
So if A is an MBA and matrix, that means this we'll call I am because it's a square matrix that has columns.
If A is then an end by Matrix, we can also multiply by the end by N identity matrix.
So ones along the diagonal and zeros off the diagonal to get back as well.
So this thing we call I. N the end by an identity matrix because A is not necessarily square.
This identity matrix might be of a different size than this identity matrix.
OK, so, again, the proofs of these are relatively straightforward,
it's mostly just kind of chasing through the identities and computing what they come out to be.
I recommend trying one yourself just to make sure that you could do it.
And if you want to see any of them worked out, just come to office hours like.
So that's a great question, so I want to know, like what thing I can multiply by B in order to get back B. Well,
it's different on the right versus the left right. So if I took this matrix over here, B and I wanted to put something on the left,
that means that I would have to have what size while I'm doing row by column.
So that means I need to. Right, but now if I put it over here on the left, I put it on the right, I'm doing row by column.
So that means I need to match the number of columns here with the number of rows here.
So this would be one zero zero zero one zero zero zero one.
Does that make sense? All right, so what I wanted to do now, so so far, I claim that everything that I've presented is exactly as you would expect,
that might not necessarily be the case, but it's at least probably not wildly different from what you would expect.
The other hand, things do get a little bit different.
So in the spirit of the types of questions that you see on problems and so forth,
I wanted to give you some true problems that are of the variety prove I give a counterexample.
So here are three, four true or false questions that I want to do. So true or false?
So some practice prove or give a counterexample questions. Is it true that a B is in general equal to a B, B, A four to three and by and matrices.
So assume that they're all square. So suppose. And we are just and by nature, since they're square matrices.
So the first statement I want you to consider, whether it's true or false, is that one.
So just like on a quiz, a midterm or a problem, that I want a concrete counterexample with actual numbers, if you say it's not true.
Similarly, if a B equals the zero matrix, then is it true that A equals zero or B equals zero?
Third one, see, suppose we know that a squared is equal to zero, then, is it true that A is equal to zero D.
Suppose that if you knew A times B was equal to eight times C, then is it necessarily true that B is equal to C?
So just to start off, if you take these to be one by one matrices, that number a case each of these three, four properties hold true.
So I'm asking you for larger N say and is at least two.
Is it true if you have square matrices that are at least two by two matrices, are each of these four statements true or false?
Can you give me a counterexample? Can you give me a proof? OK. So me try it.
Why don't we take a, I don't know, a few minutes,
not super long and discuss with the person next to you and tell me whether you think they're true or false, if they're false?
Give me a counterexample. If they're true, prove them. Negative one point.
It's. Come back together and discuss this one, Caleb informs me that the microphone has been cutting in and out a little bit,
so if you can't hear me, feel free to just, like, raise your hand. I'm happy to repeat myself.
I mean, that's just generally true.
I mean, I apologize for more technological glitches, but I thought last year would be the peak of technological problems, but apparently not.
OK, so thinking about these problems, what do you think?
What do you think? Yes, sure, which one?
OK, a is false. OK, so again, I mean, like, if you were doing this on, say, an exam or something,
you might not have a strong feeling to start with, whether it's true or false.
So my advice in those situations is just try something and see what happens.
So I'm going to pick some random matrices. I'll take a to be the Matrix.
One, two, three, four. Will they be to be this other random matrix?
Zero one minus one one. I mean, lots of matrices will work here.
It doesn't particularly matter what you pick. Then we just compute eight times.
So one, two, three, four times zero one minus one one.
So then when I multiply this row by this column, we're then getting minus two.
When I multiply this row by this column, then I'm getting three.
When I multiply this row by this column, I get minus four.
When I multiply this row by this column, I get seven. So now we just need to compute the other way.
The so zero one minus one one times.
One, two, three, four. So now when I compute this one, well, I have a zero there.
So it's just going to be picking out the second row. So it's three four.
So I can already see that they're not going to be equal, but I might as well compute the whole thing just for practice.
So then this will be minus one. Plus three is two. Then this one we minus two.
Plus four is two. And hey look a B is definitely not equal to the.
So this single example, of course, doesn't illustrate that they could never be equal because if you did take,
say, A to B, the identity matrix, the identity matrix does commute.
But in general matrix, multiplication is not commutative. So this is wildly false.
So here's another property. We use a lot for real numbers that if you multiply two real numbers together and you get zero,
then, you know, one of those two factors has to be equal to zero for matrices.
Again, this is false. It's maybe a little bit trickier to figure out an example, but not too difficult.
Again, my advice when you're thinking about counterexamples for questions like these,
take small matrices or stick with two by two and pick easy numbers like small numbers of zeros and ones are usually a good idea.
Zero one zero zero multiplied by zero zero one zero.
So there's my age might be. So let's verify both of these are non-zero matrices when I multiply them together.
Um, that's not going to work. Let me start and let me move this one.
This one. There we go. So if these two matrices multiply them together, then I'll get zero zero zero zero.
So I've multiplied together to non-zero matrices to get the zero matrix.
So that's a little bit odd. Something a little bit different. Just multiplying real numbers together.
But this is all want to see us take a to be equal to zero one zero zero.
So that's something that's not the zero matrix. But when you square it, it comes to zero matrix.
So the behavior can be different and more interesting.
So this last one time when we're solving equations that we want to cancel out a factor.
Right. So right here, I would like to cancel out this and conclude that B has to be equal to see.
You're probably noticing a theme here among these problems.
And again, it's false. So, for instance,
you could take the Matrix A is equal to zero one zero zero zero one zero one B could be
the matrix one one zero one and say let's take C to be the Matrix two three zero one.
So we need to compute these various things and make sure that they actually work out.
It's important to check that we actually have a counterexample.
So remember, we want a B to equal AC, but B, to be different from C, so we know B and C are definitely different.
So that's at least good. We just want to make sure AC equals B C, so if we take AC, this will be the Matrix zero one zero one.
If we e c and you multiply these together, B, I'm C so row by column.
Oh, sorry, I don't want to see I want to see AC that makes more sense.
AC. So I already have a C, I want a B, so now when I multiply these together,
so I'm going to pick out the second row and here I'm going to pick out the second row again, which gives me zero one zero one zero one zero.
So hence we have a B is equal to A C and B is not equal to C.
Yes, it's like. Zero one zero zero zero two zero zero zero each other.
You can do things like that. I mean, there's no objection to using a smaller problem.
I mean, I think you want to do the smallest problem. What can see property fail.
So, I mean, I wouldn't start by trying to be super clever on these problems.
I'd start by just writing down something and seeing what happens. Yeah, so these statements are related, certainly.
I mean, same thing when you think about these two are definitely related. So here this is often called the cancelation property.
So the question that should be lurking in your mind is, well, we can't always cancel.
Can we ever cancel? Is it ever possible that we could cancel out these matrices so that we can conclude B is equal to C?
So that's certainly something that we should keep in the back of our mind as we continue on.
So in the spirit of Matrix operations today, we've seen Matrix Edition,
we've seen scaling a matrix, we've seen matrix multiplication and we've seen properties thereof.
We've seen ways in which it behaves like we would expect and we've seen some ways in which it behaves differently.
There's one other property that we would like to see today, one other operation, the transpose of a matrix.
So we'll define the transpose of a matrix. And the following way, so if we're given an M by and Matrix A, then the transpose of A.
Uh, which is denoted by a with a superscript of a T.
So he transpose is the NT by M Matrix,
so we flipped the roles of those two and it's just obtained by taking the columns of your original matrix as the rows of your new matrix.
So given. By. Taking the columns.
Uh, as the rose. Of Trans-Pecos, so just to be really concrete, let's see an instance at.
So example, say you take the Matrix B to be one, two, three, four, five, six, so then B, transpose.
I want to take the the columns of A as the rows of A transpose so that I'm taking this column as my rose or one three five two four six.
There is a transpose so similar to the properties of matrix multiplication, their properties of matrix transposition.
So let's observe a few of those so you can regard this matrix as you're just in the eye and positions,
so the IJA position of A is the same as the eye position of a transpose.
So, for instance, if I take a transpose and then I take the transpose of that, what do you expect to get a great.
It's exactly right. So if you take the transpose twice, we get back.
If we take the transpose over a sum, then that's the same as the sum of the transposes.
Right. Similarly, we have for all scalars are we have our times, your matrix transposes the same thing as our times a transport.
What are properties B and C remind you of?
It's telling us that something is a linear transformation, so we'll make that notion precise a little bit later on in the semester.
But again, these two conditions are very much reminding you of linearity.
Again, the last one, if I take a B transpose that this is the same thing as B transpose e transpose.
So you should note that the order of those two factors did get switched on.
So there are a few good reasons to think about why they might get switched.
If you think about your matrix A, it's an M by and matrix, your matrix B then has to match.
Maybe it's an NYP matrix. So the result of doing that will then give you an M by matrix.
We take the transposition of that. That gives us a P by and Matrix, a P by Matrix to in order to do these two multiplications to get the same thing,
we would have to flip the order that we're multiplying them by in order for them to be able to be composed together.
OK, we see how much time to have time. So.
Do I want to do this, let's see. Let's move on.
So I think A, B and C are pretty reasonable proofs to write down there, again, just kind of working through the notation.
The notation does pose some challenges, but they're good problems to practice on.
D, however, I think is a little bit more tricky. So why don't we prove.
Proof of the.
So one strategy for proving that two matrices are the same would be to show that the IJA entry and one is the same as the IJA entry in the other.
That's roughly speaking, what I'm going to do here. OK, so let's just recall how we were able to compute the idea of just any given matrix at all.