It's good to see so many people I know I've gotten a lot of emails from people saying they're already traveling and hence couldn't be here today.
0:06
So I do appreciate that there's a good representation of students here.
0:14
So let me just quickly run through some announcements. So you have an idea of what's coming.
0:19
We have Thanksgiving break for the rest of the week, so we won't have class on Wednesday and Friday.
0:26
Then we will have our two remaining classes on Monday and Wednesday of the week after next week,
0:34
the week after Thanksgiving break, problems at 12 or very last problems that his due Wednesday, December 1st.
0:41
So keep that in mind. The final web work is also do.
0:52
Then the major thing that you should be keeping in mind is the project.
0:56
Drafts are due on Wednesday, December 1st, so that's pretty soon.
1:01
I would encourage you very much to like before you leave Town,
1:09
make sure that you talk with your whoever you're working with and make a plan so you have a clear idea of when things are going to happen.
1:12
The projects that tend to score the highest are the ones that are closest to being completed the stage, the it's not the I shouldn't say this,
1:21
the you'll get much higher quality feedback if your project is more intact when you turn in the draft.
1:34
If there's large sections where like, we're going to do this, then the greater can really only say, Well, looking forward to reading that.
1:39
I mean, there's not much more to really more feedback than you could give if the sections doesn't exist.
1:46
So keep that in mind when you're thinking about what's what's the remaining things that are do for us?
1:55
So now you've finished Quiz six, so that's really all that's left is the final product and peace at 12 and the last web work.
2:02
OK, so just those last three things for our class and then we're done for the semester.
2:12
Are there any questions on how the should I go through the timeline for how the projects will look?
2:17
OK, a few people are nodding. So what's going to happen with the project? So now you should all from.
2:23
It was a little bit messy when we released the grades for peace at 12 because I released them before they were.
2:28
We were all finished grading just so that you could get feedback a little bit sooner, but from peace at war or peace at 10, I'm sorry.
2:34
Peace at 10. You should have now a reader assigned to your project,
2:40
so you should know who the first reader is going to be when we grade these at the end of the semester.
2:45
You should feel free to reach out to that person to get feedback. You should feel free to reach out to any of us.
2:50
Come to office hours. Ask questions.
2:55
But keep in mind that the first person to read and evaluate your project will be the TRF that was assigned on piece at 10.
2:59
So on December 1st, you'll turn in your project drafts and they're all going to be amazing, I'm going to be really excited to read them.
3:09
Then that evening, you'll all be assigned to peer reviews,
3:16
so this will be done automatically on canvas and you'll get to then see what some other people have done for their project projects.
3:21
You, you're part of your project grade will be the sort of thoughtfulness of the feedback that you give to your classmates.
3:29
So I mean, I don't think this is something you need to spend many hours on, but certainly give it like an honest read.
3:38
Give your feedback. Say if there's places where you don't really understand what's happening,
3:45
say like how things where you think you could be constructive or make suggestions for improvement.
3:50
If you notice like glaring structural things like they haven't inserted any references yet,
3:54
that will be a part of their grade point that out, if it seems like the references are largely incomplete or not precise.
3:59
That's a good thing to point out.
4:06
If there's something about the project that could be certainly improved, make sure that you indicate that in some way.
4:07
So that will be do I think that's do I think you have from Wednesday until Monday to do that also between Wednesday and I think Tuesday,
4:16
you should check in with your t.f reader in some way, whether that's meeting in person,
4:27
which is what I'm hoping to do with the groups that I'm working with. Have a brief conversation or I'll mark up your draft.
4:32
I'll give you lots of comments on things that I want to see different in the final version.
4:39
And then it's up to you to decide how much of those comments that you want to take when the final version is due on December 13th.
4:44
Yes. It should say on grade scope on the comments, if yours doesn't say it should be one of your group mates,
4:51
they might have commented on the submission from one of your crewmates. So I would check in with your with your who, whoever you're working with.
5:01
And if none of you can figure out who the operator is,
5:08
then there might be an oversight or a glitch and then in which case just email me and I'll be happy to sort of figure it out.
5:12
But I think everyone should now have comments on great scope for peace at 10 as of last night at around like two a.m. or something.
5:20
So does that kind of give you an idea of what the timeline for the project should look like? Fish.
5:31
So the one thing that we're going to be looking for in the cover letter that you submit,
5:38
which is a required part of your final submission, is that you indicate how you're taking into account the feedback that we're giving.
5:43
So in particular, so I'm thinking of this is sort of like the journal submission process.
5:50
When you submit a research paper to a journal, it goes to some editor.
5:54
The editor decides what referees to send it out to you.
5:58
The peer reviews will be the referees for the paper.
6:02
You'll give a bunch of feedback that the reader will be another person giving feedback on the paper.
6:05
Then you, as the authors of the paper, get a chance to respond to those comments.
6:10
You are not required to follow every piece of advice that you get.
6:16
And in fact, what often happens both in the math twenty two and also in the publication process for
6:21
research papers is you'll get contradictory advice on how to make your paper better.
6:27
Some people will say you need to delete this section entirely and someone else to say this is the best part of the paper.
6:31
You need to improve this section.
6:36
So then, as the authors, you need to think about what's actually going to make your paper as strong as it can possibly be.
6:38
And sometimes that's a subtle call. But as a part of your cover letter that you're writing to me that you need to make sure that you have put forth an
6:45
argument for why you're not following the advice of someone and why you are following the advice of someone else.
6:55
What changes are you're making to your paper? So makes sense.
7:01
Any questions, comments, concerns? All right.
7:05
Great. If any comments or concerns do appear. I hope all of you would feel free to just email me and I can.
7:12
We would be happy to try to answer those questions as quickly as I can.
7:18
So we have three classes left, and none of this material is particularly relevant for 20 to be.
7:23
But it is nice material. It's exciting material. I think it's fun stuff.
7:32
So I think of this is kind of like capstone of 20 to a certainly material that's very widely applicable.
7:35
So and in fact, I think it leads nicely into several of the projects that you might read about.
7:43
So it's, I think, good material to know.
7:50
But again, you might not necessarily need to agonize over every last word, as you would if we were having a final exam on this material.
7:52
So what the idea is, the goal for today's class is that in Chapter six,
8:02
we were able to start doing geometry in our end because we introduced another mathematical structure on our vector
8:09
space that allowed us to measure distances that allowed us to measure the lengths that allowed us to measure angles.
8:15
And that's sort of what you need in order to do geometry. So what now?
8:21
What we would like to do is we would like to do geometry in an abstract vector space.
8:26
We'd like to go beyond just doing geometry in our end. And so what we want, then our goal.
8:30
Just like our big goal from airspaces was to abstract the essential.
8:40
There are a great. Well, Oren had some additional properties that we could also abstract to make a new mathematical structure.
8:46
So our goal here is to generalize. The Dot product.
8:54
To abstract vector spaces, and for us, that means real vector spaces.
9:08
You can certainly generalize this to complex vector spaces as well,
9:20
and that would be a nice project idea and permission, linear algebra, very relevant for physics.
9:24
But for us will mostly be focusing on real vector spaces. So that's what we're trying to do today is we'd like to say what made the product so great.
9:30
And let's try to build on that. So this is what mathematicians like to do.
9:40
We like to abstract good ideas to make fly in a bigger setting.
9:44
So let's try to define an abstract quantity, an abstract structure that will encapsulate what made the dot product so useful.
9:50
So we need a new term. We can't use a dot product now because that's has a specific meaning in our end.
10:04
So we want to then give a name to a new type of product. We're going to put on a general vector space and we're going to go for this general object.
10:10
We're going to call them all in our products. So an inner product.
10:16
Well,
10:28
think about what the DOT product did and sort of the most abstract perspective you could take on it is that at some function that takes two inputs,
10:28
two vectors in our end, and it spits out a scalar output, an element in your scalar field.
10:36
So that's what I want for my inner as well. It's going to take two vectors,
10:42
so it'll take two elements in your vector space V and then it's going to output something in the scalar field for that vector field.
10:46
So the real numbers? So an entire product on a vector space, I should say real vector space.
10:54
V is a function. So the notation we usually use is these angle brackets, and here are my two inputs.
11:06
These two dots, so I plug in a vector here, I plug in a vector here.
11:18
So this is a vector from pairs. It takes two pairs of vectors like two factors in our end and then outputs a real number.
11:22
So that's what kind of function it is. It's a binary operation. But I don't just want an arbitrary function.
11:32
I'd like it to satisfy some properties.
11:43
So the DOT product had a number of nice properties that it satisfied, I'd like this thing, this Inter product, to satisfy some properties as well.
11:46
So the first property as we usually insist on cemetery or that it's going to
11:55
be the same if I plugged in you here and be here versus be here and you here.
12:00
So namely you v the inter product of you, and we should give you the same thing as the product of the and you.
12:06
So this is for all you and we inside my vector space.
12:15
This is usually called cemetery. It's important to pay attention here if you've seen this idea before,
12:19
because especially if you've seen it in a physics context before physicists and mathematicians
12:27
have a long standing feel good about the appropriate way to define it in our product.
12:31
So I'm going to, of course, give you the mathematicians definition. Especially when you generalize to complex vector spaces.
12:35
Then there's a little bit of a disagreement for how things should work.
12:44
We also would like linearity in the first slot. So see you.
12:53
Plus De V enter product with W. Well, I'd like W. to distribute through just as it did for the DOT product.
13:01
So this is what I mean by linearity. So this should be the same thing as see in our product you w plus d in our product v w.
13:09
And again, this will now be for all U V and W elements in your vector space.
13:22
And for all Steelers C and D. So this property is usually called linearity in the first.
13:29
Component first factor. So we have linearity in the first term.
13:42
So linearity in the first term. Yes.
13:55
Yes. So, yeah, you could certainly change the order of these two, then get linearity.
14:02
And the second factor as well, this is one place where it does change a bit.
14:06
If you're working over a complex vector spaces, whether it's linear in the second factor or not.
14:10
So that's part of the subtlety if you're thinking about complex objects here.
14:15
But for us, you're exactly right. I mean, you don't have to go through the steps.
14:19
Real antibiotic resistance becomes an area. Yes.
14:29
So and our third factor for third term axiom that we often we impose is again coming from the DOT product.
14:34
The most fundamental property that we had from the DOT product was that if I took the dot product of a vector with itself,
14:45
then it was always bigger than or equal to zero. And this was important for us and thinking about length.
14:51
So this we want to be true for a general entered product.
14:57
So three, I would like the entire product of you with itself to always be bigger than or equal to zero for all vectors you and your vector space.
15:02
And the second part of this condition for the product was that if it's equal to zero, then that meant you had to have had this zero vector.
15:14
Furthermore. If you have the inner product of you with itself as equal to zero.
15:22
This is true if and only if you is the zero actor.
15:31
So these are the conditions for this operation being an Inter product on your vector space.
15:41
So perhaps unsurprisingly, if you have an Inter product on your vector space,
15:49
the combination of your Inter product with this new function is then called an enter product space.
15:53
The remark. We belong.
15:59
With an entire product. So the combination of these two things is called an inner product space.
16:04
So now we have a new structure. Yes. Why don't we define Orthogonal City as the DOT product being zero?
16:29
And we will see, yeah, I'm going to and in fact,
16:52
I'm going to I'm going to call any vectors in my inner product space orthogonal if they're dot product is zero.
16:56
So many of the though you're anticipating,
17:02
I think the next point in that basically everything that I did with the DOT product on our end should go through again for our product spaces.
17:05
This is very much like what happened when we did Chapter four,
17:15
where we worked in a vector space as long as we were working in a finite dimensional vector space basically worked like a copy of our end.
17:19
And in fact, we observed that observation by saying that those were Eissa Morphic spaces.
17:26
So that's a really good point.
17:32
Well, basically every definition we had for the DOT product on our end in Chapter six will take going forward and now our entire product space.
17:33
Tommy. All are all.
17:43
Oh, that's a wonderful question.
17:52
So there there's a great question here about thinking about like when you can actually put this kind of a structure on your space,
17:54
like how could you do it? So it's a nice question to think about. Like, how could you put different dinner products, space structures on a space?
18:00
So like the first question you might try to play around with is like, what if you took a finite dimensional vector space?
18:06
Is there a way that you could from having it be finite dimensional, easily write down what an entire product structure would be on that?
18:12
And so that's a really nice question. Another nice question beyond that. It was with our end.
18:20
When we think about finite dimensional vector spaces,
18:25
we proved what is often regarded as the fundamental theorem of linear algebra that if you have an end dimensional real vector space,
18:27
then it's Isaiah Morphic to our end.
18:33
So what that's essentially telling you from an sort of more abstract point of view is that end is really the only thing there is.
18:36
And if you know the dimension of it, you can always just work in the appropriate and dimensional space, assuming it's finite dimensional.
18:42
So like that was one reason why we're thinking about the space of polynomials.
18:49
We're like after a while started to feel kind of the same as when you were working in R R N Plus one appropriately for a.
18:54
Same thing when you're singing about the space of NY and matrices, well, that seems like it's really just the appropriate.
19:02
It's just kind of a weird looking version of R and Squared.
19:07
And so that's one nice question you could ask here is and could you classify all of these, our product spaces to say, when are they the same one?
19:11
Are they different? That's a good question. All right.
19:20
So I think to maybe even build on Tommy's question a little bit.
19:27
The first thing you need when you have a new definition is you need some examples in mind to think about what it really is.
19:33
Well, the whole point, after all of this, is that our with the DOT product is an entire product space.
19:40
And maybe this is getting at a little bit of Tommy's question as well. It's certainly not the only one.
20:00
Even on our end, we could mess with the DOT product on our end to make a different one.
20:04
So example two, we could take again our Vector Space V to be equal to our end, but we could put a different inner product on that space.
20:10
So, for instance, we could define what's called the weighted dot product where you take, say,
20:19
x inter product with Y is equal to B you want x one y one plus you and X and Y kn where w one through W-when are just not negative real numbers.
20:28
So this is what we call in our product, the way the product on our air, even in statistics,
20:51
this is often useful and important because like maybe you have more confidence in certain data points than others,
20:57
you would like to emphasize them more in, say, doing or at least squares regression.
21:03
So you could then have a way to start product instead of a normal dot product.
21:08
So then it would just be putting a bit more emphasis on that particular observation, or it could be from a sampling point of view.
21:14
Maybe you're really only interested in getting an accurate representation near the origin, so you'd want to put more data points.
21:21
You're there to sample what your study. So this is called the weighted product satisfies the same.
21:28
All right. So but the point of doing this, perhaps is to go beyond our end and we could think about, say, some function spaces.
21:36
So one of the most important function spaces is, say, the space of continuous functions on the interval AB.
21:48
So we could put an inner product on this space in the following way.
21:58
So now my vectors are functions, so I could take the entire product of F and G to be the integral over A, to be of the product of F of T of T d t.
22:03
So now this is a more interesting inner product. It also kind of makes sense, I would think, as a generalization of your product,
22:18
because now you're not sampling over one to three for the components of your vector.
22:27
You're now thinking of them as having sort of an infinite number of components and you're then adding up this infinite number,
22:34
the product of all of these terms. So that's one way we often generalize from a finite dimensional sum to an integral.
22:39
So let's just think quickly about the properties. Well, the.
22:50
If I change the order of my two factors, F.A., certainly that doesn't change the in the integral.
22:55
So that's perfectly fine. Linearity follows from our usual properties of the integral as well that we can just pull out the sailors and so forth.
23:01
If I have a real valued function and I'm taking, then the entire product of F with itself,
23:10
then I would be taking the integral from A to B of F of T squared, which then we know equity squared since real value will be non-negative here.
23:16
Take any interval of something that's not negative. So that's something that's bigger than or equal to zero.
23:25
If then, the integral of all those not negative terms is equal to zero,
23:31
then you could prove that then that would have to be the zero function using sort of the properties of continuity.
23:35
If it were continuous, then it would have to be positive on some small interval, which would then make your integral positive.
23:42
It's a nice exercise to work through kind of hints at more ideas from math.
23:48
One 12 real analysis class.
23:52
That's a particularly nice and important Inter product, and you'll see it in a number of the projects that people are pursuing as well.
23:58
One last example. So or maybe two more examples.
24:13
Example four. So one of our most beloved, perhaps vector spaces, that's not our end was the space of polynomials.
24:22
So we could take, say, a space of polynomials or concreteness.
24:33
I'll just say p four. So polynomials of degree for less. And I'm going to define an entire product on this space by just evaluating the
24:38
polynomials at some number of X values and then adding up the sum of the products.
24:46
So namely, I'm going to define.
24:55
A particular Inter product of these two polynomials, P and Q, so if, say, P and Q are polynomials of degree four or less,
24:58
I'm going to define the end product to be p evaluated at negative two times Q evaluated at negative two
25:09
plus p evaluated it negative one times Q evaluated at negative one plus p evaluated at zero times.
25:18
Q evaluated at zero plus p evaluated at one time.
25:26
Q evaluated at one plus p evaluated at two times.
25:30
Q evaluated at two. So again, it doesn't look maybe wildly different from the way even our DOT product was defined,
25:35
because when we were thinking about our DOT product, we took these some of the products of their components.
25:46
Here, I'm taking the some of a bunch of products, but now just corresponding to different X values where polynomials.
25:52
And for most of the properties here, it's pretty quick to verify that these properties hold certainly cemetery holds linearity,
26:02
holds just from the properties of polynomials again.
26:10
If I took a polynomial in our product with itself, I'm getting the sum of a bunch of squares, so I know the result will be non-negative.
26:14
And if there's the sum of the squares is equal to zero,
26:24
then I would know that the polynomial was then zero in each of these points, so different than it would be the zero polynomial.
26:30
But it feels a little bit weird or arbitrary. So let's think about a different example.
26:39
Yes. That's a great point.
26:48
So that's a great point. What if the polynomial just happened to have zeros at those points?
26:56
So if I have a polynomial and p for how many zeros can it have?
27:00
Right. So then I know of degree for so I can have at most four zero distinct zeros or it's identically equal to zero.
27:08
So here you've actually kind of exactly thought about why I chose five points here, because then once you know,
27:17
it's zero at five points, if you're in P four, that forces it to be the zero polynomial.
27:25
That's Jonathan. Or maybe.
27:31
Yes. They don't have to. Got it. Yeah, yeah, I'm not saying yeah, for PM Q when I mean they could be.
27:41
They don't have to have necessarily any zeros. All right. Yep, yep.
27:48
So yeah, I mean, in fact, these X values, I just need them to be distinct.
27:54
The the points you would choose where to sample are coming entirely, most likely from the application that you're pursuing.
27:58
Like, you might be that you have a good reason to pursue certain ones. Laura.
28:06
So the points that I choose have to be the same for any P and Q, so like once I've this is for my entire product space,
28:24
I have to choose like this expression will be for any P and Q that I use any polynomials for this particular Inter product.
28:32
But the way in which I'm saying that it's arbitrary is I could define a new Inter product by,
28:41
say, making this one evaluate at negative three, negative three and keep the others the same.
28:45
That would just give me a different inner product space because it's a different in our product that I've defined on that space.
28:52
That's the sense in which I mean that it's arbitrary. It doesn't have to be negative to negative one zero one two.
28:59
It could be any five distinct X values. Yes.
29:05
Yeah. You've got a different dinner product space photos, different X values. Yep.
29:13
I could also insist on sampling this at more points if I wanted to as well.
29:21
There's no reason why it had to be five, but I knew I needed to take at least five in order to get this last property to be true.
29:25
Does that point make sense, why it has to be at least five points? Marco, I want to say what you have to be five points,
29:46
but can you also guarantee the victory by doing it like alternating, like feeling of duty to let me know what you want?
29:53
Like what? Oh, I see. But so it's not going to work out quite right in terms of the factoring, right?
30:02
When you are trying to group your terms coming out of it, then you're assuming some properties of P and Q,
30:08
which maybe you want to have your functions be more than just polynomials.
30:13
Maybe you want them to have symmetric coefficients and like certain degree restrictions or something.
30:17
OK, but so the upshot of this is that this then allows us to do geometry, so that's the point of this.
30:24
So in our products. Allow or maybe I'll say give.
30:36
US, a geometry on our vector space we once were in in in a product space.
30:49
Then think about what follows from that. Just because we have it in our product,
30:59
we then immediately the notion of length because our length of a given vector
31:04
in our vector space is the square root of the inner product of F with itself.
31:09
So I can tell that people are paying attention if they laugh at my accent. Just insert the word rut and see if people laugh.
31:16
So that's really important that it gives us this notion of length,
31:28
but then that's telling you that this notion of geometry is coming from the way that you've defined the DOT product.
31:31
So it might be capturing a different sort of geometry on your space, but it might be capturing exactly the right geometry for what you want to study.
31:37
So that gives us a notion of length.
31:46
But you'll recall also then had the notion of distances between two vectors say, F and G, as then the length of the difference between them.
31:48
So now we can also measure the distance coming from the center product. So that gives me two additional structures.
31:58
Here I get a norm structural length. This is often called the norm of f recall and then a distance structure.
32:04
This is often called the metric structure. So if you only had a norm, this would be a normed vector space instead of an inner product space.
32:10
If you only had a distance, this would be a metric space. So there are other mathematical structures showing up here, too.
32:17
But what it's telling you is that in some, the inner product space already gives you all of this additional structure.
32:24
Furthermore, we defined the angle between vectors coming from the inner product of F and G,
32:31
divided by the length of F the norm of F over the norm of G.
32:40
So this gives us a notion of angles as well.
32:46
So from a notion, once you've defined it in a product, it then gives you all of this additional structure.
32:49
It gives you a normal length, it gives you a distance, it gives you angles.
32:56
So the rather remarkable thing that's coming up here is that now, for instance,
33:01
in this vector space, with that in our product, I can talk about the angle between two functions.
33:05
I can talk about the angle between two polynomials. For us, it's always the cosine we're used to.
33:12
I'm never going to take like cosine of complex numbers or anything like that. We're not going to.
33:28
It's a fun thing to think about. But like, how do you generalize these notions to complex analysis?
33:32
So my own research areas and complex analysis and a large high number of dimensions.
33:38
So it's a question that I like a lot, but probably takes us too far afield.
33:42
So. It's not like there's not there's like.
33:49
Mm-Hmm. So it does mean.
33:55
Well, and that's that's sort of true, even when you think about vectors in our end, right, when you're thinking about two vectors in our air,
34:02
and if I have these, I could go around like the angle right here, but then I could also go around again and measure it.
34:07
So that's coming from just are those subtleties already exist in trigonometry and the usual sense.
34:12
So usually what we do here is we're going to take it in the same way that we would with vectors.
34:20
In our end, we're going to take that to be sort of the smallest angle that you could have between them.
34:24
So good questions. Yes.
34:27
I think this also goes back to was it? Uh.
34:40
The list goes back to a few classes ago when we started first thinking about this,
34:49
when we had our product being zero, we defined those factors to be orthogonal.
34:53
We can do the same thing here.
34:58
Product G equals zero, means F and G are orthogonal, so our notion of orthogonal ality is then defined in terms of whatever our interplay is.
35:02
Now, once we have a notion of orthogonal city that gives us then a notion of orthogonal projection because then you could project onto
35:19
a vector or a subspace where your your measure of orthogonal city is coming from the specific Inter product that you defined.
35:27
So if you had some collection of vectors through FP that are orthogonal?
35:35
I guess I should say is, since I'm talking about the set is orthogonal, then we could talk about the orthogonal projection.
35:48
And just like we did. We're worried to find orthogonal projection onto our subspace, we define it in terms of our entire product.
35:56
So the orthogonal projection onto the subspace generated by the span of one.
36:06
I've given Vector B. I'm sort of deliberately using thin GS.
36:14
Evoke more function spaces rather than vectors and are in.
36:22
Inter product F1 Times F1 Plus that are up to the last day and are product happy over FP in our product FP.
36:33
So that was our orthogonal projection formula, and we knew that our vectors were coming from an orthogonal set,
36:48
so we can then define orthogonal projection in an arbitrary in our product space.
36:57
From then this, we then get that orthogonal decomposition theorem goes through exactly the same way.
37:03
We also then get the best approximation theorem, and that's the exact same way.
37:10
So all of this theory that we've built up on our end goes through really nicely on now in in our product space.
37:14
It's sort of the exact right generalization to prove these theories. So I should say so now and.
37:23
We also get. You orthogonal decomposition theorem.
37:34
And the best approximation throw. The nice thing about that is that you remember the best approximation theorem then was
37:46
telling us about that we could solve sort of this constrained optimization problem,
37:58
like how close could you get to a particular vector while staying within a particular sub space?
38:03
Well, now you could do that in any one of these vector spaces with these inter products.
38:09
I can say, how close can I get to a particular function in terms of a bunch of other functions while I'm
38:13
then measured minimizing distance where the distances coming from this integration formula?
38:19
So it's kind of a nice example of the things we've been seeing.
38:27
Oh, I should also point out once we have the orthogonal projection formula, it'll also work.
38:34
Graham Schmidt works the same way. And again, once you know, government works the same way, then you don't have to do it by hand.
38:40
You can have a computer. Oops!
38:51
All right. So let's try to make this discussion a little bit more concrete by actually doing some calculations.
39:00
Which is what the rest of the class is really about. That I will leave your exploration of inner product spaces and Norman Vector spaces,
39:09
for instance, to some of the projects which go into these notions in much greater depth.
39:28
All right, so let's do a concrete example. It's also a nice example, it's very much like your very last web work problem, I believe.
39:40
And it's a nice example to reinforce some of the earlier ideas, too.
39:48
So I think this is maybe the first problem. Your handout? So I'm going to take a new vector space.
39:57
So the space of continuous functions from minus one to one, I'm going to define my inner product as I did over there,
40:06
so my inner product between two arbitrary functions in the space of two continuous
40:13
functions will be the interval from minus one to one of their product.
40:18
As we remarked, this does indeed give you an inner product.
40:24
Then I'm going to take a nice subspace of this space.
40:31
I'm going to take the subspace, which I believe I'm calling W2 to be the span of one T. A. Squared.
40:34
Oh, it's just w OK, hello, I'll just call it so W. So what problem number is it, Jonathan, is it one?
40:45
Number two, oh, number one, I think was my example.
40:55
So number two, so here, what I would like to do is I would like to then find an orthogonal basis for this space.
40:59
OK. After all, that's what's useful, because if I wanted to compute an orthogonal projection onto this space,
41:10
I would need to have an orthogonal basis in order to do that. So.
41:16
Find. And orthogonal not necessarily worth the normal, but an orthogonal basis.
41:22
For W. So A.A. squared or certainly continuous functions are continuous on the entire rail line,
41:31
so there are definitely elements in V, so then any linear combination of these will also be elements in V.
41:40
So this is definitely a space. So now let's just find an orthogonal basis for this thing.
41:46
So in order to find an orthogonal basis, we used the Graham Schmidt orthogonal ization process.
41:54
Because what we're going to do is we just say, well, take our first base suspected to be one, that's no problem.
42:07
We can definitely do that. So define. Or first base is factor v one to be equal to one.
42:15
Now we want an element, the two that's inside of our vector space that's orthogonal to v one where orthogonal city is measured,
42:23
according to this, our product.
42:31
So the only thing that we can really do is, well, take one of the other bases factors say X to here or T, and then we subtract off from that.
42:34
All of T that's in the direction of one.
42:45
So the we want the orthogonal component of T with respect to one under this Inter product.
42:49
So in order to compute some Inter products.
42:56
So note, if I take the Inter product of one and T, so we just want to know what their current relationship is like.
42:59
Well, that means compute the integral from minus one to one of the product of one.
43:08
And T d t. And now I know I haven't asked you to do very many integrals this semester, but hopefully this is one we can do.
43:15
What do you notice about this integral? That's zero.
43:24
So it's an odd function over a symmetric domain, so I don't even need to compute it.
43:30
I can just reason geometrically. So I know. And in this Inter product, one and T or orthogonal.
43:34
So hence the polynomial one and the polynomial t minamino's are orthogonal.
43:42
And maybe with respect to this particular. So it's important to note that it's with respect to the geometry that you're working in.
43:55
You could have defined a different inner product.
44:10
You could have even to find this to be over a different interval and then gotten a very different answer.
44:12
OK, so there are orthogonal, so that makes the Graham Schmidt part a lot easier, so then I can take my some cheers for that.
44:24
So then I can take my second basis factor to just tee to now.
44:34
We need to be equal to T, since we know we won't be too orthogonal.
44:40
Graham Schmidt is working out pretty nicely for us. OK, well now we just need to project onto the span of the first two vectors.
44:45
Let's give a name to those first two vectors, so I'm going to define one to be equal to the span of the first two.
44:55
So this is an orthogonal basis for W1 and W2. So that means I can compute the orthogonal projection just using the usual formula.
45:03
So then we want to find the orthogonal projection of my third reported basis fact or my third basis vector here t squared onto one.
45:12
So this is asking how much do we need to subtract off from T squared so that the result is orthogonal to W1 and W2, or sorry to V1 and V2?
45:25
OK. Well, we now have a dollar for that,
45:36
so it's going to be the inner product of T squared with one over one in a product with one member of that and because I didn't make it with.
45:39
And one plus two squared in our product with tea over tea and our product 40 times tea.
45:49
So that would be the amount that I need to subtract off to get something that would be orthogonal with this orthogonal latte relation.
45:59
So now there is a bunch of Inter products that I need to compute here.
46:10
So let's compute a few. So t squared and product with one.
46:14
So that's by definition integral from minus one to one. Dty, so that becomes two thirds.
46:19
And a product of one with itself the denominator. Three, four, minus one to one of one T.
46:30
So that's just to. Then I have the inner product of T Squared with T.
46:38
Well, that's the integral from minus one to one of T times t squared d t what's that one?
46:45
Zero, good. I assume people can answer with gestures.
46:56
Zero. So those are orthogonal, so that's pretty nice. That means that this term just drops out.
47:02
So I don't even really care about computing the entire product of tea with itself.
47:07
You might also note that that is just the same as this one, so it wouldn't have been so bad to compute it.
47:11
So that means now I find my third orthogonal basis factor by taking T squared faces factor that I had
47:33
over here and then subtracting off the projection onto the first two basis factors of T squared.
47:41
All right. Well, if I look up at this formula this term just dropped out, so I don't have to worry about that one.
47:50
I then have two thirds divided by two, so then I just got a one.
47:57
So I'm just subtracting off. So this becomes two squared t squared minus one third.
48:02
And because I was only interested in orthogonal basis, not necessarily an all the normal one, I can scale this to be anything that I want.
48:12
And so in particular, I'm going to scale it so I don't have any fractions showing up.
48:21
So I'm just going to take my third base factor to be E3 Prime, which will be three T squared minus one.
48:25
So that means I have an orthogonal basis for my space where the dog analogy is again measured,
48:35
according to this new Inter product formula of one T and three T squared minus one is an orthogonal.
48:41
Basis. For W.
48:56
These three vectors, if you have depending on how much physics you've studied before they show up a lot, these are called orthogonal polynomials.
49:03
They're polynomials that show up a lot as a nice basis in physics.
49:11
If you keep going with, say, one T T squared t cubed, you would get another polynomial in this sequence.
49:15
The sequence then forms the polynomials draw a very useful basis to have.
49:24
So what genre polynomials? Yes.
49:30
So why is it why is the interval minus one to one useful or why is the.
50:02
Might come from like the lamb, you might be using us to solve some differential equations, for instance,
50:10
in physics and the interval in which you care about your solutions might be from minus one to one.
50:15
Or it could be under some totally different interval,
50:21
but you often might have your interval centered around the origin or near the audience from the way that you've set up the problem.
50:24
So, I mean, for instance,
50:30
suppose you're studying like the wave equation in physics or something or the heat equation or something you have to normalize.
50:31
You take your your, your equation and you set it up. So you have like your rod placed at zero and then it goes out some length or you have your
50:36
string placed at zero and it goes out some distance and you start oscillating that thing.
50:44
So it comes up pretty naturally as just you're working over some interval and it could be the interval as symmetrically with respect to zero.
50:48
Or it could just be it starting at zero and going off to some length.
50:57
But that's the reason why these sorts of problems show up a lot is because you're often trying to use them to solve differential equations.
51:01
Yes. Yeah, that's a great question.
51:08
Audience question, what if you make this continuous functions over the entire rail line and then you might want to then make
51:19
an integral that's an an integral for going from minus infinity to infinity to take that into account.
51:25
So then having improper integrals show up, certainly. So there are some more subtleties that show up when you have to take those additional limits.
51:32
But yeah, that's a certainly a nice thing to think about.
51:39
You often might take your problem and make it what's called a compact interval, a closed interval from A to B and solve over that compact thing.
51:42
And then maybe take the limit as those intervals to get larger and larger.
51:50
So good questions. Yes, Marco.
51:55
That's how this is useful, because we can then use it to, like, find the poor finding like.
52:00
Yes, I know. Yup. But when you put the formula like Cosine Theta equals nothing, like what would that even represent in terms of like?
52:07
So I mean, I think you would be thinking about like some sense, closeness.
52:19
Right. So it's another another way of measuring like the angle between something to factors in space is in some sense measuring how close they are.
52:24
So like even when you're thinking about them sort of in statistics,
52:32
you might be thinking about them as measuring some kind of a correlation or something. You'd be doing the same kind of thing here.
52:34
It won't be capturing exactly the same geometry that you maybe were thinking of before, but it can still be measuring something similar.
52:39
So you want to take as much intuition as you can from the way we usually do geometry on our end.
52:49
But you need to be mindful of that, like not necessarily everything will go through exactly the same way.
52:55
So one of the most useful things about having an orthogonal basis is that we can compute projections now,
53:04
which then through the best approximation theorem tells us that that's as close as you can get inside of a particular subspace to a given function.
53:11
So if you go back to even calc two, when you're doing, say, BSC Calculus, a lot of what your story was doing series approximations.
53:19
I mean, a full third of our second semester calculus class at Harvard is about doing series approximations
53:26
and because it gives you a really powerful way to approximate what's going on with a given function.
53:33
So here we get another way of computing approximations to a given function in kind of a similar way.
53:38
So, for instance, in this particular problem, then approximate.
53:46
Really, any function you want, but you could pick a particularly nice one, say E to the T on the interval minus one to one.
53:54
So this is one reason we we might have in mind that we want to study this function just mostly in that region.
54:02
We don't particularly care what happens outside that region. And then we say using these legendary polynomials.
54:09
So namely, what do I mean by that? I just mean. The orthogonal projection.
54:19
Onto this space, W of E to the T, then the best approximation there is that that will be minimizing the error as
54:29
measured from this Inter product between those functions so we can actually do it.
54:38
I'm not going to do the calculation just to save some time.
54:48
But it's. Kind of cool that we can do it.
54:52
You just write out the formula. The projection onto W of E to the T.
54:58
Well, then this will be the end product of eternity,
55:03
with one over one Inter product of one times one plus utility and our product with T over T Inter product with T times T plus e
55:05
to the T three T squared minus one three T squared minus one times three T squared minus one times three T squared minus one.
55:16
Oh, so you'll note this will just be a real number.
55:30
This will just be a real number. This will just be a real number.
55:35
So the end result of that looking calculation will just be a quadratic polynomial.
55:39
OK. So it's going to give you some polynomial that is going to approximate this function.
55:45
It's not the same as the best quadratic approximation as coming from like a Taylor series,
55:51
because now it's trying to minimize not just the error match.
55:57
The derivatives at the particular point is trying to minimize the error over the entire interval, minus one to one.
56:00
So I'll spare you the sort of integration by parts that shows up in these calculations.
56:08
But if you compute this, you end up with the following expression.
56:13
Thirty three over four e minus three e over four.
56:19
That's my constant term plus three t over e minus one hundred and five T squared over four e plus 15 e t squared over four.
56:24
So this gives you an approximation that you could certainly use numerically if you needed to in some algorithm that you're writing.
56:41
And the nice thing about that is you can compute the error between this polynomial and or function, and I did that earlier today.
56:51
The error between E to the T minus the projection onto the subspace of E to the T.
57:01
So if I compute this error where I'm computing it by integrating over the interval of minus one to one,
57:08
it comes out to be approximately zero point zero four.
57:13
So it's remarkably close given over this entire interval when you're adding up all of the difference.
57:20
And that's as close as we can get while staying in that sub space. So what I want to do in my last little bit of time here is you can play around with
57:31
this theme even more of choosing a different basis and studying a different basis.
57:44
And this leads you to really important topics in applied math, for instance.
57:47
So here, just taking one T.A. squared and then orthogonal izing.
57:53
That said, let us to these legendary polynomials, the sequence of orthogonal polynomials.
57:58
That's quite useful. There are a lot of other intervals.
58:02
They go for a lot of other products and so forth.
58:07
Let's give you a I want to give you an example of one canonical one that their entire course is about.
58:11
So this is a topic of quite a series. So many of the projects are going in this direction and much more depth, and I will over the next few minutes.
58:22
The basic idea? Is that you want to study, say, a function f on the space of continuous functions?
58:35
From zero to two PI, for instance. And now your goal.
58:45
Just to approximate Earth as closely as you can.
58:51
Using you script functions, so use of simple sinusoidal.
58:58
Functions. So what does that mean?
59:04
That means we're going to take a base consisting of functions that we know and understand really well, we'll call the set.
59:09
W n so this could be the set one and that coastline to cosign to T Dot Dot Dot up to Cosign A.
59:20
And then I'm going to do the same thing with my signs. So sign. Sign to tee up through sign and.
59:32
So that's a particular set. The remarkable thing that happens is that if you actually compute a bunch of integrals here,
59:45
these are all orthogonal under the natural Inter product here.
59:53
So this other. With respect to the attacked from zero to party of tea and GOP.
59:57
That's one reason why we choose to work with this base is quite a lot in applications where there's an entire field called for
1:00:13
analysis has because they happen to be orthogonal and that makes it very useful for this kind of these kinds of calculations.
1:00:19
For instance, if I want to approximate closely as possible because I'm in in the entire product space,
1:00:29
I have a best approximation theorem so I can minimize the error by computing the orthogonal.
1:00:36
Paul, the tenth order for April. So your question question.
1:00:46
Yes, Jonathan, is there a way that we would know that you'd compute it?
1:00:54
Yeah, you would plug them into this integral and you would actually compute them? Yeah.
1:00:59
So this would go back to again some of the late topics in. I don't think we even do this in one b anymore.
1:01:03
But in many second semester calculus classes,
1:01:10
there's a section called trigonometric integrals where you could compute these things and verify that they're orthogonal.
1:01:12
And students wonder, Why am I doing this? And it's because when you want to do for analysis, you want to know that there are orthogonal.
1:01:17
OK. It's one thing that comes out of this is that we can form the order.
1:01:26
For approximation.
1:01:33
To a given function f by the orthogonal projection onto the subspace, so the projection of fusion of F, so we literally just write it out.
1:01:40
So that will then just be f inter product with one over one Inter product of one times one plus that thought
1:01:54
and then f inter product with sine and T over the Inter product of sine and T and sine and tea time sine A.
1:02:02
So.
1:02:16
Are called your four AAA coefficients, then sort of measuring how much you're moving in each of those particular directions of those basic factors.
1:02:18
So this is your foray approximation, because you're only taking in terms, are you?
1:02:30
They're just in the dot dot dot. So all the terms do appear there.
1:02:36
That was a mean, trying to go a little bit more quickly, but you have to cosign terms are there too.
1:02:41
So these coefficients, we call them the four AAA coefficients. OK,
1:02:47
so there are often called the generalized foya coefficients because you can do the exact same orthogonal projection with respect to any ongoing basis.
1:02:51
And in terms of that orthogonal basis, you would call them the generalized Fourier coefficients.
1:03:01
So you're right it. Well, patience.
1:03:06
Are called. Generalized for A.
1:03:15
So this is the order for the approximation, how do you think it could make this a better approximation?
1:03:29
David? Yeah, why not let em go to infinity if you let and go to infinity, then you get what's called the four eight series.
1:03:38
And then, of course, as you might read about in some of the projects at that stage,
1:03:46
then you might need to worry about convergence of your theories to know whether or
1:03:50
not it actually converges to particular value when you left and go to infinity,
1:03:53
which is a similar idea to when you're thinking about Taylor series as well.
1:03:58
But one big advantage that this has over Taylor series is that you notice when you're talking about the Taylor series,
1:04:03
you need your function to be infinitely differential. You need to have infinitely many derivatives your function.
1:04:07
However, I was only required content.
1:04:12
You can use this to study and approximate more general classes of functions, which is very important when you're doing, say, signal analysis.
1:04:21
So there's even an entire class that's mostly a while much of it is about for you.
1:04:31
Analysis a.m. one oh four. So we're going certainly further in those particular directions.
1:04:35
So. What do I want you to get out of this?
1:04:42
Oh, I need to insert the word for it here. To me, the most important thing to get out of this is that when you're doing four year analysis.
1:04:47
The what you're actually doing behind the scenes is you're working with an inner product space,
1:05:04
and within that entire product space, you're doing things like computing orthogonal projections.
1:05:08
And so one big thing that I think you want to take away from what we've been talking about
1:05:13
over the last few classes is that many ideas that you've seen in very different domains,
1:05:17
like in statistics, when you're talking about linear regression, what you're doing is an orthogonal projection onto the column space.
1:05:22
If you're doing for you and Alice in physics, what you're doing is you're computing an orthogonal projection onto the span of some vector,
1:05:30
some sinusoidal vectors and this function space.
1:05:36
So in many different contexts,
1:05:40
there's actually an underlying geometry that you're using and studying in order to understand what's happening in that context.
1:05:41
So what I would really like you to get out of this is just to know that what for you analysis,
1:05:49
how it relates back to vector geometry and analysis in in vector spaces?
1:05:56
So I think I'm going to skip the last example on there,
1:06:03
and probably I'll leave that for all of the projects will go in wildly more depth in for a series that I've gone to here, really here.
1:06:07
All you're getting out of this is that I think how the story starts coming out of having in our product spaces.
1:06:15
If you take also math am one of four or math one 10, there's certainly much more depth that one goes into on these topics.
1:06:24
Because one last teaser many of you might have seen the the viral video going around.
1:06:34
Maybe was it last summer when you look at the sub one plus two plus three plus dot dot dot some to negative one over 12.
1:06:42
And what that means? That's one thing that we often precisely cover in a class like math one 10.
1:06:49
At least when I teach it, we cover that topic and we make that idea mathematically precise and show what that really means,
1:06:56
avoiding all of the nonsense that you see on the internet.
1:07:03
But the key point in why it relates is that the idea behind that comes from Fourier analysis and the geometry that we're doing here.
1:07:08
So I think that's where I'm going to end things today. So I'm going to take one more minute and then I'll let you all out five minutes early,
1:07:16
which will then hopefully pay off all of the debt that I owe you. And then.
1:07:23
So in my my last minute before I let you know, five minutes early,
1:07:36
I just want to say I'm going to walk back to my office and I'm going to release the exam grades.
1:07:39
I just had to ruin the applause, didn't I?
1:07:48
I couldn't just end on a good note in terms of the exam, I think I first all apologize for for us taking so long to grade it, it was.
1:07:51
It took a lot longer than I thought it would take to grade it.
1:08:01
And let me just say that the scores were I mean, I was very happy with how people did on the exam.
1:08:05
I mean, I can definitely tell that people learned a lot and studied very hard for the exam.
1:08:12
If anyone does want to meet with me to discuss their exam, they should feel free to reach out to me either.
1:08:17
This afternoon we could meet or once you get back from Thanksgiving break.
1:08:24
So if you want to reach out to me and talk to me about the exam, certainly feel free.
1:08:29
But otherwise will the exams should be up in, you know, 10 15 minutes.
1:08:35
OK. Yes, Quinn. So, no.
1:08:41
So last time, what I did is I told you a range of scores that I felt like corresponded a particular letter grades.
1:08:49
The idea behind that was thinking about like, if that exam were the sole way that I were assessing you in this class, which of course, that it's not.
1:08:56
So on this exam, we I think those numbers might be a little bit higher than last time.
1:09:04
So versus the the ranges that we were giving on the last score. So if you're thinking about like, what would be in a range on this particular exam,
1:09:12
I think it would probably be something like high eighties to 100 hundred would be the age range on the exam.
1:09:19
If you think about what the B range on the exam would be,
1:09:25
you'd would probably go from then like the high eighties down to like, I don't know, the mid seventies or something.
1:09:28
And then the C range would probably go from like the mid seventies or something down to, I don't know, low sixties.
1:09:34
Yes. The fact that this example was in the last 10 or 15.
1:09:48
Very cool, very cool, they're not weighted differently. So.
1:09:55
Yes, David, are there the same?
1:09:59
So the the reason why we do test corrections on the first exam is to try to support success later on in the course for the second exam.
1:10:08
So I don't see as strong of an argument for test corrections on the second exam.
1:10:19
So I think instead, if you want to talk to me about your exam to think about like methods for improvement and understanding those questions,
1:10:23
I think it's better to do that in a little bit more of an individual way rather than having everybody submit test corrections for the exam.
1:10:31
So, so no, there won't be test corrections for the exam. All right.
1:10:40
Let me not keep you longer, so this is not only three minutes. All right.
1:10:46
Have a good break.