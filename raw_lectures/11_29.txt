All right. Welcome, welcome back. It's great to see everyone again.
0:01
I hope you all had a nice Thanksgiving break.
0:08
I got a chance to rest of it and relax.
0:12
Let's just quickly run through some announcements before we get going here today.
0:17
So in terms of things coming up in the course, we are kind of wrapping things up.
0:23
We have problems at 12 due on Wednesday, so our last problems at the last bit of web work is due on Wednesday.
0:30
Then we're totally done with all the quizzes, all the problem sets.
0:37
The project draft is also due on Wednesday. So keep that in mind once.
0:43
So what's going to happen on Wednesday? I think I set the due time to be like our normal sets for the project draft,
0:50
but there's a little bit of wiggle room there, but not a tremendous amount.
0:57
So please make sure that like, that's a deadline that we should really try to hit whatever you have.
1:03
You need to give that to me on Wednesday, and that's what we'll go out for the peer reviews.
1:10
OK? The peer reviews that is a part of your final project grade is sort of the thoughtfulness of your review,
1:15
your reviews of other projects, so everyone will review two projects.
1:26
These will be automatically assigned to you on canvas. You should get an email about them when they're assigned, it should come at.
1:31
I think it's automatically scheduled to go at five p.m. on on Wednesday.
1:39
So it's not that I expect that it'll take you a huge amount of time to do the peer reviews.
1:45
But I mean, I I'm giving a fair amount of time to actually do them, but I don't don't feel like you need to spend, you know,
1:50
all of your waking hours between Wednesday and Monday, just working on the peer reviews,
2:00
the then the final version of your project is actually do two weeks from today.
2:08
OK. So after you turn in your final project, then of course, your attention can turn to reading the peer reviews that are assigned to you,
2:14
trying to both give good feedback to your classmates, but also thinking about like,
2:26
how could you incorporate some of their good ideas into your project?
2:30
Does it suggest some different ways you might tailor your project to kind of address
2:34
some things you're noticing that are mildly awkward in someone else's organization?
2:38
So like, try to think about how you can apply some of those same ideas or lessons from that draft to back to your own project as well.
2:44
I think oftentimes that's a really good opportunity to improve the quality of your draft.
2:53
Then between Wednesday and I think Tuesday is the deadline that we've set, like end of day on Tuesday, you should get feedback from your t.f reader.
3:00
So either me or Caleb seeing Lucy or Kevin, it's.
3:15
I'm planning on meeting with all of my groups, but I've not made it a requirement that the TFS necessarily have a meeting,
3:23
some teams might prefer to write written feedback. It's then totally up to you if you would prefer to, like, meet again to discuss that feedback.
3:31
I mean, the staffs are encouraged to support you and what you think is best for your project.
3:40
They're not going to. They're not there to tell you what to do. OK? They're there to kind of help guide and answer questions as they come up.
3:46
But so that's why you can kind of expect if you're if I am your reader, then if I haven't,
3:55
you haven't heard from me by Thursday to schedule a meeting to talk about your
4:02
project and feel free to just email me and we can schedule the time to to chat.
4:05
OK? Are there questions on what's coming? Is everything clear on what's left for the class?
4:10
It's clear. OK, so what I wanted to do today, the last two classes are mostly just what I regard as just fun days.
4:19
Hopefully, the rest of the class agrees with me that their fun days.
4:29
Part of that is by design for the course because as I mean, you can note, there are no more major assessments left of the course on this material.
4:34
And so it's not as if I'm going to test you on any of the material that you're going to see on today's class or Wednesdays class.
4:46
Instead, what the point is is that I'm hoping to give some nice capstone, some nice tour of the material that we've seen this semester.
4:53
So you can kind of see like some of the themes coming together. And I also hope to set up some of the projects nicely.
5:01
So there are some major results that you might want for your project that hopefully I can put in place over the next class and a half or so.
5:08
So thinking back to one of the major questions that we had this semester, one of them was.
5:20
What's a nice coordinate system to work in, what's a nice basis?
5:28
So that was sort of a big goal that we had. A big goal.
5:32
To find a nice space.
5:41
And I sort of deliberately use the word nice there, because nice is in the eye of the beholder.
5:49
It kind of depends on what you want to do, what nice means. So it leads you to the question of, well, what do you really mean by nice?
5:55
So what exactly do you mean by this niceness so well, that could depend on your particular application,
6:09
so one version of being a nice basis that we've talked about would be saying giving an eigen basis.
6:19
So that's back to Chapter five. Answer one. An ongoing basis was pretty nice.
6:26
One reason why an eigen basis was nice was because it was diagonals.
6:33
So it was sort of a very simple representation for your transformation that your study.
6:40
OK. Another question, another answer rather to the question answer two, as you could say, well, I prefer when things weren't worth the normal.
6:46
I like to wear the normal bases, so that's what I mean by nice. So you could say, I want to know what the normal basis.
7:00
So you could think about this was the focus of Chapter five and Diane Analyzation.
7:11
This was the focus of Chapter six and optimization techniques in this context,
7:18
when we're thinking about orthogonal decomposition and orthogonal projection and understanding geometry.
7:24
It was really nice to have an author normal basis. So I guess I'm a greedy person.
7:30
But then the question that occurs to me is why not both?
7:37
So what I'd like to know is if I'm given some matrix, can I find an orthogonal or an author's normal again basis?
7:47
So can I have everything? So let's give a name to matrices that have this property in them.
7:57
The main point of today's class is really just to study and understand these matrices,
8:08
and this is really a capstone of a first course and linear algebra.
8:12
One of the most important theorems in linear algebra generalized is really nicely to the infinite dimensional setting.
8:17
It comes up a lot in physics, it comes up in all many of the projects people are discussing.
8:22
So it's really quite an important result. So.
8:27
Let's take a to B and and by and matrix.
8:36
Then we say, hey, is orthogonal, diagonals visible?
8:46
So not just diagonal lines of all but orthogonal diagonal visible.
8:54
If. A has an orthogonal and henceforth the normal again basis.
9:08
Namely, it has exactly the property that we can find a basis that does both tasks.
9:24
So you should note that the way that we define diagonal sizable meant that there was
9:33
some basis in which we could represent our linear transformation as a diagonal matrix.
9:39
Namely, that your matrix was similar to a diagonal matrix. So A was equal to PDP inverse, where D was a diagonal matrix.
9:44
So we're taking a little bit of a different feeling for the definition here,
9:53
but the diagonal ization theorem sort of immediately gives us that result.
9:57
So note. So if a is orthogonal or diagonal sizable, so then a has an orthogonal eigen basis.
10:03
So what that means is that then we can say that it's similar to a diagonal matrix.
10:14
So this means. There exists an orthogonal matrix.
10:21
Matrix. I guess as.
10:33
And a diagonal matrix. Did such that a is equal to s d s inverse?
10:40
So it's similar to a diagonal matrix, where now it's not just a random vertical matrix that did this change of coordinates,
11:05
it's a special one, as had the additional property that it's an orthogonal matrix.
11:13
So thinking back to before break, how do you invert orthogonal matrices?
11:19
Yeah. We take the transport, so in this case, it becomes a nicer formula,
11:30
so this will actually just be as the as transposed or orthogonal matrices had some pretty nice properties.
11:34
In particular, their inverse is given by the transpose.
11:41
So that gives you kind of a nice formula there, so to undo that coordinate change here.
11:46
You can just take the transpose of that matrix, Jonathan, or want to do on the floor or a.
11:51
I. Well, the deglaciation theorem says that if you have an linearly independent eigenvectors, then you can diagonals your matrix, right?
11:59
So then to diagonals, your matrix means that you can write it in the form A is equal to SD as inverse.
12:12
I guess the diagonal I get the I can't see how many people I pass.
12:18
So that's the orthogonal part, right? So the orthogonal part is where I'm getting a little bit of extra.
12:26
Otherwise, this is just diagonally visible. So if I'm saying that now, in addition to having my basis of eigenvectors, it's also an orthogonal basis.
12:33
So it's an orthogonal collection of actors. Well, then this s inverse thing here is that a matrix that has all the normal columns.
12:42
So there just. Elements of the cases.
12:52
Yes. OK. Yeah. Yeah. Mm-Hmm. Other questions.
12:57
OK. So now the important theorem, the spectral theorem.
13:07
So when we talk about the spectrum of a matrix, that's just the set of the eigenvalues.
13:20
So the spectral theorem is then going to characterize for us when we can answer these two questions,
13:25
when we can give an author normal again basis and the condition is going to be exactly that, your matrix needs to be symmetric.
13:31
So we have an end by end matrix with real entries.
13:43
Everything is tied to real. So this just means it's an end by end matrix.
13:51
It's an element in that vector space, then a is orthogonal diagonal, sizable diagonal, sizable if and only if a is a symmetric matrix.
13:57
So namely, they transpose is equal to a.
14:20
So this exactly characterizes when we can do this. So it's an if and only if statement.
14:34
So we know there are then two directions we need to settle. There are not equivalently difficult.
14:52
So let's settle this sort of easier direction before we consider the harder direction.
14:58
So saying that it is worth arguing diagonal sizable seems to give a lot of additional structure,
15:04
and checking that a matrix is symmetric doesn't seem wildly difficult.
15:09
So why not try to prove that direction first? So let's try to prove that first.
15:13
So suppose a is orthogonal a.
15:18
Diagonal sizable. Well, then by the previous smart remark or previous note, that means that a is equal to.
15:27
That's the first transpose where word is a diagonal matrix.
15:39
So now I just want to compute a transpose so that a transpose will be as d as transpose transpose.
15:47
Well, now I know how the transport effects matrix product, so it's going to reverse this.
15:59
So this will become as transpose transpose primes de transpose times as transpose well the transpose of the transpose giving back the original matrix.
16:04
We've seen that before. So this becomes s.
16:17
If I transpose a diagonal matrix, well, then I'm just going to get back DX and then I have s transpose and hey, look, that's a.
16:20
So hence. A is symmetric, so we just proved the forward direction.
16:32
We proved a necessary condition in order to be orthogonal, diagonally visible, so we must be a symmetric matrix.
16:42
So the counter positive would tell us that if we have a matrix, then it's not symmetric and it's not orthogonal diagonals all.
16:51
And all of these, if you're thinking about for your projects, for instance,
17:01
generalizing these to complex vector spaces, all of these ideas have nice extensions to complex numbers, too.
17:05
So it's kind of a nice thing to think about there. How would this look?
17:13
What's the right notion of cemetery in this case? And so forth.
17:18
So the other direction is a little bit more subtle and difficult, but it's a nice tour of the ideas that we've seen this semester.
17:24
But in order to really see how that proof is going to go,
17:33
I'm going to suggest that we take one of my favorite strategies for understanding a new statement,
17:39
and that's to try a small example to see what's really happening.
17:46
So let's just try an example and see it from the example.
17:51
We can identify any patterns that might be useful in proving a result like this.
17:55
So and this is often the way that I would even proceed in my own research is that I have some big theorem.
18:01
I want to prove I don't immediately see how to prove it. Well, then let's just try it.
18:08
Let's see what happens. Maybe write some code. Maybe do some simulations. Maybe do a small back of the envelope calculation, do a thought experiment.
18:13
But you want to do something to kind of get some evidence, some idea of how the structure could look generate some data.
18:21
All right. So let's do a small example. So my small example, I want to take a particular symmetric matrix because after all,
18:29
the reverse implication, the converse, I want to start with a symmetric matrix.
18:37
And then I want to be able to construct an Earth a normal eigen basis.
18:41
So let's just try to see how we might do that.
18:46
So my example again, I don't want my examples to be wildly computationally challenging, so let's just take a two by two.
18:50
So let's take a to be the Matrix three negative two negative to negative two three.
19:00
There, it's definitely a symmetric matrix. All real entries. So it's my theorem should apply here.
19:12
So if the theorem is true, there should be a way to do this.
19:18
Well, let's just proceed as we normally would. And let's find a diagonal ization.
19:23
So how would we start a financial organization? Exactly right.
19:28
So we want to start by finding the eigenvalues, then you'd find the eigen spaces and then you could use that idealization theorem.
19:42
So let's find the eigenvalues.
19:49
So step one, find our eigenvalues.
19:54
So we do that, the only way that I've really taught you to do that is to compute the zeros of the characteristic polynomial.
20:00
Certainly, you can look to some of the projects for nice extensions, for other ways of getting at the eigenvalues,
20:11
especially for a very large matrices where it's not going to be practical to compute the characteristic polynomial, let alone the zeros.
20:16
So, so I'm looking at now the determinant of a minus lambda times the two by two identity matrix.
20:28
So this comes out to be three minus lambda squared minus four.
20:36
And if you multiply this out, becomes five minus six lambda plus lambda squared.
20:46
So then we can just factor and get my two eigenvalues, so my eigenvalues.
20:53
Then one in five. OK, so now we find our eigen spaces.
21:03
So step to find our eigen spaces again now this is just finding some null spaces.
21:12
So the eigen space corresponding to the eigenvalue of one will be the null space of a minus one times the two by two identity matrix.
21:21
So this becomes the span of a single factor.
21:32
One one. And if you look at the eigen space corresponding to the eigenvalue of five.
21:37
Then you have that this would be a minus five times the two by two identity matrix, which nicely becomes the span of the single vector minus one one.
21:47
What do you notice about those two eigen spaces? There are orthogonal, so that seems quite nice.
22:04
That seems worthy of note. So we have one one dotted with minus one one is equal to zero, so e one and E five are orthogonal sub spaces.
22:20
So there's some additional geometry showing up here. So one might naturally wonder if that persists, if that stays to be the case.
22:48
But let's continue on. So how would I then find my diagonal analyzing space?
22:56
How could I do that? On the eigenvectors, so we've have the eigenvectors right here, one one minus one one.
23:04
So then a diagonal izing basis would be one one minus one one I set out for in all the normal eigen basis.
23:16
So then there are these vectors are orthogonal. So then what's left to do it, to make it in the normal eigen basis?
23:24
What do I have to do?
23:43
Yeah, divide by the norm, so then in this case, I would then get one over the square two to one over the square to two and then over here,
23:45
I then get minus one over the square two to one over the square to two.
23:55
So there is my worth the normal eigen basis. So if I now represent my well, if I represent my original matrix, I can factor it with this new basis.
24:01
And then I get the best of the both of both worlds. So. In this case, and we could say take a to be my change of coordinates Matrix P.
24:25
The one over the square root of two one over the square to two minus one for the square two to one over the square to two.
24:39
Then my matrix one zero zero five and then my inverse matrix.
24:49
If I wanted to compute the inverse of this matrix, how would I compute the inverse of this matrix?
24:57
Transpose, because all the normal columns, we might as well take that to our advantage.
25:03
So one over the square to two minus one over the square to one over square, two one over square to.
25:07
So then that gives me an orthogonal diagonal ization of my matrix now.
25:16
So I'm getting the best of both worlds of both worlds. I get.
25:24
A nice basis in order to understand and study the geometry,
25:30
and I also get a basis that reflects the operator that I'm studying so that it also takes into account what a actually does Tommy.
25:33
Yeah. So everything in one is orthogonal dot product with zero, with the thing in the other.
25:50
So we have our orthogonal compliments. Yeah. Yep.
25:59
If we have an orthogonal item basis, I can just scale them and there that won't change whether they're eigenvectors,
26:13
so that those will capture the same notion. So.
26:20
Other questions, but I did I used Orthogonal in the definition. OK.
26:28
So again, let's not lose sight of why we did this example.
26:37
One reason why we did this example is to try to inform how this argument might look.
26:42
So it's not just a matter of like throwing a party because we finished the calculation.
26:47
You know, we're all done. We get to box this up and go home. We really want to think about like, what can we take away from this?
26:52
And so there are a few observations that you might make that are sort of useful here.
27:00
One observation that we've already made is that these eigen spaces are orthogonal.
27:05
And so you might wonder if that's a general phenomenon. Does that always happen for symmetric matrices?
27:12
And the answer is that that is a general phenomenon. What so we can, let's prove.
27:18
So let's see, I'll move this down. The entire class is about one theorem, so it's tricky to think about what to address.
27:25
So building towards the reverse direction, the Converse statement, so we start with our hypothesis if a is a symmetric matrix, so if a symmetric.
28:02
Then the eigen spaces are orthogonal.
28:16
So then any eigenvectors from distinct Asian spaces.
28:22
Spaces are orthogonal. I mean, this would have been a reasonable question.
28:36
And you did prove a version of the spectral theorem under the additional hypothesis that you are distinct.
28:55
Distinct eigenvalues, so then that the eigen spaces.
29:03
Our small. So let's let's go ahead and prove the proof.
29:10
So let's just set up some machinery here,
29:18
so let's take the one to be an element in your first alien space slam to one and be two to be and I can vector corresponding to your second can space.
29:21
The nonzero. Where I'm the one is different from the two.
29:39
So now what I want to prove is just that the DOT product between V one and V two will be equal to zero.
29:54
And hence, any elements in those spaces will be zero.
30:03
I guess I don't even need to assume that they're non-zero because those that will be immediate from zero.
30:08
So. The way that we're going to approach this is actually, you know what, I'm going to move to the boards below to prove it.
30:22
So the way we're going to prove this is.
30:52
Well, I want to compute the DOT product of V1 and V2, but the only thing I really have in the problem is these eigenvalues lurking around.
30:56
So I'm going to multiply the dot product by lambda one to try to get it into the problem.
31:04
That's really my whole heuristic here of trying to do. So I'm going to start with.
31:11
Lambda, one times the dot product of V one in V two.
31:17
And then I just want to sort of use that to somehow show that the DOT product would have to be equal to zero.
31:26
OK. Well, I can play around with the properties of DOT products now.
31:35
So, for instance, it's a scalar I can move it through the DOT product and I could apply it to the one or I can apply it to be two.
31:39
So let's move it through. So this becomes lambda one times v one.
31:47
We to. OK.
31:54
Well, now what could I do? Quinn.
32:00
Perfect, right, we use the fact that it's an eigenvalue, so this is the same as eight times the one dotted with B2.
32:10
All right. Well, I don't know, it's hard to know what to do next.
32:20
So my thought is generally, if I don't know what to do, let's at least try to like, simplify things somehow.
32:24
So in my case, I'm going to try to use the definition of the dot product.
32:30
So the definition of the DOT product tells me that this is equal to a v one transpose times v two.
32:34
OK. Well, I now know that I can use properties of matrix transposition to then rewrite this as the one transpose a transpose V to.
32:44
OK. What do you know about a transpose? It's a because it's a symmetric matrix, so that's good, OK, fine.
33:01
So then I'm going to move over to the next board. Sure, keep writing large enough.
33:09
So then I have the one transpose times a times v two.
33:24
Well, now I can use a sensitivity and rewrite it like this.
33:31
And now this looks like a dot product again, so why not turn this into a dot product?
33:38
Well, actually, before we do that eight times v two, what could we do with that?
33:44
Lambda, too. OK. So then we have one transpose times Lambda two v two.
33:50
And now that's just a scalar that lambda, too. So you can just pull that out front, that's just multiplying by a single real number.
33:58
So this is then lambda to be one transpose times B to B one transpose times v two is again the dot product A V one and V two.
34:06
So then I have lambda two times v one dotted with V two.
34:18
All right, so now if you put this all together. We get the following.
34:25
So hence, we have lambda one times v one dotted with V two is equal to lambda two times v one dotted with v two.
34:33
So I mean, there are a few ways you could go from here, but I prefer to sort of move it over to the other side and factor.
34:49
So this becomes lambda one minus lambda two times v one dotted with V2.
34:56
It's important to keep track of the types of all the things you're looking at here.
35:04
So the DOT product is just a real number. This is just giving you a real number.
35:09
This is just a difference of real numbers. So you have the product of two real numbers being equal to zero.
35:13
So, you know, one of them has to be equal to zero. Could the first factor be equal to zero?
35:18
Oh, because they're distinct, so it must be the second factor, since V1 and v2 are distinct, they're assumed to not be equal to one another.
35:24
Then we know that V1 started with V2 as equal to zero and hence they are orthogonal.
35:37
Jonathan, I thought you would have to.
35:49
Values are the same I've had multiple. That were the same, but her.
35:54
Distinct eigenvalues, distinct eigenvectors corresponding to the same eigenvalue.
36:04
Yes, we can. So like here in this particular case, like I had, we.
36:08
I mean, this was an element in you one.
36:18
I also had one, one was an element in each one.
36:23
Those are distinct eigenvectors corresponding to the same eigenvalue of one.
36:28
So it's an entire eigen space. So we get lots of eigenvectors corresponding to that single eigenvalue.
36:33
But but what I take a vector and the corresponding to the other eigenvalue, it has to be orthogonal to whatever value I chose here.
36:40
Because the sense in this case, since they're in different eigen spaces, they have to be orthogonal to one another.
36:50
But if you have. If you have to do that, you have to be.
36:56
Yes. Yes, because if they correspond to the same eigenvalue, then they're they're on the same eigen space.
37:03
Yeah. So you might be thinking of, though, that if a matrix, if the matrix isn't symmetric, the space spaces definitely don't have to be orthogonal.
37:11
I mean, we've seen examples where you might have like one in space is the x axis and the other eigen space is like the line Y equals x or something.
37:20
I mean, those are definitely not orthogonal. It's not true for general matrices that the eigen spaces will meet at a right angle.
37:27
It is true for symmetric matrices. Well, yes, that's what we're trying to prove here.
37:35
Yeah, we have not. I mean, we've just proven it here in this theorem. But.
37:42
The four symmetric matrices, this will be true. If the matrix is not symmetric, then you don't expect this to be true.
37:47
And Lisa. General I.
37:57
This proof is completely general, I didn't assume anything about the size of the Matrix.
38:04
And by and it's totally fine here. My example just kind of inspired that this might be true.
38:08
So then I tried to prove it, but in no place in my proof, like, here's the start of the proof,
38:15
and here's the rest I made no mention of the size of the matrix. So that's not necessarily relevant.
38:21
No. And it's the same thing is true when you're coming up with ideas, right?
38:27
I mean, a two by two matrix might be how you came up with the idea that something might be true and then you proved it in wild generality.
38:32
Or maybe it was.
38:39
Only two were true for two by two matrices, but your your single, your data point, your data doesn't necessarily encapsulate everything.
38:39
Yeah. So this is going to cover that as well, and that there's still going to be orthogonal,
38:52
they don't necessarily they won't be orthogonal compliments because there could be lots of eigen spaces like you could have in by a matrix.
39:05
You could have and distinct ones at most, but you could also have like one, for instance.
39:12
I mean, you could take the identity matrix.
39:18
Well, if you take the identity matrix, that's definitely orthogonal, diagnosable because it's already diagonal.
39:21
Right? You could just take the standard basis on our end, and that would orthogonal diagonals, the identity matrix.
39:26
So and that one only has the one eigenvalue of one with the algebraic multiplicity of N.
39:32
But then in that case, you get enough linearly independent eigenvectors.
39:38
So does that answer your question? Other questions?
39:45
Yeah, severe. You know.
39:52
We haven't shown that. No, no. Definitely haven't shown that.
40:02
Other questions. It's actually a lot, a lot to do to show the reverse direction.
40:07
It's I mean, it's the hardest theorem of the semester, so that's why I'm kind of spending a lot of time building up to it.
40:16
Don't want I don't want to spoil the suspense here.
40:24
There's another thing you might notice about the example that we considered
40:43
was that and this came up even on the quiz when we're thinking about dying,
40:49
analyzing matrices. There's the subtlety of whether we're thinking about dying, analyzing them over the real numbers or over the complex numbers.
40:54
Because when you're thinking about the zeros of your characteristic polynomial over the complex numbers,
41:02
this is one reason why we like the complex numbers. So much is because you have zeros for your functions.
41:06
For polynomials and the real numbers, you might not you might not have a point where you intersect with the real axis there,
41:14
so you have a little bit of a problem. But in this case, you notice we did have real eigenvalues.
41:22
That observation also persists for symmetric matrices. If you have a symmetric matrix, then all of your eigenvalues will be real.
41:31
So it's another result we can prove.
41:41
So theorem. If a symmetric.
41:46
So again, we're setting up under the hypothesis of the converse direction in the spectral theorem of going in the reverse direction,
41:53
if you start with that hypothesis, do you have a symmetric matrix? What structure do you have starting to look like?
42:00
You've got a lot of structure, so then a has all real eigenvalues.
42:05
OK. So let's prove it.
42:27
So proof. So one way that you can prove that a number is real is you can compare it with its complex conjugate.
42:46
If it's equal to its complex conjugate, then it's a real number. It's going to be our strategy here, but I'm going to take an eigenvalue.
42:57
I'm going to compute the complex conjugate of that real number or of that number and then verify that it's equal so that it has to be the same.
43:04
Oh. So that's what we're going to shoot for.
43:13
So when every time I use an over line here, a bar over my variable, that means take the complex conjugate.
43:17
Is everyone comfortable of complex conjugation as everyone remember this?
43:24
No. OK. Let me.
43:30
So if I have a no say Z is equal to a plus by this only take a second, but it's worth making sure we all agree on what we're talking about.
43:41
So here is and the complex plan we call this the real axis and the imaginary axis and Z is equal to a plus b i.
43:53
So what that means is that this length here we call this distance B and this distance over here A, so that's representing the complex numbers.
44:03
And it's sort of rectangular form could also think about it represented in its polar form.
44:13
So if you want to compute the complex conjugate, then you're reflecting across the real axis.
44:18
So that would be this point down here, so we call this zee bar for Zee conjugate.
44:23
So this is by definition equal to a minus VII slot in this distance.
44:30
This is now minus B. So Zee Bar the complex conjugate.
44:37
Of Z, an element in the complex numbers is given by.
44:51
Zee Bar. Is equal to you, a minus by.
44:59
Z is equal to a plus by. So an observation that you can then make is that.
45:09
Z is an element in the reals, so that means everything about Z is a complex number.
45:22
It's on the real axis. If its big component is equal to zero.
45:31
So if and only if Z is equal to Z Bar.
45:37
So if Z is equal to Z Bar, then reflecting across this axis gave you back the same thing.
45:46
So that meant, hey, look, you're on this axis.
45:53
So our strategy approved here so we can prove that something is real by showing that it's equal to its complex conjugate.
45:56
So standard trick and complex analysis. Oh, my.
46:03
OK. So now let's prove it. So whenever I use the bar here, that always means complex conjugate, not not the little vector arrow.
46:12
It's one reason why I don't like the vector aero notation. So what Lambda B and eigenvalue?
46:33
For asymmetric. Matrix Bay.
46:44
Or they're more likely be a corresponding.
47:01
Eigenvectors. All right.
47:11
So we then get the equation. A times B is equal to Lambda Times V.
47:28
So we're just going to kind of play around with this equation and manipulate it with the goal of
47:39
eventually showing that Lambda Bar Lambda complex conjugate of lambda is equal to lambda itself.
47:43
So one thing that I could do with this equation is I could take the transpose of both sides.
47:51
So let's try that take transpose.
47:57
Of both sides. OK.
48:05
So then we get a v transpose equal to Lambda V transpose.
48:20
OK, so we can use properties of the transpose again, so this becomes V transpose, a transpose is equal to lambda b transpose.
48:29
OK. Well.
48:44
I said the strategy was going to be to compare with the complex conjugate, so I'd like the complex conjugate to get into the problem somehow.
48:49
So why not take the complex conjugate of both sides and see what that looks like?
48:58
Hey. So now take conjugate.
49:03
Both sides were conjugate both sides. Both sides.
49:16
So then we have we transpose a transpose complex conjugate as equal to lambda b transpose complex conjugate.
49:23
OK, so now there are some concerns that you might have about how complex country, Jonathan, maybe you're going to get.
49:40
That's a great question. So there are complex conjugate of a vector is going to be the complex conjugate of each entry in that vector.
49:48
The same thing with a matrix. Then your next question will probably be what happens when you take the complex conjugate of a product?
49:57
So you might worry about what that will be.
50:06
It actually turns out to behave as nicely as you could hope that the complex conjugate of the product is the product of the complex conjugates.
50:09
I'm not going to prove that for you, but I am a complex analyst for a living, and so hopefully you'll believe me.
50:16
So what that means is that I can then break this up to be the complex conjugate transpose a
50:26
complex conjugate transpose as equal to lambda complex conjugate the complex conjugate transpose.
50:34
All right, so now let's think for a moment, what do we know about a what are the entries of like?
50:43
Your matrix, your original matrix that we started with, where did the entries have to live?
50:56
Tommy. Or sorry, you might not have had your hand up.
51:00
I shouldn't have called on you. Yeah. Yeah. They have to live in the real numbers, right,
51:05
so if I take the complex conjugate of a we started out with our matrix had to be only matrices with real entries.
51:11
OK, so here if I take the complex conjugate of a, that's just going to be a itself again.
51:20
So that's somewhat nice. So then we get the complex conjugate transpose time pay as equal to lambda complex conjugate the conjugate transpose.
51:25
And now, unfortunately, there's not that much we can really do in terms of simplifying with complex conjugation.
51:39
So we need to think about how else we might approach simplifying this.
51:47
Well, one thing that you can notice is that when you see a V transpose by itself,
51:52
it sort of calls out for multiplying by something, multiplying by V, for instance.
51:57
We did in the last problem to be kind of a nice thing to simplify.
52:02
So we're going to take the dot product of both sides with the.
52:06
So let's try that and see what happens. So now take the DOT product.
52:12
Both sides. With the.
52:29
OK, so then what will we get, we'll get the bar transpose times, A B is then equal to Lambda Bar.
52:39
We are transpose B OK.
52:51
Now we're really in business here. So what can we do?
52:56
How can we simplify this? Yes, of course.
53:01
It was a good thing to focus on the AV part. I like that idea.
53:15
What could we do with the AV part? It's Lambda V, right, so we could replace that because we remember Aves equal to Lambda V.
53:19
Well, hey, why not get rid of the eh? So this becomes the bar transpose times.
53:28
Lambda V is equal to Lambda Bar Times V Bar Transpose Times V.
53:33
OK.
53:41
Well, the lambda, again, is just a scalar that pulls up front you have lambda times v bar transpose V is equal to lambda bar times v bar transpose V.
53:42
Who is an eigenvectors? So it's nonzero. So this thing is very much like the DOT product.
53:56
So it's like, I can't it's it's zero if and only if it's the zero vector, so we know it's not non-zero vector.
54:06
So then that means that these coefficients lambda and lambda bar then have to be equal.
54:13
I'm telling you that Lambda is equal to the bar. Jonathan, how do you?
54:20
V is an Eigenvectors V and observe actor. But what about like you just feel like we also plan to get that?
54:30
But the transpose that's a good question. So just ignore the complex conjugation for a moment, pretend that V is a real vector.
54:39
So then this would be v transpose times v. So that's v dotted with V.
54:48
Right? So if you have V dotted with V, then that's like the length squared of V, right?
54:53
And that's only going to be zero zero. Right? But I don't understand how.
55:00
How to get the. OK, so let's go back over here for a moment and think about what the product of a complex conjugate is.
55:05
I should have maybe reviewed slightly more.
55:15
So if we take the product of each of complex conjugative Z with itself, so Z transpose time Z, this will be equal to a minus B times a plus by.
55:19
Right. So then when I multiply this out, it's a difference of squares.
55:38
So then this is going to be a squared plus b squared.
55:43
So if you think about what that is geometrically, what does that represent?
55:50
In this picture that I've drawn. We're on the air.
55:55
Yeah, it's sort of like this length here. The curve getting at the the square of the hypotenuse of that triangle.
56:01
So this is the modulus squared of the the same thing is going to be true here when we're taking the products of these vectors,
56:09
it's going to happen in each component because then so then it would be saying each component would have to be equal to zero.
56:20
So go. Here, hear what is multiplying, yes.
56:27
Yup, here we're just multiplying. Oh, you're right, you're right, I just want to multiply.
56:33
You're right, it could take the dot product, I suppose, if I wanted to, but I don't want to transpose.
56:44
So let's just multiply. You're right.
56:52
Got too, too fixated on my. Fix it on my dot products fix.
56:57
Now there are questions. So the upshot of this is it's telling us that symmetric matrices actually have a lot of structure, unlike general matrices.
57:09
They have to have real eigenvalues, and their eigen spaces can't just be sort of like skew in space, they have to meet orthogonal.
57:19
And so that's actually a pretty useful bit of geometry to know when we're thinking about how we might
57:29
approach or giving an orthogonal basis for my for my four hour at an orthogonal eigen basis for our.
57:35
OK. Are there questions? All right.
57:47
Let's see. So I'm not going to have time to finish the proof of the theorem today.
57:54
So what I want to do is I just want to set it up because the logic there's actually quite a lot to the overall structure, to the proof.
58:07
And so I want to set it up so that you can kind of think about it between now and Wednesday's class.
58:14
We won't take all of Wednesday's class to finish it. Then we'll prove the spectral theorem and we'll kind of go from there.
58:20
So let me go through how the proof is going to look.
58:28
It's really a quite nice because it tours a lot of the ideas that we've seen over the semester.
58:37
And it's a fundamental theorem for anybody doing a project related to principal component analysis or singular value decomposition.
58:45
Oh. So for.
58:53
So the idea is we're going to suppose. But a is a symmetric.
59:04
And by and matrix. So we're going to prove this result by contradiction.
59:20
So the way that we're going to do the thing we're going to contradict is we're going to assume that we've chosen and a war end is minimal.
59:34
OK, so we're and is as small as possible where it's not or legally diagnosable.
59:44
So let's prove suppose a is a symmetric and by and matrix.
59:51
And let's actually for contradiction, we're then going to say that a is an example of not just a symmetric one by a matrix.
59:57
But one that's not worth arguing diagonals. So where the conclusion would fail, so we're trying to prove this by contradiction.
1:00:06
So I suppose it is a symmetric and by a matrix, but is not going to be diagnosable.
1:00:17
So we're supposing we have a counter example to our theorem. And.
1:00:29
If we had a one by one matrix, if we had a one by one matrix, it's always orthogonal diagonals.
1:00:42
So it's just a one by one matrix. It doesn't really make sense to talk about analyzing a one by one matrix.
1:00:52
So that means that for NW will be then strictly larger than one when we're thinking
1:00:58
about what the smallest one could be the smallest size matrix where this happens.
1:01:04
So we want it to be a symmetric and by a matrix that is not orthogonal, diagnosable where an is as small as possible.
1:01:09
So what that means is that no matter how small and is, maybe it's like a thirty seven by thirty seven matrix,
1:01:27
that means every matrix from every symmetric matrix from size one by one up through thirty six by thirty six,
1:01:34
then would have to be orthogonal, diagnosable. This is the smallest size you can have where it's not orthogonal.
1:01:42
Diagnosable. OK.
1:01:50
So if you think about how I'm trying to set up, the proof is that I start with my essay that I say is not orthogonal, diagonals visible.
1:01:52
Then I'm going to take a smaller piece of a to get a smaller matrix,
1:02:00
something that's say, not thirty seven by thirty seven, but thirty six by thirty six.
1:02:05
Then, because that's a smaller matrix. My assumption here would then tell me it must be orthogonal diagonal sizable.
1:02:09
I thought the only diagonal size, that smaller piece.
1:02:18
And then I put that together with my with a vector from my original matrix to then diagonals, the original matrix giving a contradiction.
1:02:21
That's going to be the general structure of what we're doing. There's certainly some work in doing that, but that's the general structure.
1:02:31
So we should also note that if NW is the smallest, it can possibly be.
1:02:40
Well, one by one matrices, we say, are sort of trivially diagonal of also and has to be strictly bigger than one.
1:02:45
Are there any questions so far? Yes, Jonathan.
1:02:57
Well, yeah, there are a number of names that this kind of structure goes by,
1:03:05
I mean, proof by smallest counterexample or something approved by the descent.
1:03:11
So yeah, there's a number of different ways that you can think about this. It's very similar to approved by induction.
1:03:15
Yeah, it's a nice way of thinking about it. OK.
1:03:20
So again, I I don't want to rush this.
1:03:26
If I had like a little bit more time, I could finish today, but I don't want to rush this, so I really want to focus on just what the big ideas are.
1:03:30
And then we'll probably and a little bit early. And Isa.
1:03:36
Yep. Because of the assumption that that's the smallest size that isn't worth diagnosable, because then if you went to a smaller size,
1:03:46
then it must be orthogonal dying leasable because otherwise you would have taken that to be the smallest size.
1:03:59
The size of. The smallest and where this wouldn't be with diagnosable up.
1:04:06
So, yeah, that's a great question. OK, so now what we want to do is we want the contradiction.
1:04:18
So the way that I'm going to contradict this is I'm going to then construct an orthogonal basis consisting of eigenvectors.
1:04:36
So our goal is we want to find all the normal for the normal.
1:04:43
Basis for our NW, consisting of eigenvectors 16 eigenvectors of a.
1:04:52
Because once we have that, that tells us that this word applies is with arguably diagnosable and we're done well in that case,
1:05:13
then we're saying that we have a contradiction to be a contradiction, then we're not.
1:05:26
OK, so that's what we're hoping for, if we can do that. We're done.
1:05:34
So we want to somehow be able to do that.
1:05:38
So the way that we're going to do that is we're going to use the previous two theorems about the structure of symmetric matrices to then say,
1:05:41
we must have real eigenvalues, they must have orthogonal eigen spaces to then piece together a way of putting of diagonals in your entire space.
1:05:49
OK. So I think rather than rushing through the sort of 15 minutes left of this proof, I'm going to finish it on Wednesdays class.
1:06:01
On Wednesdays class. I'm also going to spend a little bit of time answering one question that I've spent
1:06:11
quite a lot of time answering questions about by email and in individual meetings.
1:06:17
And that's what are some additional fun math classes you might take.
1:06:23
So not maybe not everyone is thinking about that,
1:06:28
but I do want to give us some idea of what other options there are for people for next semester or next year.
1:06:33
So that's where we'll kind of go with next class, and I think that'll make a nice bookend to the semester.
1:06:40
All right. See everyone on Wednesday. Check one you.